{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "colab": {
   "name": "predictCounty.ipynb",
   "provenance": []
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "CsluoFv7Sp2j"
   },
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "tlcuuCFhSp3G"
   },
   "source": [
    "years = range(2000,2020,4)\n",
    "df = pd.read_csv('counties.csv')"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "PrR6Do9iSp3f",
    "outputId": "8ac723de-d0f5-4e58-d36c-5ebb3373e731",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    }
   },
   "source": [
    "data = df.drop(columns=['state', 'county'])\n",
    "data = data.dropna(axis=0,how='any')\n",
    "data.describe()"
   ],
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2000D</th>\n",
       "      <th>2000R</th>\n",
       "      <th>2000O</th>\n",
       "      <th>2004D</th>\n",
       "      <th>2004R</th>\n",
       "      <th>2004O</th>\n",
       "      <th>2008D</th>\n",
       "      <th>2008R</th>\n",
       "      <th>2008O</th>\n",
       "      <th>2012D</th>\n",
       "      <th>2012R</th>\n",
       "      <th>2012O</th>\n",
       "      <th>2016D</th>\n",
       "      <th>2016R</th>\n",
       "      <th>2016O</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.875000e+03</td>\n",
       "      <td>2875.000000</td>\n",
       "      <td>2875.000000</td>\n",
       "      <td>2.875000e+03</td>\n",
       "      <td>2.875000e+03</td>\n",
       "      <td>2875.000000</td>\n",
       "      <td>2.875000e+03</td>\n",
       "      <td>2875.000000</td>\n",
       "      <td>2875.000000</td>\n",
       "      <td>2.875000e+03</td>\n",
       "      <td>2875.000000</td>\n",
       "      <td>2875.000000</td>\n",
       "      <td>2.875000e+03</td>\n",
       "      <td>2875.000000</td>\n",
       "      <td>2875.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.699424e+04</td>\n",
       "      <td>16533.856000</td>\n",
       "      <td>1352.073391</td>\n",
       "      <td>1.956915e+04</td>\n",
       "      <td>2.028555e+04</td>\n",
       "      <td>508.671652</td>\n",
       "      <td>2.298134e+04</td>\n",
       "      <td>19539.632696</td>\n",
       "      <td>610.991304</td>\n",
       "      <td>2.171365e+04</td>\n",
       "      <td>19761.090783</td>\n",
       "      <td>886.516522</td>\n",
       "      <td>2.175607e+04</td>\n",
       "      <td>20524.178783</td>\n",
       "      <td>2543.772522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.942532e+04</td>\n",
       "      <td>39923.542134</td>\n",
       "      <td>4232.141433</td>\n",
       "      <td>6.769202e+04</td>\n",
       "      <td>4.869764e+04</td>\n",
       "      <td>1976.171276</td>\n",
       "      <td>7.944593e+04</td>\n",
       "      <td>45955.629730</td>\n",
       "      <td>1913.652362</td>\n",
       "      <td>7.628626e+04</td>\n",
       "      <td>45649.528293</td>\n",
       "      <td>3056.034805</td>\n",
       "      <td>8.298175e+04</td>\n",
       "      <td>44262.493478</td>\n",
       "      <td>7726.198627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.400000e+01</td>\n",
       "      <td>106.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.200000e+01</td>\n",
       "      <td>6.500000e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000e+00</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.751500e+03</td>\n",
       "      <td>2689.000000</td>\n",
       "      <td>109.000000</td>\n",
       "      <td>1.882500e+03</td>\n",
       "      <td>3.124500e+03</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>1.930500e+03</td>\n",
       "      <td>2956.000000</td>\n",
       "      <td>78.500000</td>\n",
       "      <td>1.662500e+03</td>\n",
       "      <td>3021.500000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>1.240000e+03</td>\n",
       "      <td>3346.000000</td>\n",
       "      <td>195.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.830000e+03</td>\n",
       "      <td>5363.000000</td>\n",
       "      <td>278.000000</td>\n",
       "      <td>4.177000e+03</td>\n",
       "      <td>6.522000e+03</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>4.493000e+03</td>\n",
       "      <td>6297.000000</td>\n",
       "      <td>182.000000</td>\n",
       "      <td>4.014000e+03</td>\n",
       "      <td>6398.000000</td>\n",
       "      <td>195.000000</td>\n",
       "      <td>3.199000e+03</td>\n",
       "      <td>7194.000000</td>\n",
       "      <td>529.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>9.580500e+03</td>\n",
       "      <td>13166.000000</td>\n",
       "      <td>886.500000</td>\n",
       "      <td>1.078350e+04</td>\n",
       "      <td>1.646800e+04</td>\n",
       "      <td>302.000000</td>\n",
       "      <td>1.262450e+04</td>\n",
       "      <td>16042.000000</td>\n",
       "      <td>453.500000</td>\n",
       "      <td>1.147700e+04</td>\n",
       "      <td>16391.500000</td>\n",
       "      <td>564.000000</td>\n",
       "      <td>9.940500e+03</td>\n",
       "      <td>17945.500000</td>\n",
       "      <td>1736.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.710505e+06</td>\n",
       "      <td>871930.000000</td>\n",
       "      <td>112719.000000</td>\n",
       "      <td>1.907736e+06</td>\n",
       "      <td>1.076225e+06</td>\n",
       "      <td>39515.000000</td>\n",
       "      <td>2.295853e+06</td>\n",
       "      <td>956425.000000</td>\n",
       "      <td>65970.000000</td>\n",
       "      <td>2.216903e+06</td>\n",
       "      <td>885333.000000</td>\n",
       "      <td>78831.000000</td>\n",
       "      <td>2.464364e+06</td>\n",
       "      <td>769743.000000</td>\n",
       "      <td>200201.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              2000D          2000R  ...          2016R          2016O\n",
       "count  2.875000e+03    2875.000000  ...    2875.000000    2875.000000\n",
       "mean   1.699424e+04   16533.856000  ...   20524.178783    2543.772522\n",
       "std    5.942532e+04   39923.542134  ...   44262.493478    7726.198627\n",
       "min    1.400000e+01     106.000000  ...      58.000000       3.000000\n",
       "25%    1.751500e+03    2689.000000  ...    3346.000000     195.000000\n",
       "50%    3.830000e+03    5363.000000  ...    7194.000000     529.000000\n",
       "75%    9.580500e+03   13166.000000  ...   17945.500000    1736.500000\n",
       "max    1.710505e+06  871930.000000  ...  769743.000000  200201.000000\n",
       "\n",
       "[8 rows x 15 columns]"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 3
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "0-LVluzeSp4P"
   },
   "source": [
    "data = data.to_numpy(dtype=np.int64)"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "YMNKgPOuSp5D"
   },
   "source": [
    "def plotCounty(state,county,d,r,o,predicted=None):\n",
    "    years = range(2000,2020,4)\n",
    "    plt.figure()\n",
    "    plt.plot(years,d,'b',label='Democrat')\n",
    "    plt.plot(years,r,'r',label='Republican')\n",
    "    plt.plot(years,o,'y',label='Other')\n",
    "    plt.plot(years,d+r+o,'k',label='Total')\n",
    "    if predicted is not None:\n",
    "        new_years = range(2008,2024,4)\n",
    "        plt.plot(new_years,predicted[0]*20000,'c',label='Democrat Model')\n",
    "        plt.plot(new_years,predicted[1]*20000,'m',label='Republican Model')\n",
    "        plt.plot(new_years,predicted[2]*250,'g',label='Other Model')\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Votes')\n",
    "    plt.title(county+', '+state)\n",
    "    plt.xticks(years)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ],
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "DJh6SpbjSp5s"
   },
   "source": [
    "def calcState(state):\n",
    "    state_df = df[df['state']==state]\n",
    "    state_df = state_df.drop(columns=['state','county'])\n",
    "    state_votes = state_df.values.tolist()\n",
    "    state_votes = np.array(state_votes)\n",
    "    state_votes = np.sum(state_votes,axis=0,dtype=np.int64)\n",
    "    d = []\n",
    "    r = []\n",
    "    o = []\n",
    "    for ind, val in enumerate(state_votes):\n",
    "        party = ind % 3\n",
    "        if party == 0:\n",
    "            d.append(val)\n",
    "        elif party == 1:\n",
    "            r.append(val)\n",
    "        else:\n",
    "            o.append(val)\n",
    "    d = np.array(d)\n",
    "    r = np.array(r)\n",
    "    o = np.array(o)\n",
    "    return d,r,o"
   ],
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "Ir1pXANGSp6F"
   },
   "source": [
    "def plotState(state,d,r,o):\n",
    "    years = range(2000,2020,4)\n",
    "    plt.figure()\n",
    "    plt.plot(years,d,'b',label='Democrat')\n",
    "    plt.plot(years,r,'r',label='Republican')\n",
    "    plt.plot(years,o,'y',label='Other')\n",
    "    plt.plot(years,d+r+o,'k',label='Total')\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Votes')\n",
    "    plt.title(state)\n",
    "    plt.xticks(years)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ],
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "BhlfhjBISp6o"
   },
   "source": [
    "def convertbyParty(point):\n",
    "    d = point[:,::3]/20000\n",
    "    r = point[:,1::3]/20000\n",
    "    o = point[:,2::3]/250\n",
    "    return d,r,o"
   ],
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "sKpS8inlSp7A"
   },
   "source": [
    "def getDataset(data,batch_size,split=True):\n",
    "    data = np.expand_dims(data,axis=-1)\n",
    "    np.random.shuffle(data)\n",
    "    num = data.shape[0]\n",
    "    train_data = data[:int(num*0.6)]\n",
    "    val_data = data[int(num*0.6):int(num*0.8)]\n",
    "    test_data = data[int(num*0.8):]\n",
    "    def createDS(point,label=True):\n",
    "        d,r,o=convertbyParty(point)\n",
    "        new_data = np.vstack((d,r,o))\n",
    "        new_data = np.vstack((new_data,new_data*1.5))\n",
    "        ds = tf.data.Dataset.from_tensor_slices(new_data)\n",
    "        if label:\n",
    "            ds = ds.map(lambda p: (p[:-1],p[1:]))\n",
    "        else:\n",
    "            ds = ds.map(lambda p: p[:-1])\n",
    "        return ds.batch(batch_size).prefetch(1)\n",
    "    if split:\n",
    "        return createDS(train_data),createDS(val_data),createDS(test_data)\n",
    "    else:\n",
    "        return createDS(data,False)"
   ],
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "FuRS8y-8Sp7O",
    "outputId": "2bffa668-6a35-475d-863b-981d69b4ba82",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    }
   },
   "source": [
    "train_ds, val_ds, test_ds = getDataset(data,8)\n",
    "train_ds\n"
   ],
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ((None, 4, 1), (None, 4, 1)), types: (tf.float64, tf.float64)>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 10
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "bHcc7PIxSp7h"
   },
   "source": [
    "# model = tf.keras.Sequential([\n",
    "#     tf.keras.layers.Conv1D(2,2,activation='relu',input_shape=(4,1)),\n",
    "#     tf.keras.layers.Conv1D(2,2,activation='relu'),\n",
    "#     tf.keras.layers.Flatten(),\n",
    "#     tf.keras.layers.Dense(4,activation='relu'),\n",
    "#     tf.keras.layers.Reshape((4,1)),\n",
    "#     tf.keras.layers.LSTM(8,return_sequences=True),\n",
    "#     tf.keras.layers.LSTM(8,return_sequences=True),\n",
    "#     tf.keras.layers.LSTM(8,return_sequences=True),\n",
    "# ])"
   ],
   "execution_count": 26,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "bKa-U7wESp7w"
   },
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv1D(4,2,input_shape=(4,1),activation='relu'),\n",
    "    tf.keras.layers.LSTM(8,return_sequences=True),\n",
    "    tf.keras.layers.LSTM(16,return_sequences=True),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(4),\n",
    "    tf.keras.layers.Reshape((4,1))\n",
    "])"
   ],
   "execution_count": 35,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "Sm_qpTzDSp7-",
    "outputId": "ad6aeea9-2d7d-4787-aa25-e7da61f89351",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    }
   },
   "source": [
    "lr_schedule = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-6 * 10**(epoch / 20))\n",
    "optimizer = tf.keras.optimizers.Adam(lr=1e-3)\n",
    "model.compile(loss=tf.keras.losses.Huber(),optimizer=optimizer,metrics=['mae','mse'])\n",
    "model.summary()"
   ],
   "execution_count": 36,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_7 (Conv1D)            (None, 3, 4)              12        \n",
      "_________________________________________________________________\n",
      "lstm_11 (LSTM)               (None, 3, 8)              416       \n",
      "_________________________________________________________________\n",
      "lstm_12 (LSTM)               (None, 3, 16)             1600      \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 48)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 4)                 196       \n",
      "_________________________________________________________________\n",
      "reshape_4 (Reshape)          (None, 4, 1)              0         \n",
      "=================================================================\n",
      "Total params: 2,224\n",
      "Trainable params: 2,224\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "SR-cfHLeSp8N"
   },
   "source": [
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\"checkpoint4.h5\", save_best_only=True)\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=125)\n"
   ],
   "execution_count": 37,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "lmf1QmWbSp8c",
    "outputId": "bd60f37b-94ab-4e53-f488-2f319d256fa6",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    }
   },
   "source": [
    "# history = model.fit(train_ds,epochs=100,validation_data=val_ds,callbacks=[lr_schedule])\n",
    "# plt.semilogx(history.history[\"lr\"], history.history[\"loss\"])\n",
    "# plt.axis([1e-6, 10, 0, 3])\n",
    "def getlr(epoch):\n",
    "  return 1e-5\n",
    "new_lr = tf.keras.callbacks.LearningRateScheduler(getlr)\n",
    "model.fit(train_ds,epochs=10000,validation_data=val_ds,callbacks=[model_checkpoint,early_stopping,new_lr])"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Epoch 1/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3164 - mae: 0.3858 - mse: 57.9642 - val_loss: 0.1509 - val_mae: 0.2194 - val_mse: 2.5308\n",
      "Epoch 2/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3164 - mae: 0.3850 - mse: 57.9582 - val_loss: 0.1508 - val_mae: 0.2187 - val_mse: 2.5299\n",
      "Epoch 3/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3163 - mae: 0.3848 - mse: 57.9619 - val_loss: 0.1508 - val_mae: 0.2184 - val_mse: 2.5325\n",
      "Epoch 4/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3163 - mae: 0.3847 - mse: 57.9619 - val_loss: 0.1508 - val_mae: 0.2182 - val_mse: 2.5324\n",
      "Epoch 5/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3163 - mae: 0.3846 - mse: 57.9605 - val_loss: 0.1508 - val_mae: 0.2181 - val_mse: 2.5328\n",
      "Epoch 6/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3162 - mae: 0.3845 - mse: 57.9582 - val_loss: 0.1508 - val_mae: 0.2180 - val_mse: 2.5327\n",
      "Epoch 7/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3162 - mae: 0.3845 - mse: 57.9558 - val_loss: 0.1508 - val_mae: 0.2179 - val_mse: 2.5329\n",
      "Epoch 8/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3162 - mae: 0.3845 - mse: 57.9531 - val_loss: 0.1508 - val_mae: 0.2178 - val_mse: 2.5328\n",
      "Epoch 9/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3162 - mae: 0.3844 - mse: 57.9504 - val_loss: 0.1508 - val_mae: 0.2178 - val_mse: 2.5330\n",
      "Epoch 10/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3162 - mae: 0.3844 - mse: 57.9475 - val_loss: 0.1508 - val_mae: 0.2177 - val_mse: 2.5330\n",
      "Epoch 11/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3162 - mae: 0.3844 - mse: 57.9447 - val_loss: 0.1508 - val_mae: 0.2177 - val_mse: 2.5330\n",
      "Epoch 12/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3162 - mae: 0.3843 - mse: 57.9418 - val_loss: 0.1508 - val_mae: 0.2177 - val_mse: 2.5330\n",
      "Epoch 13/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3161 - mae: 0.3843 - mse: 57.9388 - val_loss: 0.1508 - val_mae: 0.2176 - val_mse: 2.5331\n",
      "Epoch 14/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3161 - mae: 0.3843 - mse: 57.9359 - val_loss: 0.1508 - val_mae: 0.2176 - val_mse: 2.5331\n",
      "Epoch 15/10000\n",
      "1294/1294 [==============================] - 7s 6ms/step - loss: 0.3161 - mae: 0.3843 - mse: 57.9329 - val_loss: 0.1508 - val_mae: 0.2176 - val_mse: 2.5331\n",
      "Epoch 16/10000\n",
      "1294/1294 [==============================] - 7s 6ms/step - loss: 0.3161 - mae: 0.3842 - mse: 57.9300 - val_loss: 0.1508 - val_mae: 0.2176 - val_mse: 2.5332\n",
      "Epoch 17/10000\n",
      "1294/1294 [==============================] - 7s 6ms/step - loss: 0.3161 - mae: 0.3842 - mse: 57.9270 - val_loss: 0.1508 - val_mae: 0.2175 - val_mse: 2.5332\n",
      "Epoch 18/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3161 - mae: 0.3842 - mse: 57.9240 - val_loss: 0.1508 - val_mae: 0.2175 - val_mse: 2.5332\n",
      "Epoch 19/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3161 - mae: 0.3842 - mse: 57.9210 - val_loss: 0.1508 - val_mae: 0.2175 - val_mse: 2.5332\n",
      "Epoch 20/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3161 - mae: 0.3842 - mse: 57.9180 - val_loss: 0.1508 - val_mae: 0.2175 - val_mse: 2.5332\n",
      "Epoch 21/10000\n",
      "1294/1294 [==============================] - 7s 6ms/step - loss: 0.3160 - mae: 0.3841 - mse: 57.9150 - val_loss: 0.1508 - val_mae: 0.2174 - val_mse: 2.5332\n",
      "Epoch 22/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3160 - mae: 0.3841 - mse: 57.9120 - val_loss: 0.1508 - val_mae: 0.2174 - val_mse: 2.5332\n",
      "Epoch 23/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3160 - mae: 0.3841 - mse: 57.9090 - val_loss: 0.1508 - val_mae: 0.2174 - val_mse: 2.5332\n",
      "Epoch 24/10000\n",
      "1294/1294 [==============================] - 7s 6ms/step - loss: 0.3160 - mae: 0.3841 - mse: 57.9059 - val_loss: 0.1508 - val_mae: 0.2174 - val_mse: 2.5331\n",
      "Epoch 25/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3160 - mae: 0.3841 - mse: 57.9029 - val_loss: 0.1508 - val_mae: 0.2174 - val_mse: 2.5331\n",
      "Epoch 26/10000\n",
      "1294/1294 [==============================] - 7s 6ms/step - loss: 0.3160 - mae: 0.3840 - mse: 57.8999 - val_loss: 0.1508 - val_mae: 0.2174 - val_mse: 2.5331\n",
      "Epoch 27/10000\n",
      "1294/1294 [==============================] - 7s 6ms/step - loss: 0.3160 - mae: 0.3840 - mse: 57.8968 - val_loss: 0.1508 - val_mae: 0.2173 - val_mse: 2.5331\n",
      "Epoch 28/10000\n",
      "1294/1294 [==============================] - 7s 6ms/step - loss: 0.3159 - mae: 0.3840 - mse: 57.8938 - val_loss: 0.1508 - val_mae: 0.2173 - val_mse: 2.5331\n",
      "Epoch 29/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3159 - mae: 0.3840 - mse: 57.8907 - val_loss: 0.1508 - val_mae: 0.2173 - val_mse: 2.5330\n",
      "Epoch 30/10000\n",
      "1294/1294 [==============================] - 7s 6ms/step - loss: 0.3159 - mae: 0.3840 - mse: 57.8877 - val_loss: 0.1508 - val_mae: 0.2173 - val_mse: 2.5330\n",
      "Epoch 31/10000\n",
      "1294/1294 [==============================] - 7s 6ms/step - loss: 0.3159 - mae: 0.3839 - mse: 57.8847 - val_loss: 0.1508 - val_mae: 0.2173 - val_mse: 2.5330\n",
      "Epoch 32/10000\n",
      "1294/1294 [==============================] - 7s 6ms/step - loss: 0.3159 - mae: 0.3839 - mse: 57.8816 - val_loss: 0.1508 - val_mae: 0.2173 - val_mse: 2.5330\n",
      "Epoch 33/10000\n",
      "1294/1294 [==============================] - 7s 6ms/step - loss: 0.3159 - mae: 0.3839 - mse: 57.8786 - val_loss: 0.1508 - val_mae: 0.2173 - val_mse: 2.5329\n",
      "Epoch 34/10000\n",
      "1294/1294 [==============================] - 7s 6ms/step - loss: 0.3159 - mae: 0.3839 - mse: 57.8756 - val_loss: 0.1508 - val_mae: 0.2172 - val_mse: 2.5329\n",
      "Epoch 35/10000\n",
      "1294/1294 [==============================] - 7s 6ms/step - loss: 0.3159 - mae: 0.3839 - mse: 57.8725 - val_loss: 0.1508 - val_mae: 0.2172 - val_mse: 2.5329\n",
      "Epoch 36/10000\n",
      "1294/1294 [==============================] - 7s 6ms/step - loss: 0.3158 - mae: 0.3839 - mse: 57.8695 - val_loss: 0.1508 - val_mae: 0.2172 - val_mse: 2.5329\n",
      "Epoch 37/10000\n",
      "1294/1294 [==============================] - 7s 6ms/step - loss: 0.3158 - mae: 0.3838 - mse: 57.8664 - val_loss: 0.1508 - val_mae: 0.2172 - val_mse: 2.5328\n",
      "Epoch 38/10000\n",
      "1294/1294 [==============================] - 7s 6ms/step - loss: 0.3158 - mae: 0.3838 - mse: 57.8634 - val_loss: 0.1508 - val_mae: 0.2172 - val_mse: 2.5328\n",
      "Epoch 39/10000\n",
      "1294/1294 [==============================] - 7s 6ms/step - loss: 0.3158 - mae: 0.3838 - mse: 57.8603 - val_loss: 0.1508 - val_mae: 0.2172 - val_mse: 2.5328\n",
      "Epoch 40/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3158 - mae: 0.3838 - mse: 57.8573 - val_loss: 0.1508 - val_mae: 0.2172 - val_mse: 2.5328\n",
      "Epoch 41/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3158 - mae: 0.3838 - mse: 57.8543 - val_loss: 0.1508 - val_mae: 0.2172 - val_mse: 2.5327\n",
      "Epoch 42/10000\n",
      "1294/1294 [==============================] - 7s 6ms/step - loss: 0.3158 - mae: 0.3838 - mse: 57.8513 - val_loss: 0.1508 - val_mae: 0.2172 - val_mse: 2.5327\n",
      "Epoch 43/10000\n",
      "1294/1294 [==============================] - 7s 6ms/step - loss: 0.3158 - mae: 0.3837 - mse: 57.8482 - val_loss: 0.1508 - val_mae: 0.2171 - val_mse: 2.5327\n",
      "Epoch 44/10000\n",
      "1294/1294 [==============================] - 7s 6ms/step - loss: 0.3157 - mae: 0.3837 - mse: 57.8452 - val_loss: 0.1508 - val_mae: 0.2171 - val_mse: 2.5327\n",
      "Epoch 45/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3157 - mae: 0.3837 - mse: 57.8421 - val_loss: 0.1508 - val_mae: 0.2171 - val_mse: 2.5326\n",
      "Epoch 46/10000\n",
      "1294/1294 [==============================] - 7s 6ms/step - loss: 0.3157 - mae: 0.3837 - mse: 57.8391 - val_loss: 0.1508 - val_mae: 0.2171 - val_mse: 2.5326\n",
      "Epoch 47/10000\n",
      "1294/1294 [==============================] - 7s 6ms/step - loss: 0.3157 - mae: 0.3837 - mse: 57.8361 - val_loss: 0.1508 - val_mae: 0.2171 - val_mse: 2.5326\n",
      "Epoch 48/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3157 - mae: 0.3837 - mse: 57.8330 - val_loss: 0.1508 - val_mae: 0.2171 - val_mse: 2.5326\n",
      "Epoch 49/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3157 - mae: 0.3836 - mse: 57.8300 - val_loss: 0.1508 - val_mae: 0.2171 - val_mse: 2.5325\n",
      "Epoch 50/10000\n",
      "1294/1294 [==============================] - 7s 6ms/step - loss: 0.3157 - mae: 0.3836 - mse: 57.8270 - val_loss: 0.1508 - val_mae: 0.2171 - val_mse: 2.5325\n",
      "Epoch 51/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3157 - mae: 0.3836 - mse: 57.8240 - val_loss: 0.1508 - val_mae: 0.2171 - val_mse: 2.5325\n",
      "Epoch 52/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3156 - mae: 0.3836 - mse: 57.8209 - val_loss: 0.1508 - val_mae: 0.2171 - val_mse: 2.5324\n",
      "Epoch 53/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3156 - mae: 0.3836 - mse: 57.8179 - val_loss: 0.1508 - val_mae: 0.2171 - val_mse: 2.5324\n",
      "Epoch 54/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3156 - mae: 0.3836 - mse: 57.8148 - val_loss: 0.1508 - val_mae: 0.2170 - val_mse: 2.5324\n",
      "Epoch 55/10000\n",
      "1294/1294 [==============================] - 7s 6ms/step - loss: 0.3156 - mae: 0.3835 - mse: 57.8119 - val_loss: 0.1508 - val_mae: 0.2170 - val_mse: 2.5324\n",
      "Epoch 56/10000\n",
      "1294/1294 [==============================] - 7s 6ms/step - loss: 0.3156 - mae: 0.3835 - mse: 57.8089 - val_loss: 0.1508 - val_mae: 0.2170 - val_mse: 2.5324\n",
      "Epoch 57/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3156 - mae: 0.3835 - mse: 57.8059 - val_loss: 0.1508 - val_mae: 0.2170 - val_mse: 2.5323\n",
      "Epoch 58/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3156 - mae: 0.3835 - mse: 57.8029 - val_loss: 0.1508 - val_mae: 0.2170 - val_mse: 2.5323\n",
      "Epoch 59/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3156 - mae: 0.3835 - mse: 57.7999 - val_loss: 0.1508 - val_mae: 0.2170 - val_mse: 2.5323\n",
      "Epoch 60/10000\n",
      "1294/1294 [==============================] - 7s 6ms/step - loss: 0.3155 - mae: 0.3835 - mse: 57.7969 - val_loss: 0.1508 - val_mae: 0.2170 - val_mse: 2.5323\n",
      "Epoch 61/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3155 - mae: 0.3834 - mse: 57.7939 - val_loss: 0.1508 - val_mae: 0.2170 - val_mse: 2.5322\n",
      "Epoch 62/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3155 - mae: 0.3834 - mse: 57.7909 - val_loss: 0.1508 - val_mae: 0.2170 - val_mse: 2.5322\n",
      "Epoch 63/10000\n",
      "1294/1294 [==============================] - 7s 6ms/step - loss: 0.3155 - mae: 0.3834 - mse: 57.7879 - val_loss: 0.1508 - val_mae: 0.2170 - val_mse: 2.5322\n",
      "Epoch 64/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3155 - mae: 0.3834 - mse: 57.7849 - val_loss: 0.1508 - val_mae: 0.2170 - val_mse: 2.5322\n",
      "Epoch 65/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3155 - mae: 0.3834 - mse: 57.7819 - val_loss: 0.1508 - val_mae: 0.2170 - val_mse: 2.5321\n",
      "Epoch 66/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3155 - mae: 0.3834 - mse: 57.7790 - val_loss: 0.1508 - val_mae: 0.2170 - val_mse: 2.5321\n",
      "Epoch 67/10000\n",
      "1294/1294 [==============================] - 7s 6ms/step - loss: 0.3155 - mae: 0.3834 - mse: 57.7760 - val_loss: 0.1508 - val_mae: 0.2169 - val_mse: 2.5321\n",
      "Epoch 68/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3154 - mae: 0.3833 - mse: 57.7730 - val_loss: 0.1508 - val_mae: 0.2169 - val_mse: 2.5321\n",
      "Epoch 69/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3154 - mae: 0.3833 - mse: 57.7701 - val_loss: 0.1508 - val_mae: 0.2169 - val_mse: 2.5320\n",
      "Epoch 70/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3154 - mae: 0.3833 - mse: 57.7671 - val_loss: 0.1508 - val_mae: 0.2169 - val_mse: 2.5320\n",
      "Epoch 71/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3154 - mae: 0.3833 - mse: 57.7641 - val_loss: 0.1508 - val_mae: 0.2169 - val_mse: 2.5320\n",
      "Epoch 72/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3154 - mae: 0.3833 - mse: 57.7611 - val_loss: 0.1508 - val_mae: 0.2169 - val_mse: 2.5320\n",
      "Epoch 73/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3154 - mae: 0.3833 - mse: 57.7581 - val_loss: 0.1508 - val_mae: 0.2169 - val_mse: 2.5319\n",
      "Epoch 74/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3154 - mae: 0.3832 - mse: 57.7552 - val_loss: 0.1508 - val_mae: 0.2169 - val_mse: 2.5319\n",
      "Epoch 75/10000\n",
      "1294/1294 [==============================] - 7s 6ms/step - loss: 0.3154 - mae: 0.3832 - mse: 57.7522 - val_loss: 0.1508 - val_mae: 0.2169 - val_mse: 2.5319\n",
      "Epoch 76/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3153 - mae: 0.3832 - mse: 57.7492 - val_loss: 0.1508 - val_mae: 0.2169 - val_mse: 2.5319\n",
      "Epoch 77/10000\n",
      "1294/1294 [==============================] - 7s 6ms/step - loss: 0.3153 - mae: 0.3832 - mse: 57.7463 - val_loss: 0.1508 - val_mae: 0.2169 - val_mse: 2.5319\n",
      "Epoch 78/10000\n",
      "1294/1294 [==============================] - 7s 6ms/step - loss: 0.3153 - mae: 0.3832 - mse: 57.7433 - val_loss: 0.1508 - val_mae: 0.2169 - val_mse: 2.5318\n",
      "Epoch 79/10000\n",
      "1294/1294 [==============================] - 7s 6ms/step - loss: 0.3153 - mae: 0.3832 - mse: 57.7403 - val_loss: 0.1508 - val_mae: 0.2169 - val_mse: 2.5318\n",
      "Epoch 80/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3153 - mae: 0.3832 - mse: 57.7374 - val_loss: 0.1508 - val_mae: 0.2169 - val_mse: 2.5318\n",
      "Epoch 81/10000\n",
      "1294/1294 [==============================] - 7s 6ms/step - loss: 0.3153 - mae: 0.3831 - mse: 57.7344 - val_loss: 0.1508 - val_mae: 0.2169 - val_mse: 2.5318\n",
      "Epoch 82/10000\n",
      "1294/1294 [==============================] - 7s 6ms/step - loss: 0.3153 - mae: 0.3831 - mse: 57.7314 - val_loss: 0.1508 - val_mae: 0.2168 - val_mse: 2.5318\n",
      "Epoch 83/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3153 - mae: 0.3831 - mse: 57.7285 - val_loss: 0.1508 - val_mae: 0.2168 - val_mse: 2.5317\n",
      "Epoch 84/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3152 - mae: 0.3831 - mse: 57.7255 - val_loss: 0.1508 - val_mae: 0.2168 - val_mse: 2.5317\n",
      "Epoch 85/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3152 - mae: 0.3831 - mse: 57.7226 - val_loss: 0.1508 - val_mae: 0.2168 - val_mse: 2.5317\n",
      "Epoch 86/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3152 - mae: 0.3831 - mse: 57.7196 - val_loss: 0.1508 - val_mae: 0.2168 - val_mse: 2.5317\n",
      "Epoch 87/10000\n",
      "1294/1294 [==============================] - 7s 6ms/step - loss: 0.3152 - mae: 0.3831 - mse: 57.7166 - val_loss: 0.1508 - val_mae: 0.2168 - val_mse: 2.5317\n",
      "Epoch 88/10000\n",
      "1294/1294 [==============================] - 7s 6ms/step - loss: 0.3152 - mae: 0.3830 - mse: 57.7137 - val_loss: 0.1508 - val_mae: 0.2168 - val_mse: 2.5316\n",
      "Epoch 89/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3152 - mae: 0.3830 - mse: 57.7107 - val_loss: 0.1508 - val_mae: 0.2168 - val_mse: 2.5316\n",
      "Epoch 90/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3152 - mae: 0.3830 - mse: 57.7077 - val_loss: 0.1508 - val_mae: 0.2168 - val_mse: 2.5316\n",
      "Epoch 91/10000\n",
      "1294/1294 [==============================] - 7s 6ms/step - loss: 0.3152 - mae: 0.3830 - mse: 57.7048 - val_loss: 0.1508 - val_mae: 0.2168 - val_mse: 2.5316\n",
      "Epoch 92/10000\n",
      "1294/1294 [==============================] - 7s 6ms/step - loss: 0.3151 - mae: 0.3830 - mse: 57.7018 - val_loss: 0.1508 - val_mae: 0.2168 - val_mse: 2.5315\n",
      "Epoch 93/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3151 - mae: 0.3830 - mse: 57.6989 - val_loss: 0.1508 - val_mae: 0.2168 - val_mse: 2.5315\n",
      "Epoch 94/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3151 - mae: 0.3830 - mse: 57.6959 - val_loss: 0.1508 - val_mae: 0.2168 - val_mse: 2.5315\n",
      "Epoch 95/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3151 - mae: 0.3829 - mse: 57.6930 - val_loss: 0.1508 - val_mae: 0.2168 - val_mse: 2.5315\n",
      "Epoch 96/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3151 - mae: 0.3829 - mse: 57.6900 - val_loss: 0.1508 - val_mae: 0.2168 - val_mse: 2.5315\n",
      "Epoch 97/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3151 - mae: 0.3829 - mse: 57.6871 - val_loss: 0.1508 - val_mae: 0.2168 - val_mse: 2.5314\n",
      "Epoch 98/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3151 - mae: 0.3829 - mse: 57.6841 - val_loss: 0.1508 - val_mae: 0.2168 - val_mse: 2.5314\n",
      "Epoch 99/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3151 - mae: 0.3829 - mse: 57.6812 - val_loss: 0.1508 - val_mae: 0.2168 - val_mse: 2.5314\n",
      "Epoch 100/10000\n",
      "1294/1294 [==============================] - 7s 6ms/step - loss: 0.3150 - mae: 0.3829 - mse: 57.6782 - val_loss: 0.1508 - val_mae: 0.2167 - val_mse: 2.5314\n",
      "Epoch 101/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3150 - mae: 0.3828 - mse: 57.6752 - val_loss: 0.1508 - val_mae: 0.2167 - val_mse: 2.5314\n",
      "Epoch 102/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3150 - mae: 0.3828 - mse: 57.6723 - val_loss: 0.1508 - val_mae: 0.2167 - val_mse: 2.5313\n",
      "Epoch 103/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3150 - mae: 0.3828 - mse: 57.6693 - val_loss: 0.1508 - val_mae: 0.2167 - val_mse: 2.5313\n",
      "Epoch 104/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3150 - mae: 0.3828 - mse: 57.6664 - val_loss: 0.1508 - val_mae: 0.2167 - val_mse: 2.5313\n",
      "Epoch 105/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3150 - mae: 0.3828 - mse: 57.6634 - val_loss: 0.1508 - val_mae: 0.2167 - val_mse: 2.5313\n",
      "Epoch 106/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3150 - mae: 0.3828 - mse: 57.6605 - val_loss: 0.1508 - val_mae: 0.2167 - val_mse: 2.5312\n",
      "Epoch 107/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3150 - mae: 0.3828 - mse: 57.6575 - val_loss: 0.1508 - val_mae: 0.2167 - val_mse: 2.5312\n",
      "Epoch 108/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3149 - mae: 0.3827 - mse: 57.6546 - val_loss: 0.1507 - val_mae: 0.2167 - val_mse: 2.5312\n",
      "Epoch 109/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3149 - mae: 0.3827 - mse: 57.6516 - val_loss: 0.1507 - val_mae: 0.2167 - val_mse: 2.5312\n",
      "Epoch 110/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3149 - mae: 0.3827 - mse: 57.6487 - val_loss: 0.1507 - val_mae: 0.2167 - val_mse: 2.5311\n",
      "Epoch 111/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3149 - mae: 0.3827 - mse: 57.6457 - val_loss: 0.1507 - val_mae: 0.2167 - val_mse: 2.5311\n",
      "Epoch 112/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3149 - mae: 0.3827 - mse: 57.6428 - val_loss: 0.1507 - val_mae: 0.2167 - val_mse: 2.5311\n",
      "Epoch 113/10000\n",
      "1294/1294 [==============================] - 7s 6ms/step - loss: 0.3149 - mae: 0.3827 - mse: 57.6398 - val_loss: 0.1507 - val_mae: 0.2167 - val_mse: 2.5311\n",
      "Epoch 114/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3149 - mae: 0.3827 - mse: 57.6369 - val_loss: 0.1507 - val_mae: 0.2167 - val_mse: 2.5310\n",
      "Epoch 115/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3149 - mae: 0.3826 - mse: 57.6339 - val_loss: 0.1507 - val_mae: 0.2167 - val_mse: 2.5310\n",
      "Epoch 116/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3149 - mae: 0.3826 - mse: 57.6310 - val_loss: 0.1507 - val_mae: 0.2167 - val_mse: 2.5310\n",
      "Epoch 117/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3148 - mae: 0.3826 - mse: 57.6280 - val_loss: 0.1507 - val_mae: 0.2167 - val_mse: 2.5310\n",
      "Epoch 118/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3148 - mae: 0.3826 - mse: 57.6251 - val_loss: 0.1507 - val_mae: 0.2167 - val_mse: 2.5309\n",
      "Epoch 119/10000\n",
      "1294/1294 [==============================] - 7s 6ms/step - loss: 0.3148 - mae: 0.3826 - mse: 57.6221 - val_loss: 0.1507 - val_mae: 0.2166 - val_mse: 2.5309\n",
      "Epoch 120/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3148 - mae: 0.3826 - mse: 57.6192 - val_loss: 0.1507 - val_mae: 0.2166 - val_mse: 2.5309\n",
      "Epoch 121/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3148 - mae: 0.3826 - mse: 57.6162 - val_loss: 0.1507 - val_mae: 0.2166 - val_mse: 2.5309\n",
      "Epoch 122/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3148 - mae: 0.3825 - mse: 57.6133 - val_loss: 0.1507 - val_mae: 0.2166 - val_mse: 2.5308\n",
      "Epoch 123/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3148 - mae: 0.3825 - mse: 57.6103 - val_loss: 0.1507 - val_mae: 0.2166 - val_mse: 2.5308\n",
      "Epoch 124/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3148 - mae: 0.3825 - mse: 57.6074 - val_loss: 0.1507 - val_mae: 0.2166 - val_mse: 2.5308\n",
      "Epoch 125/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3147 - mae: 0.3825 - mse: 57.6044 - val_loss: 0.1507 - val_mae: 0.2166 - val_mse: 2.5308\n",
      "Epoch 126/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3147 - mae: 0.3825 - mse: 57.6015 - val_loss: 0.1507 - val_mae: 0.2166 - val_mse: 2.5307\n",
      "Epoch 127/10000\n",
      "1294/1294 [==============================] - 7s 6ms/step - loss: 0.3147 - mae: 0.3825 - mse: 57.5985 - val_loss: 0.1507 - val_mae: 0.2166 - val_mse: 2.5307\n",
      "Epoch 128/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3147 - mae: 0.3825 - mse: 57.5956 - val_loss: 0.1507 - val_mae: 0.2166 - val_mse: 2.5307\n",
      "Epoch 129/10000\n",
      "1294/1294 [==============================] - 7s 6ms/step - loss: 0.3147 - mae: 0.3824 - mse: 57.5926 - val_loss: 0.1507 - val_mae: 0.2166 - val_mse: 2.5307\n",
      "Epoch 130/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3147 - mae: 0.3824 - mse: 57.5897 - val_loss: 0.1507 - val_mae: 0.2166 - val_mse: 2.5306\n",
      "Epoch 131/10000\n",
      "1294/1294 [==============================] - 7s 6ms/step - loss: 0.3147 - mae: 0.3824 - mse: 57.5868 - val_loss: 0.1507 - val_mae: 0.2166 - val_mse: 2.5306\n",
      "Epoch 132/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3147 - mae: 0.3824 - mse: 57.5838 - val_loss: 0.1507 - val_mae: 0.2166 - val_mse: 2.5306\n",
      "Epoch 133/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3146 - mae: 0.3824 - mse: 57.5809 - val_loss: 0.1507 - val_mae: 0.2166 - val_mse: 2.5306\n",
      "Epoch 134/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3146 - mae: 0.3824 - mse: 57.5779 - val_loss: 0.1507 - val_mae: 0.2166 - val_mse: 2.5305\n",
      "Epoch 135/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3146 - mae: 0.3824 - mse: 57.5750 - val_loss: 0.1507 - val_mae: 0.2166 - val_mse: 2.5305\n",
      "Epoch 136/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3146 - mae: 0.3823 - mse: 57.5720 - val_loss: 0.1507 - val_mae: 0.2166 - val_mse: 2.5305\n",
      "Epoch 137/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3146 - mae: 0.3823 - mse: 57.5691 - val_loss: 0.1507 - val_mae: 0.2166 - val_mse: 2.5305\n",
      "Epoch 138/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3146 - mae: 0.3823 - mse: 57.5661 - val_loss: 0.1507 - val_mae: 0.2166 - val_mse: 2.5304\n",
      "Epoch 139/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3146 - mae: 0.3823 - mse: 57.5632 - val_loss: 0.1507 - val_mae: 0.2166 - val_mse: 2.5304\n",
      "Epoch 140/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3146 - mae: 0.3823 - mse: 57.5602 - val_loss: 0.1507 - val_mae: 0.2165 - val_mse: 2.5304\n",
      "Epoch 141/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3145 - mae: 0.3823 - mse: 57.5573 - val_loss: 0.1507 - val_mae: 0.2165 - val_mse: 2.5304\n",
      "Epoch 142/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3145 - mae: 0.3823 - mse: 57.5544 - val_loss: 0.1507 - val_mae: 0.2165 - val_mse: 2.5303\n",
      "Epoch 143/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3145 - mae: 0.3822 - mse: 57.5514 - val_loss: 0.1507 - val_mae: 0.2165 - val_mse: 2.5303\n",
      "Epoch 144/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3145 - mae: 0.3822 - mse: 57.5485 - val_loss: 0.1507 - val_mae: 0.2165 - val_mse: 2.5303\n",
      "Epoch 145/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3145 - mae: 0.3822 - mse: 57.5455 - val_loss: 0.1507 - val_mae: 0.2165 - val_mse: 2.5302\n",
      "Epoch 146/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3145 - mae: 0.3822 - mse: 57.5426 - val_loss: 0.1507 - val_mae: 0.2165 - val_mse: 2.5302\n",
      "Epoch 147/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3145 - mae: 0.3822 - mse: 57.5397 - val_loss: 0.1507 - val_mae: 0.2165 - val_mse: 2.5302\n",
      "Epoch 148/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3145 - mae: 0.3822 - mse: 57.5367 - val_loss: 0.1507 - val_mae: 0.2165 - val_mse: 2.5302\n",
      "Epoch 149/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3145 - mae: 0.3822 - mse: 57.5338 - val_loss: 0.1507 - val_mae: 0.2165 - val_mse: 2.5301\n",
      "Epoch 150/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3144 - mae: 0.3821 - mse: 57.5308 - val_loss: 0.1507 - val_mae: 0.2165 - val_mse: 2.5301\n",
      "Epoch 151/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3144 - mae: 0.3821 - mse: 57.5279 - val_loss: 0.1507 - val_mae: 0.2165 - val_mse: 2.5301\n",
      "Epoch 152/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3144 - mae: 0.3821 - mse: 57.5249 - val_loss: 0.1507 - val_mae: 0.2165 - val_mse: 2.5300\n",
      "Epoch 153/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3144 - mae: 0.3821 - mse: 57.5220 - val_loss: 0.1507 - val_mae: 0.2165 - val_mse: 2.5300\n",
      "Epoch 154/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3144 - mae: 0.3821 - mse: 57.5190 - val_loss: 0.1507 - val_mae: 0.2165 - val_mse: 2.5300\n",
      "Epoch 155/10000\n",
      "1294/1294 [==============================] - 7s 6ms/step - loss: 0.3144 - mae: 0.3821 - mse: 57.5161 - val_loss: 0.1507 - val_mae: 0.2165 - val_mse: 2.5300\n",
      "Epoch 156/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3144 - mae: 0.3821 - mse: 57.5132 - val_loss: 0.1507 - val_mae: 0.2165 - val_mse: 2.5299\n",
      "Epoch 157/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3144 - mae: 0.3820 - mse: 57.5102 - val_loss: 0.1507 - val_mae: 0.2165 - val_mse: 2.5299\n",
      "Epoch 158/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3143 - mae: 0.3820 - mse: 57.5073 - val_loss: 0.1507 - val_mae: 0.2165 - val_mse: 2.5299\n",
      "Epoch 159/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3143 - mae: 0.3820 - mse: 57.5043 - val_loss: 0.1507 - val_mae: 0.2165 - val_mse: 2.5298\n",
      "Epoch 160/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3143 - mae: 0.3820 - mse: 57.5014 - val_loss: 0.1507 - val_mae: 0.2165 - val_mse: 2.5298\n",
      "Epoch 161/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3143 - mae: 0.3820 - mse: 57.4984 - val_loss: 0.1507 - val_mae: 0.2165 - val_mse: 2.5298\n",
      "Epoch 162/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3143 - mae: 0.3820 - mse: 57.4955 - val_loss: 0.1507 - val_mae: 0.2165 - val_mse: 2.5298\n",
      "Epoch 163/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3143 - mae: 0.3820 - mse: 57.4926 - val_loss: 0.1507 - val_mae: 0.2164 - val_mse: 2.5297\n",
      "Epoch 164/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3143 - mae: 0.3819 - mse: 57.4897 - val_loss: 0.1507 - val_mae: 0.2164 - val_mse: 2.5297\n",
      "Epoch 165/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3143 - mae: 0.3819 - mse: 57.4867 - val_loss: 0.1507 - val_mae: 0.2164 - val_mse: 2.5297\n",
      "Epoch 166/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3142 - mae: 0.3819 - mse: 57.4838 - val_loss: 0.1507 - val_mae: 0.2164 - val_mse: 2.5296\n",
      "Epoch 167/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3142 - mae: 0.3819 - mse: 57.4808 - val_loss: 0.1507 - val_mae: 0.2164 - val_mse: 2.5296\n",
      "Epoch 168/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3142 - mae: 0.3819 - mse: 57.4779 - val_loss: 0.1507 - val_mae: 0.2164 - val_mse: 2.5296\n",
      "Epoch 169/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3142 - mae: 0.3819 - mse: 57.4750 - val_loss: 0.1507 - val_mae: 0.2164 - val_mse: 2.5295\n",
      "Epoch 170/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3142 - mae: 0.3819 - mse: 57.4720 - val_loss: 0.1507 - val_mae: 0.2164 - val_mse: 2.5295\n",
      "Epoch 171/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3142 - mae: 0.3818 - mse: 57.4691 - val_loss: 0.1507 - val_mae: 0.2164 - val_mse: 2.5295\n",
      "Epoch 172/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3142 - mae: 0.3818 - mse: 57.4661 - val_loss: 0.1507 - val_mae: 0.2164 - val_mse: 2.5295\n",
      "Epoch 173/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3142 - mae: 0.3818 - mse: 57.4632 - val_loss: 0.1507 - val_mae: 0.2164 - val_mse: 2.5294\n",
      "Epoch 174/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3142 - mae: 0.3818 - mse: 57.4603 - val_loss: 0.1507 - val_mae: 0.2164 - val_mse: 2.5294\n",
      "Epoch 175/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3141 - mae: 0.3818 - mse: 57.4573 - val_loss: 0.1507 - val_mae: 0.2164 - val_mse: 2.5294\n",
      "Epoch 176/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3141 - mae: 0.3818 - mse: 57.4544 - val_loss: 0.1507 - val_mae: 0.2164 - val_mse: 2.5293\n",
      "Epoch 177/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3141 - mae: 0.3818 - mse: 57.4515 - val_loss: 0.1507 - val_mae: 0.2164 - val_mse: 2.5293\n",
      "Epoch 178/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3141 - mae: 0.3818 - mse: 57.4485 - val_loss: 0.1507 - val_mae: 0.2164 - val_mse: 2.5293\n",
      "Epoch 179/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3141 - mae: 0.3817 - mse: 57.4456 - val_loss: 0.1507 - val_mae: 0.2164 - val_mse: 2.5292\n",
      "Epoch 180/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3141 - mae: 0.3817 - mse: 57.4426 - val_loss: 0.1507 - val_mae: 0.2164 - val_mse: 2.5292\n",
      "Epoch 181/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3141 - mae: 0.3817 - mse: 57.4397 - val_loss: 0.1507 - val_mae: 0.2164 - val_mse: 2.5292\n",
      "Epoch 182/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3141 - mae: 0.3817 - mse: 57.4367 - val_loss: 0.1507 - val_mae: 0.2164 - val_mse: 2.5292\n",
      "Epoch 183/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3140 - mae: 0.3817 - mse: 57.4338 - val_loss: 0.1507 - val_mae: 0.2164 - val_mse: 2.5291\n",
      "Epoch 184/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3140 - mae: 0.3817 - mse: 57.4308 - val_loss: 0.1507 - val_mae: 0.2164 - val_mse: 2.5291\n",
      "Epoch 185/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3140 - mae: 0.3817 - mse: 57.4279 - val_loss: 0.1507 - val_mae: 0.2164 - val_mse: 2.5291\n",
      "Epoch 186/10000\n",
      "1294/1294 [==============================] - 7s 6ms/step - loss: 0.3140 - mae: 0.3816 - mse: 57.4249 - val_loss: 0.1507 - val_mae: 0.2164 - val_mse: 2.5290\n",
      "Epoch 187/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3140 - mae: 0.3816 - mse: 57.4220 - val_loss: 0.1507 - val_mae: 0.2163 - val_mse: 2.5290\n",
      "Epoch 188/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3140 - mae: 0.3816 - mse: 57.4190 - val_loss: 0.1507 - val_mae: 0.2163 - val_mse: 2.5290\n",
      "Epoch 189/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3140 - mae: 0.3816 - mse: 57.4161 - val_loss: 0.1507 - val_mae: 0.2163 - val_mse: 2.5289\n",
      "Epoch 190/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3140 - mae: 0.3816 - mse: 57.4131 - val_loss: 0.1507 - val_mae: 0.2163 - val_mse: 2.5289\n",
      "Epoch 191/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3139 - mae: 0.3816 - mse: 57.4102 - val_loss: 0.1507 - val_mae: 0.2163 - val_mse: 2.5289\n",
      "Epoch 192/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3139 - mae: 0.3816 - mse: 57.4072 - val_loss: 0.1507 - val_mae: 0.2163 - val_mse: 2.5288\n",
      "Epoch 193/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3139 - mae: 0.3815 - mse: 57.4043 - val_loss: 0.1507 - val_mae: 0.2163 - val_mse: 2.5288\n",
      "Epoch 194/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3139 - mae: 0.3815 - mse: 57.4013 - val_loss: 0.1507 - val_mae: 0.2163 - val_mse: 2.5288\n",
      "Epoch 195/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3139 - mae: 0.3815 - mse: 57.3984 - val_loss: 0.1507 - val_mae: 0.2163 - val_mse: 2.5288\n",
      "Epoch 196/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3139 - mae: 0.3815 - mse: 57.3955 - val_loss: 0.1507 - val_mae: 0.2163 - val_mse: 2.5287\n",
      "Epoch 197/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3139 - mae: 0.3815 - mse: 57.3925 - val_loss: 0.1507 - val_mae: 0.2163 - val_mse: 2.5287\n",
      "Epoch 198/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3139 - mae: 0.3815 - mse: 57.3895 - val_loss: 0.1507 - val_mae: 0.2163 - val_mse: 2.5287\n",
      "Epoch 199/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3138 - mae: 0.3815 - mse: 57.3866 - val_loss: 0.1507 - val_mae: 0.2163 - val_mse: 2.5286\n",
      "Epoch 200/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3138 - mae: 0.3814 - mse: 57.3837 - val_loss: 0.1507 - val_mae: 0.2163 - val_mse: 2.5286\n",
      "Epoch 201/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3138 - mae: 0.3814 - mse: 57.3807 - val_loss: 0.1507 - val_mae: 0.2163 - val_mse: 2.5286\n",
      "Epoch 202/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3138 - mae: 0.3814 - mse: 57.3778 - val_loss: 0.1507 - val_mae: 0.2163 - val_mse: 2.5285\n",
      "Epoch 203/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3138 - mae: 0.3814 - mse: 57.3748 - val_loss: 0.1507 - val_mae: 0.2163 - val_mse: 2.5285\n",
      "Epoch 204/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3138 - mae: 0.3814 - mse: 57.3719 - val_loss: 0.1506 - val_mae: 0.2163 - val_mse: 2.5285\n",
      "Epoch 205/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3138 - mae: 0.3814 - mse: 57.3689 - val_loss: 0.1506 - val_mae: 0.2163 - val_mse: 2.5285\n",
      "Epoch 206/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3138 - mae: 0.3814 - mse: 57.3660 - val_loss: 0.1506 - val_mae: 0.2163 - val_mse: 2.5284\n",
      "Epoch 207/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3138 - mae: 0.3813 - mse: 57.3630 - val_loss: 0.1506 - val_mae: 0.2163 - val_mse: 2.5284\n",
      "Epoch 208/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3137 - mae: 0.3813 - mse: 57.3601 - val_loss: 0.1506 - val_mae: 0.2163 - val_mse: 2.5284\n",
      "Epoch 209/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3137 - mae: 0.3813 - mse: 57.3571 - val_loss: 0.1506 - val_mae: 0.2163 - val_mse: 2.5283\n",
      "Epoch 210/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3137 - mae: 0.3813 - mse: 57.3542 - val_loss: 0.1506 - val_mae: 0.2163 - val_mse: 2.5283\n",
      "Epoch 211/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3137 - mae: 0.3813 - mse: 57.3513 - val_loss: 0.1506 - val_mae: 0.2162 - val_mse: 2.5283\n",
      "Epoch 212/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3137 - mae: 0.3813 - mse: 57.3483 - val_loss: 0.1506 - val_mae: 0.2162 - val_mse: 2.5282\n",
      "Epoch 213/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3137 - mae: 0.3813 - mse: 57.3453 - val_loss: 0.1506 - val_mae: 0.2162 - val_mse: 2.5282\n",
      "Epoch 214/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3137 - mae: 0.3812 - mse: 57.3424 - val_loss: 0.1506 - val_mae: 0.2162 - val_mse: 2.5282\n",
      "Epoch 215/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3137 - mae: 0.3812 - mse: 57.3394 - val_loss: 0.1506 - val_mae: 0.2162 - val_mse: 2.5281\n",
      "Epoch 216/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3136 - mae: 0.3812 - mse: 57.3365 - val_loss: 0.1506 - val_mae: 0.2162 - val_mse: 2.5281\n",
      "Epoch 217/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3136 - mae: 0.3812 - mse: 57.3335 - val_loss: 0.1506 - val_mae: 0.2162 - val_mse: 2.5281\n",
      "Epoch 218/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3136 - mae: 0.3812 - mse: 57.3306 - val_loss: 0.1506 - val_mae: 0.2162 - val_mse: 2.5280\n",
      "Epoch 219/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3136 - mae: 0.3812 - mse: 57.3276 - val_loss: 0.1506 - val_mae: 0.2162 - val_mse: 2.5280\n",
      "Epoch 220/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3136 - mae: 0.3812 - mse: 57.3247 - val_loss: 0.1506 - val_mae: 0.2162 - val_mse: 2.5280\n",
      "Epoch 221/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3136 - mae: 0.3812 - mse: 57.3217 - val_loss: 0.1506 - val_mae: 0.2162 - val_mse: 2.5279\n",
      "Epoch 222/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3136 - mae: 0.3811 - mse: 57.3188 - val_loss: 0.1506 - val_mae: 0.2162 - val_mse: 2.5279\n",
      "Epoch 223/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3136 - mae: 0.3811 - mse: 57.3158 - val_loss: 0.1506 - val_mae: 0.2162 - val_mse: 2.5279\n",
      "Epoch 224/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3136 - mae: 0.3811 - mse: 57.3129 - val_loss: 0.1506 - val_mae: 0.2162 - val_mse: 2.5278\n",
      "Epoch 225/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3135 - mae: 0.3811 - mse: 57.3099 - val_loss: 0.1506 - val_mae: 0.2162 - val_mse: 2.5278\n",
      "Epoch 226/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3135 - mae: 0.3811 - mse: 57.3070 - val_loss: 0.1506 - val_mae: 0.2162 - val_mse: 2.5278\n",
      "Epoch 227/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3135 - mae: 0.3811 - mse: 57.3040 - val_loss: 0.1506 - val_mae: 0.2162 - val_mse: 2.5278\n",
      "Epoch 228/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3135 - mae: 0.3811 - mse: 57.3011 - val_loss: 0.1506 - val_mae: 0.2162 - val_mse: 2.5277\n",
      "Epoch 229/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3135 - mae: 0.3810 - mse: 57.2981 - val_loss: 0.1506 - val_mae: 0.2162 - val_mse: 2.5277\n",
      "Epoch 230/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3135 - mae: 0.3810 - mse: 57.2952 - val_loss: 0.1506 - val_mae: 0.2162 - val_mse: 2.5277\n",
      "Epoch 231/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3135 - mae: 0.3810 - mse: 57.2922 - val_loss: 0.1506 - val_mae: 0.2162 - val_mse: 2.5276\n",
      "Epoch 232/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3135 - mae: 0.3810 - mse: 57.2893 - val_loss: 0.1506 - val_mae: 0.2162 - val_mse: 2.5276\n",
      "Epoch 233/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3134 - mae: 0.3810 - mse: 57.2863 - val_loss: 0.1506 - val_mae: 0.2162 - val_mse: 2.5276\n",
      "Epoch 234/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3134 - mae: 0.3810 - mse: 57.2834 - val_loss: 0.1506 - val_mae: 0.2162 - val_mse: 2.5275\n",
      "Epoch 235/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3134 - mae: 0.3810 - mse: 57.2804 - val_loss: 0.1506 - val_mae: 0.2162 - val_mse: 2.5275\n",
      "Epoch 236/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3134 - mae: 0.3809 - mse: 57.2775 - val_loss: 0.1506 - val_mae: 0.2161 - val_mse: 2.5275\n",
      "Epoch 237/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3134 - mae: 0.3809 - mse: 57.2745 - val_loss: 0.1506 - val_mae: 0.2161 - val_mse: 2.5274\n",
      "Epoch 238/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3134 - mae: 0.3809 - mse: 57.2716 - val_loss: 0.1506 - val_mae: 0.2161 - val_mse: 2.5274\n",
      "Epoch 239/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3134 - mae: 0.3809 - mse: 57.2687 - val_loss: 0.1506 - val_mae: 0.2161 - val_mse: 2.5274\n",
      "Epoch 240/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3134 - mae: 0.3809 - mse: 57.2657 - val_loss: 0.1506 - val_mae: 0.2161 - val_mse: 2.5273\n",
      "Epoch 241/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3133 - mae: 0.3809 - mse: 57.2628 - val_loss: 0.1506 - val_mae: 0.2161 - val_mse: 2.5273\n",
      "Epoch 242/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3133 - mae: 0.3809 - mse: 57.2598 - val_loss: 0.1506 - val_mae: 0.2161 - val_mse: 2.5273\n",
      "Epoch 243/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3133 - mae: 0.3808 - mse: 57.2569 - val_loss: 0.1506 - val_mae: 0.2161 - val_mse: 2.5272\n",
      "Epoch 244/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3133 - mae: 0.3808 - mse: 57.2539 - val_loss: 0.1506 - val_mae: 0.2161 - val_mse: 2.5272\n",
      "Epoch 245/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3133 - mae: 0.3808 - mse: 57.2510 - val_loss: 0.1506 - val_mae: 0.2161 - val_mse: 2.5272\n",
      "Epoch 246/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3133 - mae: 0.3808 - mse: 57.2481 - val_loss: 0.1506 - val_mae: 0.2161 - val_mse: 2.5271\n",
      "Epoch 247/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3133 - mae: 0.3808 - mse: 57.2451 - val_loss: 0.1506 - val_mae: 0.2161 - val_mse: 2.5271\n",
      "Epoch 248/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3133 - mae: 0.3808 - mse: 57.2422 - val_loss: 0.1506 - val_mae: 0.2161 - val_mse: 2.5271\n",
      "Epoch 249/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3133 - mae: 0.3808 - mse: 57.2392 - val_loss: 0.1506 - val_mae: 0.2161 - val_mse: 2.5271\n",
      "Epoch 250/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3132 - mae: 0.3808 - mse: 57.2363 - val_loss: 0.1506 - val_mae: 0.2161 - val_mse: 2.5270\n",
      "Epoch 251/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3132 - mae: 0.3807 - mse: 57.2334 - val_loss: 0.1506 - val_mae: 0.2161 - val_mse: 2.5270\n",
      "Epoch 252/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3132 - mae: 0.3807 - mse: 57.2304 - val_loss: 0.1506 - val_mae: 0.2161 - val_mse: 2.5270\n",
      "Epoch 253/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3132 - mae: 0.3807 - mse: 57.2275 - val_loss: 0.1506 - val_mae: 0.2161 - val_mse: 2.5269\n",
      "Epoch 254/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3132 - mae: 0.3807 - mse: 57.2246 - val_loss: 0.1506 - val_mae: 0.2161 - val_mse: 2.5269\n",
      "Epoch 255/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3132 - mae: 0.3807 - mse: 57.2217 - val_loss: 0.1506 - val_mae: 0.2161 - val_mse: 2.5269\n",
      "Epoch 256/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3132 - mae: 0.3807 - mse: 57.2188 - val_loss: 0.1506 - val_mae: 0.2161 - val_mse: 2.5269\n",
      "Epoch 257/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3132 - mae: 0.3807 - mse: 57.2159 - val_loss: 0.1506 - val_mae: 0.2161 - val_mse: 2.5268\n",
      "Epoch 258/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3131 - mae: 0.3806 - mse: 57.2130 - val_loss: 0.1506 - val_mae: 0.2161 - val_mse: 2.5268\n",
      "Epoch 259/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3131 - mae: 0.3806 - mse: 57.2101 - val_loss: 0.1506 - val_mae: 0.2161 - val_mse: 2.5268\n",
      "Epoch 260/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3131 - mae: 0.3806 - mse: 57.2072 - val_loss: 0.1506 - val_mae: 0.2161 - val_mse: 2.5267\n",
      "Epoch 261/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3131 - mae: 0.3806 - mse: 57.2043 - val_loss: 0.1506 - val_mae: 0.2161 - val_mse: 2.5267\n",
      "Epoch 262/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3131 - mae: 0.3806 - mse: 57.2014 - val_loss: 0.1506 - val_mae: 0.2161 - val_mse: 2.5267\n",
      "Epoch 263/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3131 - mae: 0.3806 - mse: 57.1985 - val_loss: 0.1506 - val_mae: 0.2161 - val_mse: 2.5267\n",
      "Epoch 264/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3131 - mae: 0.3806 - mse: 57.1956 - val_loss: 0.1506 - val_mae: 0.2160 - val_mse: 2.5266\n",
      "Epoch 265/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3131 - mae: 0.3805 - mse: 57.1927 - val_loss: 0.1506 - val_mae: 0.2160 - val_mse: 2.5266\n",
      "Epoch 266/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3131 - mae: 0.3805 - mse: 57.1898 - val_loss: 0.1506 - val_mae: 0.2160 - val_mse: 2.5266\n",
      "Epoch 267/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3130 - mae: 0.3805 - mse: 57.1869 - val_loss: 0.1506 - val_mae: 0.2160 - val_mse: 2.5265\n",
      "Epoch 268/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3130 - mae: 0.3805 - mse: 57.1841 - val_loss: 0.1506 - val_mae: 0.2160 - val_mse: 2.5265\n",
      "Epoch 269/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3130 - mae: 0.3805 - mse: 57.1812 - val_loss: 0.1506 - val_mae: 0.2160 - val_mse: 2.5265\n",
      "Epoch 270/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3130 - mae: 0.3805 - mse: 57.1783 - val_loss: 0.1506 - val_mae: 0.2160 - val_mse: 2.5264\n",
      "Epoch 271/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3130 - mae: 0.3805 - mse: 57.1755 - val_loss: 0.1506 - val_mae: 0.2160 - val_mse: 2.5264\n",
      "Epoch 272/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3130 - mae: 0.3805 - mse: 57.1726 - val_loss: 0.1506 - val_mae: 0.2160 - val_mse: 2.5264\n",
      "Epoch 273/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3130 - mae: 0.3804 - mse: 57.1697 - val_loss: 0.1506 - val_mae: 0.2160 - val_mse: 2.5264\n",
      "Epoch 274/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3130 - mae: 0.3804 - mse: 57.1668 - val_loss: 0.1506 - val_mae: 0.2160 - val_mse: 2.5263\n",
      "Epoch 275/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3130 - mae: 0.3804 - mse: 57.1640 - val_loss: 0.1506 - val_mae: 0.2160 - val_mse: 2.5263\n",
      "Epoch 276/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3129 - mae: 0.3804 - mse: 57.1611 - val_loss: 0.1506 - val_mae: 0.2160 - val_mse: 2.5263\n",
      "Epoch 277/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3129 - mae: 0.3804 - mse: 57.1582 - val_loss: 0.1506 - val_mae: 0.2160 - val_mse: 2.5262\n",
      "Epoch 278/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3129 - mae: 0.3804 - mse: 57.1554 - val_loss: 0.1506 - val_mae: 0.2160 - val_mse: 2.5262\n",
      "Epoch 279/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3129 - mae: 0.3804 - mse: 57.1525 - val_loss: 0.1506 - val_mae: 0.2160 - val_mse: 2.5262\n",
      "Epoch 280/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3129 - mae: 0.3803 - mse: 57.1497 - val_loss: 0.1506 - val_mae: 0.2160 - val_mse: 2.5262\n",
      "Epoch 281/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3129 - mae: 0.3803 - mse: 57.1468 - val_loss: 0.1505 - val_mae: 0.2160 - val_mse: 2.5261\n",
      "Epoch 282/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3129 - mae: 0.3803 - mse: 57.1439 - val_loss: 0.1505 - val_mae: 0.2160 - val_mse: 2.5261\n",
      "Epoch 283/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3129 - mae: 0.3803 - mse: 57.1411 - val_loss: 0.1505 - val_mae: 0.2160 - val_mse: 2.5261\n",
      "Epoch 284/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3128 - mae: 0.3803 - mse: 57.1383 - val_loss: 0.1505 - val_mae: 0.2160 - val_mse: 2.5261\n",
      "Epoch 285/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3128 - mae: 0.3803 - mse: 57.1354 - val_loss: 0.1505 - val_mae: 0.2160 - val_mse: 2.5260\n",
      "Epoch 286/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3128 - mae: 0.3803 - mse: 57.1326 - val_loss: 0.1505 - val_mae: 0.2160 - val_mse: 2.5260\n",
      "Epoch 287/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3128 - mae: 0.3803 - mse: 57.1297 - val_loss: 0.1505 - val_mae: 0.2160 - val_mse: 2.5260\n",
      "Epoch 288/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3128 - mae: 0.3802 - mse: 57.1269 - val_loss: 0.1505 - val_mae: 0.2160 - val_mse: 2.5260\n",
      "Epoch 289/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3128 - mae: 0.3802 - mse: 57.1241 - val_loss: 0.1505 - val_mae: 0.2160 - val_mse: 2.5259\n",
      "Epoch 290/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3128 - mae: 0.3802 - mse: 57.1212 - val_loss: 0.1505 - val_mae: 0.2160 - val_mse: 2.5259\n",
      "Epoch 291/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3128 - mae: 0.3802 - mse: 57.1184 - val_loss: 0.1505 - val_mae: 0.2160 - val_mse: 2.5259\n",
      "Epoch 292/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3128 - mae: 0.3802 - mse: 57.1156 - val_loss: 0.1505 - val_mae: 0.2160 - val_mse: 2.5259\n",
      "Epoch 293/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3127 - mae: 0.3802 - mse: 57.1127 - val_loss: 0.1505 - val_mae: 0.2159 - val_mse: 2.5258\n",
      "Epoch 294/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3127 - mae: 0.3802 - mse: 57.1099 - val_loss: 0.1505 - val_mae: 0.2159 - val_mse: 2.5258\n",
      "Epoch 295/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3127 - mae: 0.3801 - mse: 57.1071 - val_loss: 0.1505 - val_mae: 0.2159 - val_mse: 2.5258\n",
      "Epoch 296/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3127 - mae: 0.3801 - mse: 57.1043 - val_loss: 0.1505 - val_mae: 0.2159 - val_mse: 2.5257\n",
      "Epoch 297/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3127 - mae: 0.3801 - mse: 57.1014 - val_loss: 0.1505 - val_mae: 0.2159 - val_mse: 2.5257\n",
      "Epoch 298/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3127 - mae: 0.3801 - mse: 57.0986 - val_loss: 0.1505 - val_mae: 0.2159 - val_mse: 2.5257\n",
      "Epoch 299/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3127 - mae: 0.3801 - mse: 57.0958 - val_loss: 0.1505 - val_mae: 0.2159 - val_mse: 2.5257\n",
      "Epoch 300/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3127 - mae: 0.3801 - mse: 57.0930 - val_loss: 0.1505 - val_mae: 0.2159 - val_mse: 2.5256\n",
      "Epoch 301/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3127 - mae: 0.3801 - mse: 57.0902 - val_loss: 0.1505 - val_mae: 0.2159 - val_mse: 2.5256\n",
      "Epoch 302/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3126 - mae: 0.3801 - mse: 57.0873 - val_loss: 0.1505 - val_mae: 0.2159 - val_mse: 2.5256\n",
      "Epoch 303/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3126 - mae: 0.3800 - mse: 57.0845 - val_loss: 0.1505 - val_mae: 0.2159 - val_mse: 2.5256\n",
      "Epoch 304/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3126 - mae: 0.3800 - mse: 57.0817 - val_loss: 0.1505 - val_mae: 0.2159 - val_mse: 2.5255\n",
      "Epoch 305/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3126 - mae: 0.3800 - mse: 57.0789 - val_loss: 0.1505 - val_mae: 0.2159 - val_mse: 2.5255\n",
      "Epoch 306/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3126 - mae: 0.3800 - mse: 57.0761 - val_loss: 0.1505 - val_mae: 0.2159 - val_mse: 2.5255\n",
      "Epoch 307/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3126 - mae: 0.3800 - mse: 57.0733 - val_loss: 0.1505 - val_mae: 0.2159 - val_mse: 2.5255\n",
      "Epoch 308/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3126 - mae: 0.3800 - mse: 57.0705 - val_loss: 0.1505 - val_mae: 0.2159 - val_mse: 2.5254\n",
      "Epoch 309/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3126 - mae: 0.3800 - mse: 57.0677 - val_loss: 0.1505 - val_mae: 0.2159 - val_mse: 2.5254\n",
      "Epoch 310/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3125 - mae: 0.3799 - mse: 57.0649 - val_loss: 0.1505 - val_mae: 0.2159 - val_mse: 2.5254\n",
      "Epoch 311/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3125 - mae: 0.3799 - mse: 57.0621 - val_loss: 0.1505 - val_mae: 0.2159 - val_mse: 2.5254\n",
      "Epoch 312/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3125 - mae: 0.3799 - mse: 57.0593 - val_loss: 0.1505 - val_mae: 0.2159 - val_mse: 2.5253\n",
      "Epoch 313/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3125 - mae: 0.3799 - mse: 57.0565 - val_loss: 0.1505 - val_mae: 0.2159 - val_mse: 2.5253\n",
      "Epoch 314/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3125 - mae: 0.3799 - mse: 57.0537 - val_loss: 0.1505 - val_mae: 0.2159 - val_mse: 2.5253\n",
      "Epoch 315/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3125 - mae: 0.3799 - mse: 57.0509 - val_loss: 0.1505 - val_mae: 0.2159 - val_mse: 2.5253\n",
      "Epoch 316/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3125 - mae: 0.3799 - mse: 57.0481 - val_loss: 0.1505 - val_mae: 0.2159 - val_mse: 2.5252\n",
      "Epoch 317/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3125 - mae: 0.3799 - mse: 57.0453 - val_loss: 0.1505 - val_mae: 0.2159 - val_mse: 2.5252\n",
      "Epoch 318/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3125 - mae: 0.3798 - mse: 57.0425 - val_loss: 0.1505 - val_mae: 0.2159 - val_mse: 2.5252\n",
      "Epoch 319/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3124 - mae: 0.3798 - mse: 57.0398 - val_loss: 0.1505 - val_mae: 0.2159 - val_mse: 2.5252\n",
      "Epoch 320/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3124 - mae: 0.3798 - mse: 57.0370 - val_loss: 0.1505 - val_mae: 0.2159 - val_mse: 2.5251\n",
      "Epoch 321/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3124 - mae: 0.3798 - mse: 57.0342 - val_loss: 0.1505 - val_mae: 0.2159 - val_mse: 2.5251\n",
      "Epoch 322/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3124 - mae: 0.3798 - mse: 57.0314 - val_loss: 0.1505 - val_mae: 0.2159 - val_mse: 2.5251\n",
      "Epoch 323/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3124 - mae: 0.3798 - mse: 57.0286 - val_loss: 0.1505 - val_mae: 0.2158 - val_mse: 2.5251\n",
      "Epoch 324/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3124 - mae: 0.3798 - mse: 57.0259 - val_loss: 0.1505 - val_mae: 0.2158 - val_mse: 2.5250\n",
      "Epoch 325/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3124 - mae: 0.3797 - mse: 57.0231 - val_loss: 0.1505 - val_mae: 0.2158 - val_mse: 2.5250\n",
      "Epoch 326/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3124 - mae: 0.3797 - mse: 57.0203 - val_loss: 0.1505 - val_mae: 0.2158 - val_mse: 2.5250\n",
      "Epoch 327/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3124 - mae: 0.3797 - mse: 57.0175 - val_loss: 0.1505 - val_mae: 0.2158 - val_mse: 2.5250\n",
      "Epoch 328/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3123 - mae: 0.3797 - mse: 57.0147 - val_loss: 0.1505 - val_mae: 0.2158 - val_mse: 2.5249\n",
      "Epoch 329/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3123 - mae: 0.3797 - mse: 57.0120 - val_loss: 0.1505 - val_mae: 0.2158 - val_mse: 2.5249\n",
      "Epoch 330/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3123 - mae: 0.3797 - mse: 57.0092 - val_loss: 0.1505 - val_mae: 0.2158 - val_mse: 2.5249\n",
      "Epoch 331/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3123 - mae: 0.3797 - mse: 57.0064 - val_loss: 0.1505 - val_mae: 0.2158 - val_mse: 2.5249\n",
      "Epoch 332/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3123 - mae: 0.3797 - mse: 57.0037 - val_loss: 0.1505 - val_mae: 0.2158 - val_mse: 2.5248\n",
      "Epoch 333/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3123 - mae: 0.3796 - mse: 57.0009 - val_loss: 0.1505 - val_mae: 0.2158 - val_mse: 2.5248\n",
      "Epoch 334/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3123 - mae: 0.3796 - mse: 56.9981 - val_loss: 0.1505 - val_mae: 0.2158 - val_mse: 2.5248\n",
      "Epoch 335/10000\n",
      "1294/1294 [==============================] - 8s 7ms/step - loss: 0.3123 - mae: 0.3796 - mse: 56.9953 - val_loss: 0.1505 - val_mae: 0.2158 - val_mse: 2.5248\n",
      "Epoch 336/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3123 - mae: 0.3796 - mse: 56.9925 - val_loss: 0.1505 - val_mae: 0.2158 - val_mse: 2.5247\n",
      "Epoch 337/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3122 - mae: 0.3796 - mse: 56.9898 - val_loss: 0.1505 - val_mae: 0.2158 - val_mse: 2.5247\n",
      "Epoch 338/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3122 - mae: 0.3796 - mse: 56.9870 - val_loss: 0.1505 - val_mae: 0.2158 - val_mse: 2.5247\n",
      "Epoch 339/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3122 - mae: 0.3796 - mse: 56.9842 - val_loss: 0.1505 - val_mae: 0.2158 - val_mse: 2.5247\n",
      "Epoch 340/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3122 - mae: 0.3796 - mse: 56.9814 - val_loss: 0.1505 - val_mae: 0.2158 - val_mse: 2.5246\n",
      "Epoch 341/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3122 - mae: 0.3795 - mse: 56.9787 - val_loss: 0.1505 - val_mae: 0.2158 - val_mse: 2.5246\n",
      "Epoch 342/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3122 - mae: 0.3795 - mse: 56.9759 - val_loss: 0.1505 - val_mae: 0.2158 - val_mse: 2.5246\n",
      "Epoch 343/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3122 - mae: 0.3795 - mse: 56.9732 - val_loss: 0.1505 - val_mae: 0.2158 - val_mse: 2.5246\n",
      "Epoch 344/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3122 - mae: 0.3795 - mse: 56.9704 - val_loss: 0.1505 - val_mae: 0.2158 - val_mse: 2.5246\n",
      "Epoch 345/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3121 - mae: 0.3795 - mse: 56.9676 - val_loss: 0.1505 - val_mae: 0.2158 - val_mse: 2.5245\n",
      "Epoch 346/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3121 - mae: 0.3795 - mse: 56.9649 - val_loss: 0.1505 - val_mae: 0.2158 - val_mse: 2.5245\n",
      "Epoch 347/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3121 - mae: 0.3795 - mse: 56.9621 - val_loss: 0.1505 - val_mae: 0.2158 - val_mse: 2.5245\n",
      "Epoch 348/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3121 - mae: 0.3794 - mse: 56.9593 - val_loss: 0.1505 - val_mae: 0.2158 - val_mse: 2.5245\n",
      "Epoch 349/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3121 - mae: 0.3794 - mse: 56.9566 - val_loss: 0.1505 - val_mae: 0.2158 - val_mse: 2.5244\n",
      "Epoch 350/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3121 - mae: 0.3794 - mse: 56.9538 - val_loss: 0.1505 - val_mae: 0.2158 - val_mse: 2.5244\n",
      "Epoch 351/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3121 - mae: 0.3794 - mse: 56.9511 - val_loss: 0.1505 - val_mae: 0.2158 - val_mse: 2.5244\n",
      "Epoch 352/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3121 - mae: 0.3794 - mse: 56.9483 - val_loss: 0.1505 - val_mae: 0.2158 - val_mse: 2.5244\n",
      "Epoch 353/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3121 - mae: 0.3794 - mse: 56.9456 - val_loss: 0.1505 - val_mae: 0.2158 - val_mse: 2.5244\n",
      "Epoch 354/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3120 - mae: 0.3794 - mse: 56.9428 - val_loss: 0.1505 - val_mae: 0.2158 - val_mse: 2.5243\n",
      "Epoch 355/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3120 - mae: 0.3794 - mse: 56.9401 - val_loss: 0.1505 - val_mae: 0.2158 - val_mse: 2.5243\n",
      "Epoch 356/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3120 - mae: 0.3793 - mse: 56.9373 - val_loss: 0.1505 - val_mae: 0.2158 - val_mse: 2.5243\n",
      "Epoch 357/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3120 - mae: 0.3793 - mse: 56.9346 - val_loss: 0.1505 - val_mae: 0.2157 - val_mse: 2.5243\n",
      "Epoch 358/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3120 - mae: 0.3793 - mse: 56.9318 - val_loss: 0.1505 - val_mae: 0.2157 - val_mse: 2.5242\n",
      "Epoch 359/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3120 - mae: 0.3793 - mse: 56.9291 - val_loss: 0.1505 - val_mae: 0.2157 - val_mse: 2.5242\n",
      "Epoch 360/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3120 - mae: 0.3793 - mse: 56.9263 - val_loss: 0.1505 - val_mae: 0.2157 - val_mse: 2.5242\n",
      "Epoch 361/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3120 - mae: 0.3793 - mse: 56.9236 - val_loss: 0.1505 - val_mae: 0.2157 - val_mse: 2.5242\n",
      "Epoch 362/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3120 - mae: 0.3793 - mse: 56.9208 - val_loss: 0.1505 - val_mae: 0.2157 - val_mse: 2.5242\n",
      "Epoch 363/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3119 - mae: 0.3793 - mse: 56.9181 - val_loss: 0.1505 - val_mae: 0.2157 - val_mse: 2.5241\n",
      "Epoch 364/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3119 - mae: 0.3792 - mse: 56.9153 - val_loss: 0.1505 - val_mae: 0.2157 - val_mse: 2.5241\n",
      "Epoch 365/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3119 - mae: 0.3792 - mse: 56.9126 - val_loss: 0.1505 - val_mae: 0.2157 - val_mse: 2.5241\n",
      "Epoch 366/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3119 - mae: 0.3792 - mse: 56.9099 - val_loss: 0.1505 - val_mae: 0.2157 - val_mse: 2.5241\n",
      "Epoch 367/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3119 - mae: 0.3792 - mse: 56.9071 - val_loss: 0.1504 - val_mae: 0.2157 - val_mse: 2.5241\n",
      "Epoch 368/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3119 - mae: 0.3792 - mse: 56.9044 - val_loss: 0.1504 - val_mae: 0.2157 - val_mse: 2.5240\n",
      "Epoch 369/10000\n",
      " 644/1294 [=============>................] - ETA: 3s - loss: 0.2215 - mae: 0.2813 - mse: 26.5140"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "sY9EgNpOSp8s"
   },
   "source": [
    "c = 'Autauga'\n",
    "s = 'AL'\n",
    "r = df[(df['state']==s)&(df['county']==c)]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "lCSQPQbsSp87"
   },
   "source": [
    "model4 = tf.keras.models.load_model('checkpoint4.h5')\n",
    "row = r.drop(columns=['state','county'])\n",
    "row = row.to_numpy()\n",
    "dataset = getDataset(row,2,False)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "IKZQWaW-Sp9N",
    "outputId": "a201a597-bade-49d6-fd9b-230df224ce05",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    }
   },
   "source": [
    "results = model4.predict(dataset)\n",
    "print(results)"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "[[[0.2350704 ]\n",
      "  [0.31665772]\n",
      "  [0.29744172]\n",
      "  [0.52478254]]\n",
      "\n",
      " [[0.6701347 ]\n",
      "  [0.8887835 ]\n",
      "  [0.8544977 ]\n",
      "  [1.0203351 ]]\n",
      "\n",
      " [[0.38993528]\n",
      "  [0.7494613 ]\n",
      "  [0.752346  ]\n",
      "  [2.107971  ]]]\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "J-2cK6uCSp9k",
    "outputId": "73307db3-2e1e-4dd5-ff6a-62ae54c0caa9",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    }
   },
   "source": [
    "dem = np.array(r.values.tolist()[0][2::3])\n",
    "gop = np.array(r.values.tolist()[0][3::3])\n",
    "other = np.array(r.values.tolist()[0][4::3])\n",
    "plotCounty(s,c,dem,gop,other,results)"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'plotCounty' is not defined",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-17-65842ac2c122>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[0mgop\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0marray\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mr\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtolist\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m3\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;36m3\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[0mother\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0marray\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mr\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtolist\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m4\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;36m3\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 4\u001B[1;33m \u001B[0mplotCounty\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0ms\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mc\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mdem\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mgop\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mother\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mresults\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m: name 'plotCounty' is not defined"
     ]
    }
   ]
  }
 ]
}