{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "colab": {
   "name": "predictCounty.ipynb",
   "provenance": []
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "CsluoFv7Sp2j"
   },
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "tlcuuCFhSp3G"
   },
   "source": [
    "years = range(2000,2020,4)\n",
    "df = pd.read_csv('counties.csv')"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "PrR6Do9iSp3f",
    "outputId": "8ac723de-d0f5-4e58-d36c-5ebb3373e731",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    }
   },
   "source": [
    "data = df.drop(columns=['state', 'county'])\n",
    "data = data.dropna(axis=0,how='any')\n",
    "data.describe()"
   ],
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "              2000D          2000R          2000O         2004D         2004R  \\\ncount  2.875000e+03    2875.000000    2875.000000  2.875000e+03  2.875000e+03   \nmean   1.699424e+04   16533.856000    1352.073391  1.956915e+04  2.028555e+04   \nstd    5.942532e+04   39923.542134    4232.141433  6.769202e+04  4.869764e+04   \nmin    1.400000e+01     106.000000       3.000000  1.200000e+01  6.500000e+01   \n25%    1.751500e+03    2689.000000     109.000000  1.882500e+03  3.124500e+03   \n50%    3.830000e+03    5363.000000     278.000000  4.177000e+03  6.522000e+03   \n75%    9.580500e+03   13166.000000     886.500000  1.078350e+04  1.646800e+04   \nmax    1.710505e+06  871930.000000  112719.000000  1.907736e+06  1.076225e+06   \n\n              2004O         2008D          2008R         2008O         2012D  \\\ncount   2875.000000  2.875000e+03    2875.000000   2875.000000  2.875000e+03   \nmean     508.671652  2.298134e+04   19539.632696    610.991304  2.171365e+04   \nstd     1976.171276  7.944593e+04   45955.629730   1913.652362  7.628626e+04   \nmin        0.000000  8.000000e+00      67.000000      0.000000  5.000000e+00   \n25%       43.000000  1.930500e+03    2956.000000     78.500000  1.662500e+03   \n50%       98.000000  4.493000e+03    6297.000000    182.000000  4.014000e+03   \n75%      302.000000  1.262450e+04   16042.000000    453.500000  1.147700e+04   \nmax    39515.000000  2.295853e+06  956425.000000  65970.000000  2.216903e+06   \n\n               2012R         2012O         2016D          2016R          2016O  \ncount    2875.000000   2875.000000  2.875000e+03    2875.000000    2875.000000  \nmean    19761.090783    886.516522  2.175607e+04   20524.178783    2543.772522  \nstd     45649.528293   3056.034805  8.298175e+04   44262.493478    7726.198627  \nmin        54.000000      1.000000  4.000000e+00      58.000000       3.000000  \n25%      3021.500000     84.000000  1.240000e+03    3346.000000     195.000000  \n50%      6398.000000    195.000000  3.199000e+03    7194.000000     529.000000  \n75%     16391.500000    564.000000  9.940500e+03   17945.500000    1736.500000  \nmax    885333.000000  78831.000000  2.464364e+06  769743.000000  200201.000000  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>2000D</th>\n      <th>2000R</th>\n      <th>2000O</th>\n      <th>2004D</th>\n      <th>2004R</th>\n      <th>2004O</th>\n      <th>2008D</th>\n      <th>2008R</th>\n      <th>2008O</th>\n      <th>2012D</th>\n      <th>2012R</th>\n      <th>2012O</th>\n      <th>2016D</th>\n      <th>2016R</th>\n      <th>2016O</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>2.875000e+03</td>\n      <td>2875.000000</td>\n      <td>2875.000000</td>\n      <td>2.875000e+03</td>\n      <td>2.875000e+03</td>\n      <td>2875.000000</td>\n      <td>2.875000e+03</td>\n      <td>2875.000000</td>\n      <td>2875.000000</td>\n      <td>2.875000e+03</td>\n      <td>2875.000000</td>\n      <td>2875.000000</td>\n      <td>2.875000e+03</td>\n      <td>2875.000000</td>\n      <td>2875.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>1.699424e+04</td>\n      <td>16533.856000</td>\n      <td>1352.073391</td>\n      <td>1.956915e+04</td>\n      <td>2.028555e+04</td>\n      <td>508.671652</td>\n      <td>2.298134e+04</td>\n      <td>19539.632696</td>\n      <td>610.991304</td>\n      <td>2.171365e+04</td>\n      <td>19761.090783</td>\n      <td>886.516522</td>\n      <td>2.175607e+04</td>\n      <td>20524.178783</td>\n      <td>2543.772522</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>5.942532e+04</td>\n      <td>39923.542134</td>\n      <td>4232.141433</td>\n      <td>6.769202e+04</td>\n      <td>4.869764e+04</td>\n      <td>1976.171276</td>\n      <td>7.944593e+04</td>\n      <td>45955.629730</td>\n      <td>1913.652362</td>\n      <td>7.628626e+04</td>\n      <td>45649.528293</td>\n      <td>3056.034805</td>\n      <td>8.298175e+04</td>\n      <td>44262.493478</td>\n      <td>7726.198627</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>1.400000e+01</td>\n      <td>106.000000</td>\n      <td>3.000000</td>\n      <td>1.200000e+01</td>\n      <td>6.500000e+01</td>\n      <td>0.000000</td>\n      <td>8.000000e+00</td>\n      <td>67.000000</td>\n      <td>0.000000</td>\n      <td>5.000000e+00</td>\n      <td>54.000000</td>\n      <td>1.000000</td>\n      <td>4.000000e+00</td>\n      <td>58.000000</td>\n      <td>3.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>1.751500e+03</td>\n      <td>2689.000000</td>\n      <td>109.000000</td>\n      <td>1.882500e+03</td>\n      <td>3.124500e+03</td>\n      <td>43.000000</td>\n      <td>1.930500e+03</td>\n      <td>2956.000000</td>\n      <td>78.500000</td>\n      <td>1.662500e+03</td>\n      <td>3021.500000</td>\n      <td>84.000000</td>\n      <td>1.240000e+03</td>\n      <td>3346.000000</td>\n      <td>195.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>3.830000e+03</td>\n      <td>5363.000000</td>\n      <td>278.000000</td>\n      <td>4.177000e+03</td>\n      <td>6.522000e+03</td>\n      <td>98.000000</td>\n      <td>4.493000e+03</td>\n      <td>6297.000000</td>\n      <td>182.000000</td>\n      <td>4.014000e+03</td>\n      <td>6398.000000</td>\n      <td>195.000000</td>\n      <td>3.199000e+03</td>\n      <td>7194.000000</td>\n      <td>529.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>9.580500e+03</td>\n      <td>13166.000000</td>\n      <td>886.500000</td>\n      <td>1.078350e+04</td>\n      <td>1.646800e+04</td>\n      <td>302.000000</td>\n      <td>1.262450e+04</td>\n      <td>16042.000000</td>\n      <td>453.500000</td>\n      <td>1.147700e+04</td>\n      <td>16391.500000</td>\n      <td>564.000000</td>\n      <td>9.940500e+03</td>\n      <td>17945.500000</td>\n      <td>1736.500000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>1.710505e+06</td>\n      <td>871930.000000</td>\n      <td>112719.000000</td>\n      <td>1.907736e+06</td>\n      <td>1.076225e+06</td>\n      <td>39515.000000</td>\n      <td>2.295853e+06</td>\n      <td>956425.000000</td>\n      <td>65970.000000</td>\n      <td>2.216903e+06</td>\n      <td>885333.000000</td>\n      <td>78831.000000</td>\n      <td>2.464364e+06</td>\n      <td>769743.000000</td>\n      <td>200201.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "0-LVluzeSp4P"
   },
   "source": [
    "data = data.to_numpy(dtype=np.int64)"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "YMNKgPOuSp5D"
   },
   "source": [
    "def plotCounty(state,county,d,r,o,predicted=None):\n",
    "    years = range(2000,2020,4)\n",
    "    plt.figure()\n",
    "    plt.plot(years,d,'b',label='Democrat')\n",
    "    plt.plot(years,r,'r',label='Republican')\n",
    "    plt.plot(years,o,'y',label='Other')\n",
    "    plt.plot(years,d+r+o,'k',label='Total')\n",
    "    if predicted is not None:\n",
    "        new_years = range(2008,2024,4)\n",
    "        plt.plot(new_years,predicted[0]*20000,'c',label='Democrat Model')\n",
    "        plt.plot(new_years,predicted[1]*20000,'m',label='Republican Model')\n",
    "        plt.plot(new_years,predicted[2]*250,'g',label='Other Model')\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Votes')\n",
    "    plt.title(county+', '+state)\n",
    "    plt.xticks(years)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ],
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "DJh6SpbjSp5s"
   },
   "source": [
    "def calcState(state):\n",
    "    state_df = df[df['state']==state]\n",
    "    state_df = state_df.drop(columns=['state','county'])\n",
    "    state_votes = state_df.values.tolist()\n",
    "    state_votes = np.array(state_votes)\n",
    "    state_votes = np.sum(state_votes,axis=0,dtype=np.int64)\n",
    "    d = []\n",
    "    r = []\n",
    "    o = []\n",
    "    for ind, val in enumerate(state_votes):\n",
    "        party = ind % 3\n",
    "        if party == 0:\n",
    "            d.append(val)\n",
    "        elif party == 1:\n",
    "            r.append(val)\n",
    "        else:\n",
    "            o.append(val)\n",
    "    d = np.array(d)\n",
    "    r = np.array(r)\n",
    "    o = np.array(o)\n",
    "    return d,r,o"
   ],
   "execution_count": 25,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "Ir1pXANGSp6F"
   },
   "source": [
    "def plotState(state,d,r,o):\n",
    "    years = range(2000,2020,4)\n",
    "    plt.figure()\n",
    "    plt.plot(years,d,'b',label='Democrat')\n",
    "    plt.plot(years,r,'r',label='Republican')\n",
    "    plt.plot(years,o,'y',label='Other')\n",
    "    plt.plot(years,d+r+o,'k',label='Total')\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Votes')\n",
    "    plt.title(state)\n",
    "    plt.xticks(years)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ],
   "execution_count": 26,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "BhlfhjBISp6o"
   },
   "source": [
    "def convertbyParty(point):\n",
    "    d = point[:,::3]/20000\n",
    "    r = point[:,1::3]/20000\n",
    "    o = point[:,2::3]/250\n",
    "    return d,r,o\n",
    "print(df.loc[0])\n",
    "convertbyParty(np.expand_dims(df.loc[0].values[2:],0))"
   ],
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state          AL\n",
      "county    Autauga\n",
      "2000D        4942\n",
      "2000R       11993\n",
      "2000O         273\n",
      "2004D        4758\n",
      "2004R       15196\n",
      "2004O         127\n",
      "2008D        6093\n",
      "2008R       17403\n",
      "2008O         145\n",
      "2012D        6363\n",
      "2012R       17379\n",
      "2012O         190\n",
      "2016D        5936\n",
      "2016R       18172\n",
      "2016O         865\n",
      "Name: 0, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": "(array([[0.2471, 0.2379, 0.30465, 0.31815, 0.2968]], dtype=object),\n array([[0.59965, 0.7598, 0.87015, 0.86895, 0.9086]], dtype=object),\n array([[1.092, 0.508, 0.58, 0.76, 3.46]], dtype=object))"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "sKpS8inlSp7A"
   },
   "source": [
    "def getDataset(data,batch_size,split=True):\n",
    "    data = np.expand_dims(data,axis=-1)\n",
    "    if split:\n",
    "        np.random.shuffle(data)\n",
    "    num = data.shape[0]\n",
    "    train_data = data[:int(num*0.6)]\n",
    "    val_data = data[int(num*0.6):int(num*0.8)]\n",
    "    test_data = data[int(num*0.8):]\n",
    "    def createDS(point,label=True):\n",
    "        d,r,o=convertbyParty(point)\n",
    "        new_data = np.vstack((d,r,o))\n",
    "        if label:\n",
    "            new_data = np.vstack((new_data,new_data*1.5))\n",
    "        ds = tf.data.Dataset.from_tensor_slices(new_data)\n",
    "        if label:\n",
    "            ds = ds.map(lambda p: (p[:-1],p[1:]))\n",
    "        else:\n",
    "            ds = ds.map(lambda p: p[1:])\n",
    "        return ds.batch(batch_size).prefetch(1) if label else ds\n",
    "    if split:\n",
    "        return createDS(train_data),createDS(val_data),createDS(test_data)\n",
    "    else:\n",
    "        return createDS(data,False).batch(100000)"
   ],
   "execution_count": 78,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "FuRS8y-8Sp7O",
    "outputId": "2bffa668-6a35-475d-863b-981d69b4ba82",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    }
   },
   "source": [
    "train_ds, val_ds, test_ds = getDataset(data,8)\n",
    "# list(getDataset(data,1,False))[:3]\n"
   ],
   "execution_count": 46,
   "outputs": [
    {
     "data": {
      "text/plain": "[<tf.Tensor: shape=(1, 4, 1), dtype=float64, numpy=\n array([[[0.25355],\n         [0.2673 ],\n         [0.3604 ],\n         [0.28895]]])>,\n <tf.Tensor: shape=(1, 4, 1), dtype=float64, numpy=\n array([[[0.0241],\n         [0.0204],\n         [0.0218],\n         [0.0162]]])>,\n <tf.Tensor: shape=(1, 4, 1), dtype=float64, numpy=\n array([[[0.14165],\n         [0.14605],\n         [0.1328 ],\n         [0.11325]]])>]"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "bHcc7PIxSp7h"
   },
   "source": [
    "# model = tf.keras.Sequential([\n",
    "#     tf.keras.layers.Conv1D(2,2,activation='relu',input_shape=(4,1)),\n",
    "#     tf.keras.layers.Conv1D(2,2,activation='relu'),\n",
    "#     tf.keras.layers.Flatten(),\n",
    "#     tf.keras.layers.Dense(4,activation='relu'),\n",
    "#     tf.keras.layers.Reshape((4,1)),\n",
    "#     tf.keras.layers.LSTM(8,return_sequences=True),\n",
    "#     tf.keras.layers.LSTM(8,return_sequences=True),\n",
    "#     tf.keras.layers.LSTM(8,return_sequences=True),\n",
    "# ])"
   ],
   "execution_count": 26,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "bKa-U7wESp7w"
   },
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv1D(8,2,input_shape=(4,1),activation='relu'),\n",
    "    tf.keras.layers.Conv1D(8,2,activation='relu'),\n",
    "    tf.keras.layers.LSTM(12,return_sequences=True),\n",
    "    tf.keras.layers.LSTM(12,return_sequences=True),\n",
    "    tf.keras.layers.LSTM(12,return_sequences=True),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(4),\n",
    "    tf.keras.layers.Reshape((4,1))\n",
    "])"
   ],
   "execution_count": 35,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "Sm_qpTzDSp7-",
    "outputId": "ad6aeea9-2d7d-4787-aa25-e7da61f89351",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    }
   },
   "source": [
    "lr_schedule = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-6 * 10**(epoch / 20))\n",
    "optimizer = tf.keras.optimizers.Adam(lr=1e-3)\n",
    "model.compile(loss=tf.keras.losses.Huber(),optimizer=optimizer,metrics=['mae','mse'])\n",
    "model.summary()"
   ],
   "execution_count": 36,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_7 (Conv1D)            (None, 3, 4)              12        \n",
      "_________________________________________________________________\n",
      "lstm_11 (LSTM)               (None, 3, 8)              416       \n",
      "_________________________________________________________________\n",
      "lstm_12 (LSTM)               (None, 3, 16)             1600      \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 48)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 4)                 196       \n",
      "_________________________________________________________________\n",
      "reshape_4 (Reshape)          (None, 4, 1)              0         \n",
      "=================================================================\n",
      "Total params: 2,224\n",
      "Trainable params: 2,224\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "SR-cfHLeSp8N"
   },
   "source": [
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\"checkpoint4.h5\", save_best_only=True)\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=125)\n"
   ],
   "execution_count": 37,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "lmf1QmWbSp8c",
    "outputId": "bd60f37b-94ab-4e53-f488-2f319d256fa6",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    }
   },
   "source": [
    "# history = model.fit(train_ds,epochs=100,validation_data=val_ds,callbacks=[lr_schedule])\n",
    "# plt.semilogx(history.history[\"lr\"], history.history[\"loss\"])\n",
    "# plt.axis([1e-6, 10, 0, 3])\n",
    "def getlr(epoch):\n",
    "  return 1e-5\n",
    "new_lr = tf.keras.callbacks.LearningRateScheduler(getlr)\n",
    "model.fit(train_ds,epochs=10000,validation_data=val_ds,callbacks=[model_checkpoint,early_stopping,new_lr])"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Epoch 1/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3164 - mae: 0.3858 - mse: 57.9642 - val_loss: 0.1509 - val_mae: 0.2194 - val_mse: 2.5308\n",
      "Epoch 2/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3164 - mae: 0.3850 - mse: 57.9582 - val_loss: 0.1508 - val_mae: 0.2187 - val_mse: 2.5299\n",
      "Epoch 3/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3163 - mae: 0.3848 - mse: 57.9619 - val_loss: 0.1508 - val_mae: 0.2184 - val_mse: 2.5325\n",
      "Epoch 4/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3163 - mae: 0.3847 - mse: 57.9619 - val_loss: 0.1508 - val_mae: 0.2182 - val_mse: 2.5324\n",
      "Epoch 5/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3163 - mae: 0.3846 - mse: 57.9605 - val_loss: 0.1508 - val_mae: 0.2181 - val_mse: 2.5328\n",
      "Epoch 6/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3162 - mae: 0.3845 - mse: 57.9582 - val_loss: 0.1508 - val_mae: 0.2180 - val_mse: 2.5327\n",
      "Epoch 7/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3162 - mae: 0.3845 - mse: 57.9558 - val_loss: 0.1508 - val_mae: 0.2179 - val_mse: 2.5329\n",
      "Epoch 8/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3162 - mae: 0.3845 - mse: 57.9531 - val_loss: 0.1508 - val_mae: 0.2178 - val_mse: 2.5328\n",
      "Epoch 9/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3162 - mae: 0.3844 - mse: 57.9504 - val_loss: 0.1508 - val_mae: 0.2178 - val_mse: 2.5330\n",
      "Epoch 10/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3162 - mae: 0.3844 - mse: 57.9475 - val_loss: 0.1508 - val_mae: 0.2177 - val_mse: 2.5330\n",
      "Epoch 11/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3162 - mae: 0.3844 - mse: 57.9447 - val_loss: 0.1508 - val_mae: 0.2177 - val_mse: 2.5330\n",
      "Epoch 12/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3162 - mae: 0.3843 - mse: 57.9418 - val_loss: 0.1508 - val_mae: 0.2177 - val_mse: 2.5330\n",
      "Epoch 13/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3161 - mae: 0.3843 - mse: 57.9388 - val_loss: 0.1508 - val_mae: 0.2176 - val_mse: 2.5331\n",
      "Epoch 14/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3161 - mae: 0.3843 - mse: 57.9359 - val_loss: 0.1508 - val_mae: 0.2176 - val_mse: 2.5331\n",
      "Epoch 15/10000\n",
      "1294/1294 [==============================] - 7s 6ms/step - loss: 0.3161 - mae: 0.3843 - mse: 57.9329 - val_loss: 0.1508 - val_mae: 0.2176 - val_mse: 2.5331\n",
      "Epoch 16/10000\n",
      "1294/1294 [==============================] - 7s 6ms/step - loss: 0.3161 - mae: 0.3842 - mse: 57.9300 - val_loss: 0.1508 - val_mae: 0.2176 - val_mse: 2.5332\n",
      "Epoch 17/10000\n",
      "1294/1294 [==============================] - 7s 6ms/step - loss: 0.3161 - mae: 0.3842 - mse: 57.9270 - val_loss: 0.1508 - val_mae: 0.2175 - val_mse: 2.5332\n",
      "Epoch 18/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3161 - mae: 0.3842 - mse: 57.9240 - val_loss: 0.1508 - val_mae: 0.2175 - val_mse: 2.5332\n",
      "Epoch 19/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3161 - mae: 0.3842 - mse: 57.9210 - val_loss: 0.1508 - val_mae: 0.2175 - val_mse: 2.5332\n",
      "Epoch 20/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3161 - mae: 0.3842 - mse: 57.9180 - val_loss: 0.1508 - val_mae: 0.2175 - val_mse: 2.5332\n",
      "Epoch 21/10000\n",
      "1294/1294 [==============================] - 7s 6ms/step - loss: 0.3160 - mae: 0.3841 - mse: 57.9150 - val_loss: 0.1508 - val_mae: 0.2174 - val_mse: 2.5332\n",
      "Epoch 22/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3160 - mae: 0.3841 - mse: 57.9120 - val_loss: 0.1508 - val_mae: 0.2174 - val_mse: 2.5332\n",
      "Epoch 23/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3160 - mae: 0.3841 - mse: 57.9090 - val_loss: 0.1508 - val_mae: 0.2174 - val_mse: 2.5332\n",
      "Epoch 24/10000\n",
      "1294/1294 [==============================] - 7s 6ms/step - loss: 0.3160 - mae: 0.3841 - mse: 57.9059 - val_loss: 0.1508 - val_mae: 0.2174 - val_mse: 2.5331\n",
      "Epoch 25/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3160 - mae: 0.3841 - mse: 57.9029 - val_loss: 0.1508 - val_mae: 0.2174 - val_mse: 2.5331\n",
      "Epoch 26/10000\n",
      "1294/1294 [==============================] - 7s 6ms/step - loss: 0.3160 - mae: 0.3840 - mse: 57.8999 - val_loss: 0.1508 - val_mae: 0.2174 - val_mse: 2.5331\n",
      "Epoch 27/10000\n",
      "1294/1294 [==============================] - 7s 6ms/step - loss: 0.3160 - mae: 0.3840 - mse: 57.8968 - val_loss: 0.1508 - val_mae: 0.2173 - val_mse: 2.5331\n",
      "Epoch 28/10000\n",
      "1294/1294 [==============================] - 7s 6ms/step - loss: 0.3159 - mae: 0.3840 - mse: 57.8938 - val_loss: 0.1508 - val_mae: 0.2173 - val_mse: 2.5331\n",
      "Epoch 29/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3159 - mae: 0.3840 - mse: 57.8907 - val_loss: 0.1508 - val_mae: 0.2173 - val_mse: 2.5330\n",
      "Epoch 30/10000\n",
      "1294/1294 [==============================] - 7s 6ms/step - loss: 0.3159 - mae: 0.3840 - mse: 57.8877 - val_loss: 0.1508 - val_mae: 0.2173 - val_mse: 2.5330\n",
      "Epoch 31/10000\n",
      "1294/1294 [==============================] - 7s 6ms/step - loss: 0.3159 - mae: 0.3839 - mse: 57.8847 - val_loss: 0.1508 - val_mae: 0.2173 - val_mse: 2.5330\n",
      "Epoch 32/10000\n",
      "1294/1294 [==============================] - 7s 6ms/step - loss: 0.3159 - mae: 0.3839 - mse: 57.8816 - val_loss: 0.1508 - val_mae: 0.2173 - val_mse: 2.5330\n",
      "Epoch 33/10000\n",
      "1294/1294 [==============================] - 7s 6ms/step - loss: 0.3159 - mae: 0.3839 - mse: 57.8786 - val_loss: 0.1508 - val_mae: 0.2173 - val_mse: 2.5329\n",
      "Epoch 34/10000\n",
      "1294/1294 [==============================] - 7s 6ms/step - loss: 0.3159 - mae: 0.3839 - mse: 57.8756 - val_loss: 0.1508 - val_mae: 0.2172 - val_mse: 2.5329\n",
      "Epoch 35/10000\n",
      "1294/1294 [==============================] - 7s 6ms/step - loss: 0.3159 - mae: 0.3839 - mse: 57.8725 - val_loss: 0.1508 - val_mae: 0.2172 - val_mse: 2.5329\n",
      "Epoch 36/10000\n",
      "1294/1294 [==============================] - 7s 6ms/step - loss: 0.3158 - mae: 0.3839 - mse: 57.8695 - val_loss: 0.1508 - val_mae: 0.2172 - val_mse: 2.5329\n",
      "Epoch 37/10000\n",
      "1294/1294 [==============================] - 7s 6ms/step - loss: 0.3158 - mae: 0.3838 - mse: 57.8664 - val_loss: 0.1508 - val_mae: 0.2172 - val_mse: 2.5328\n",
      "Epoch 38/10000\n",
      "1294/1294 [==============================] - 7s 6ms/step - loss: 0.3158 - mae: 0.3838 - mse: 57.8634 - val_loss: 0.1508 - val_mae: 0.2172 - val_mse: 2.5328\n",
      "Epoch 39/10000\n",
      "1294/1294 [==============================] - 7s 6ms/step - loss: 0.3158 - mae: 0.3838 - mse: 57.8603 - val_loss: 0.1508 - val_mae: 0.2172 - val_mse: 2.5328\n",
      "Epoch 40/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3158 - mae: 0.3838 - mse: 57.8573 - val_loss: 0.1508 - val_mae: 0.2172 - val_mse: 2.5328\n",
      "Epoch 41/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3158 - mae: 0.3838 - mse: 57.8543 - val_loss: 0.1508 - val_mae: 0.2172 - val_mse: 2.5327\n",
      "Epoch 42/10000\n",
      "1294/1294 [==============================] - 7s 6ms/step - loss: 0.3158 - mae: 0.3838 - mse: 57.8513 - val_loss: 0.1508 - val_mae: 0.2172 - val_mse: 2.5327\n",
      "Epoch 43/10000\n",
      "1294/1294 [==============================] - 7s 6ms/step - loss: 0.3158 - mae: 0.3837 - mse: 57.8482 - val_loss: 0.1508 - val_mae: 0.2171 - val_mse: 2.5327\n",
      "Epoch 44/10000\n",
      "1294/1294 [==============================] - 7s 6ms/step - loss: 0.3157 - mae: 0.3837 - mse: 57.8452 - val_loss: 0.1508 - val_mae: 0.2171 - val_mse: 2.5327\n",
      "Epoch 45/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3157 - mae: 0.3837 - mse: 57.8421 - val_loss: 0.1508 - val_mae: 0.2171 - val_mse: 2.5326\n",
      "Epoch 46/10000\n",
      "1294/1294 [==============================] - 7s 6ms/step - loss: 0.3157 - mae: 0.3837 - mse: 57.8391 - val_loss: 0.1508 - val_mae: 0.2171 - val_mse: 2.5326\n",
      "Epoch 47/10000\n",
      "1294/1294 [==============================] - 7s 6ms/step - loss: 0.3157 - mae: 0.3837 - mse: 57.8361 - val_loss: 0.1508 - val_mae: 0.2171 - val_mse: 2.5326\n",
      "Epoch 48/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3157 - mae: 0.3837 - mse: 57.8330 - val_loss: 0.1508 - val_mae: 0.2171 - val_mse: 2.5326\n",
      "Epoch 49/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3157 - mae: 0.3836 - mse: 57.8300 - val_loss: 0.1508 - val_mae: 0.2171 - val_mse: 2.5325\n",
      "Epoch 50/10000\n",
      "1294/1294 [==============================] - 7s 6ms/step - loss: 0.3157 - mae: 0.3836 - mse: 57.8270 - val_loss: 0.1508 - val_mae: 0.2171 - val_mse: 2.5325\n",
      "Epoch 51/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3157 - mae: 0.3836 - mse: 57.8240 - val_loss: 0.1508 - val_mae: 0.2171 - val_mse: 2.5325\n",
      "Epoch 52/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3156 - mae: 0.3836 - mse: 57.8209 - val_loss: 0.1508 - val_mae: 0.2171 - val_mse: 2.5324\n",
      "Epoch 53/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3156 - mae: 0.3836 - mse: 57.8179 - val_loss: 0.1508 - val_mae: 0.2171 - val_mse: 2.5324\n",
      "Epoch 54/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3156 - mae: 0.3836 - mse: 57.8148 - val_loss: 0.1508 - val_mae: 0.2170 - val_mse: 2.5324\n",
      "Epoch 55/10000\n",
      "1294/1294 [==============================] - 7s 6ms/step - loss: 0.3156 - mae: 0.3835 - mse: 57.8119 - val_loss: 0.1508 - val_mae: 0.2170 - val_mse: 2.5324\n",
      "Epoch 56/10000\n",
      "1294/1294 [==============================] - 7s 6ms/step - loss: 0.3156 - mae: 0.3835 - mse: 57.8089 - val_loss: 0.1508 - val_mae: 0.2170 - val_mse: 2.5324\n",
      "Epoch 57/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3156 - mae: 0.3835 - mse: 57.8059 - val_loss: 0.1508 - val_mae: 0.2170 - val_mse: 2.5323\n",
      "Epoch 58/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3156 - mae: 0.3835 - mse: 57.8029 - val_loss: 0.1508 - val_mae: 0.2170 - val_mse: 2.5323\n",
      "Epoch 59/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3156 - mae: 0.3835 - mse: 57.7999 - val_loss: 0.1508 - val_mae: 0.2170 - val_mse: 2.5323\n",
      "Epoch 60/10000\n",
      "1294/1294 [==============================] - 7s 6ms/step - loss: 0.3155 - mae: 0.3835 - mse: 57.7969 - val_loss: 0.1508 - val_mae: 0.2170 - val_mse: 2.5323\n",
      "Epoch 61/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3155 - mae: 0.3834 - mse: 57.7939 - val_loss: 0.1508 - val_mae: 0.2170 - val_mse: 2.5322\n",
      "Epoch 62/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3155 - mae: 0.3834 - mse: 57.7909 - val_loss: 0.1508 - val_mae: 0.2170 - val_mse: 2.5322\n",
      "Epoch 63/10000\n",
      "1294/1294 [==============================] - 7s 6ms/step - loss: 0.3155 - mae: 0.3834 - mse: 57.7879 - val_loss: 0.1508 - val_mae: 0.2170 - val_mse: 2.5322\n",
      "Epoch 64/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3155 - mae: 0.3834 - mse: 57.7849 - val_loss: 0.1508 - val_mae: 0.2170 - val_mse: 2.5322\n",
      "Epoch 65/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3155 - mae: 0.3834 - mse: 57.7819 - val_loss: 0.1508 - val_mae: 0.2170 - val_mse: 2.5321\n",
      "Epoch 66/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3155 - mae: 0.3834 - mse: 57.7790 - val_loss: 0.1508 - val_mae: 0.2170 - val_mse: 2.5321\n",
      "Epoch 67/10000\n",
      "1294/1294 [==============================] - 7s 6ms/step - loss: 0.3155 - mae: 0.3834 - mse: 57.7760 - val_loss: 0.1508 - val_mae: 0.2169 - val_mse: 2.5321\n",
      "Epoch 68/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3154 - mae: 0.3833 - mse: 57.7730 - val_loss: 0.1508 - val_mae: 0.2169 - val_mse: 2.5321\n",
      "Epoch 69/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3154 - mae: 0.3833 - mse: 57.7701 - val_loss: 0.1508 - val_mae: 0.2169 - val_mse: 2.5320\n",
      "Epoch 70/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3154 - mae: 0.3833 - mse: 57.7671 - val_loss: 0.1508 - val_mae: 0.2169 - val_mse: 2.5320\n",
      "Epoch 71/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3154 - mae: 0.3833 - mse: 57.7641 - val_loss: 0.1508 - val_mae: 0.2169 - val_mse: 2.5320\n",
      "Epoch 72/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3154 - mae: 0.3833 - mse: 57.7611 - val_loss: 0.1508 - val_mae: 0.2169 - val_mse: 2.5320\n",
      "Epoch 73/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3154 - mae: 0.3833 - mse: 57.7581 - val_loss: 0.1508 - val_mae: 0.2169 - val_mse: 2.5319\n",
      "Epoch 74/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3154 - mae: 0.3832 - mse: 57.7552 - val_loss: 0.1508 - val_mae: 0.2169 - val_mse: 2.5319\n",
      "Epoch 75/10000\n",
      "1294/1294 [==============================] - 7s 6ms/step - loss: 0.3154 - mae: 0.3832 - mse: 57.7522 - val_loss: 0.1508 - val_mae: 0.2169 - val_mse: 2.5319\n",
      "Epoch 76/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3153 - mae: 0.3832 - mse: 57.7492 - val_loss: 0.1508 - val_mae: 0.2169 - val_mse: 2.5319\n",
      "Epoch 77/10000\n",
      "1294/1294 [==============================] - 7s 6ms/step - loss: 0.3153 - mae: 0.3832 - mse: 57.7463 - val_loss: 0.1508 - val_mae: 0.2169 - val_mse: 2.5319\n",
      "Epoch 78/10000\n",
      "1294/1294 [==============================] - 7s 6ms/step - loss: 0.3153 - mae: 0.3832 - mse: 57.7433 - val_loss: 0.1508 - val_mae: 0.2169 - val_mse: 2.5318\n",
      "Epoch 79/10000\n",
      "1294/1294 [==============================] - 7s 6ms/step - loss: 0.3153 - mae: 0.3832 - mse: 57.7403 - val_loss: 0.1508 - val_mae: 0.2169 - val_mse: 2.5318\n",
      "Epoch 80/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3153 - mae: 0.3832 - mse: 57.7374 - val_loss: 0.1508 - val_mae: 0.2169 - val_mse: 2.5318\n",
      "Epoch 81/10000\n",
      "1294/1294 [==============================] - 7s 6ms/step - loss: 0.3153 - mae: 0.3831 - mse: 57.7344 - val_loss: 0.1508 - val_mae: 0.2169 - val_mse: 2.5318\n",
      "Epoch 82/10000\n",
      "1294/1294 [==============================] - 7s 6ms/step - loss: 0.3153 - mae: 0.3831 - mse: 57.7314 - val_loss: 0.1508 - val_mae: 0.2168 - val_mse: 2.5318\n",
      "Epoch 83/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3153 - mae: 0.3831 - mse: 57.7285 - val_loss: 0.1508 - val_mae: 0.2168 - val_mse: 2.5317\n",
      "Epoch 84/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3152 - mae: 0.3831 - mse: 57.7255 - val_loss: 0.1508 - val_mae: 0.2168 - val_mse: 2.5317\n",
      "Epoch 85/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3152 - mae: 0.3831 - mse: 57.7226 - val_loss: 0.1508 - val_mae: 0.2168 - val_mse: 2.5317\n",
      "Epoch 86/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3152 - mae: 0.3831 - mse: 57.7196 - val_loss: 0.1508 - val_mae: 0.2168 - val_mse: 2.5317\n",
      "Epoch 87/10000\n",
      "1294/1294 [==============================] - 7s 6ms/step - loss: 0.3152 - mae: 0.3831 - mse: 57.7166 - val_loss: 0.1508 - val_mae: 0.2168 - val_mse: 2.5317\n",
      "Epoch 88/10000\n",
      "1294/1294 [==============================] - 7s 6ms/step - loss: 0.3152 - mae: 0.3830 - mse: 57.7137 - val_loss: 0.1508 - val_mae: 0.2168 - val_mse: 2.5316\n",
      "Epoch 89/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3152 - mae: 0.3830 - mse: 57.7107 - val_loss: 0.1508 - val_mae: 0.2168 - val_mse: 2.5316\n",
      "Epoch 90/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3152 - mae: 0.3830 - mse: 57.7077 - val_loss: 0.1508 - val_mae: 0.2168 - val_mse: 2.5316\n",
      "Epoch 91/10000\n",
      "1294/1294 [==============================] - 7s 6ms/step - loss: 0.3152 - mae: 0.3830 - mse: 57.7048 - val_loss: 0.1508 - val_mae: 0.2168 - val_mse: 2.5316\n",
      "Epoch 92/10000\n",
      "1294/1294 [==============================] - 7s 6ms/step - loss: 0.3151 - mae: 0.3830 - mse: 57.7018 - val_loss: 0.1508 - val_mae: 0.2168 - val_mse: 2.5315\n",
      "Epoch 93/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3151 - mae: 0.3830 - mse: 57.6989 - val_loss: 0.1508 - val_mae: 0.2168 - val_mse: 2.5315\n",
      "Epoch 94/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3151 - mae: 0.3830 - mse: 57.6959 - val_loss: 0.1508 - val_mae: 0.2168 - val_mse: 2.5315\n",
      "Epoch 95/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3151 - mae: 0.3829 - mse: 57.6930 - val_loss: 0.1508 - val_mae: 0.2168 - val_mse: 2.5315\n",
      "Epoch 96/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3151 - mae: 0.3829 - mse: 57.6900 - val_loss: 0.1508 - val_mae: 0.2168 - val_mse: 2.5315\n",
      "Epoch 97/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3151 - mae: 0.3829 - mse: 57.6871 - val_loss: 0.1508 - val_mae: 0.2168 - val_mse: 2.5314\n",
      "Epoch 98/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3151 - mae: 0.3829 - mse: 57.6841 - val_loss: 0.1508 - val_mae: 0.2168 - val_mse: 2.5314\n",
      "Epoch 99/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3151 - mae: 0.3829 - mse: 57.6812 - val_loss: 0.1508 - val_mae: 0.2168 - val_mse: 2.5314\n",
      "Epoch 100/10000\n",
      "1294/1294 [==============================] - 7s 6ms/step - loss: 0.3150 - mae: 0.3829 - mse: 57.6782 - val_loss: 0.1508 - val_mae: 0.2167 - val_mse: 2.5314\n",
      "Epoch 101/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3150 - mae: 0.3828 - mse: 57.6752 - val_loss: 0.1508 - val_mae: 0.2167 - val_mse: 2.5314\n",
      "Epoch 102/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3150 - mae: 0.3828 - mse: 57.6723 - val_loss: 0.1508 - val_mae: 0.2167 - val_mse: 2.5313\n",
      "Epoch 103/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3150 - mae: 0.3828 - mse: 57.6693 - val_loss: 0.1508 - val_mae: 0.2167 - val_mse: 2.5313\n",
      "Epoch 104/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3150 - mae: 0.3828 - mse: 57.6664 - val_loss: 0.1508 - val_mae: 0.2167 - val_mse: 2.5313\n",
      "Epoch 105/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3150 - mae: 0.3828 - mse: 57.6634 - val_loss: 0.1508 - val_mae: 0.2167 - val_mse: 2.5313\n",
      "Epoch 106/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3150 - mae: 0.3828 - mse: 57.6605 - val_loss: 0.1508 - val_mae: 0.2167 - val_mse: 2.5312\n",
      "Epoch 107/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3150 - mae: 0.3828 - mse: 57.6575 - val_loss: 0.1508 - val_mae: 0.2167 - val_mse: 2.5312\n",
      "Epoch 108/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3149 - mae: 0.3827 - mse: 57.6546 - val_loss: 0.1507 - val_mae: 0.2167 - val_mse: 2.5312\n",
      "Epoch 109/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3149 - mae: 0.3827 - mse: 57.6516 - val_loss: 0.1507 - val_mae: 0.2167 - val_mse: 2.5312\n",
      "Epoch 110/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3149 - mae: 0.3827 - mse: 57.6487 - val_loss: 0.1507 - val_mae: 0.2167 - val_mse: 2.5311\n",
      "Epoch 111/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3149 - mae: 0.3827 - mse: 57.6457 - val_loss: 0.1507 - val_mae: 0.2167 - val_mse: 2.5311\n",
      "Epoch 112/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3149 - mae: 0.3827 - mse: 57.6428 - val_loss: 0.1507 - val_mae: 0.2167 - val_mse: 2.5311\n",
      "Epoch 113/10000\n",
      "1294/1294 [==============================] - 7s 6ms/step - loss: 0.3149 - mae: 0.3827 - mse: 57.6398 - val_loss: 0.1507 - val_mae: 0.2167 - val_mse: 2.5311\n",
      "Epoch 114/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3149 - mae: 0.3827 - mse: 57.6369 - val_loss: 0.1507 - val_mae: 0.2167 - val_mse: 2.5310\n",
      "Epoch 115/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3149 - mae: 0.3826 - mse: 57.6339 - val_loss: 0.1507 - val_mae: 0.2167 - val_mse: 2.5310\n",
      "Epoch 116/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3149 - mae: 0.3826 - mse: 57.6310 - val_loss: 0.1507 - val_mae: 0.2167 - val_mse: 2.5310\n",
      "Epoch 117/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3148 - mae: 0.3826 - mse: 57.6280 - val_loss: 0.1507 - val_mae: 0.2167 - val_mse: 2.5310\n",
      "Epoch 118/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3148 - mae: 0.3826 - mse: 57.6251 - val_loss: 0.1507 - val_mae: 0.2167 - val_mse: 2.5309\n",
      "Epoch 119/10000\n",
      "1294/1294 [==============================] - 7s 6ms/step - loss: 0.3148 - mae: 0.3826 - mse: 57.6221 - val_loss: 0.1507 - val_mae: 0.2166 - val_mse: 2.5309\n",
      "Epoch 120/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3148 - mae: 0.3826 - mse: 57.6192 - val_loss: 0.1507 - val_mae: 0.2166 - val_mse: 2.5309\n",
      "Epoch 121/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3148 - mae: 0.3826 - mse: 57.6162 - val_loss: 0.1507 - val_mae: 0.2166 - val_mse: 2.5309\n",
      "Epoch 122/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3148 - mae: 0.3825 - mse: 57.6133 - val_loss: 0.1507 - val_mae: 0.2166 - val_mse: 2.5308\n",
      "Epoch 123/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3148 - mae: 0.3825 - mse: 57.6103 - val_loss: 0.1507 - val_mae: 0.2166 - val_mse: 2.5308\n",
      "Epoch 124/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3148 - mae: 0.3825 - mse: 57.6074 - val_loss: 0.1507 - val_mae: 0.2166 - val_mse: 2.5308\n",
      "Epoch 125/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3147 - mae: 0.3825 - mse: 57.6044 - val_loss: 0.1507 - val_mae: 0.2166 - val_mse: 2.5308\n",
      "Epoch 126/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3147 - mae: 0.3825 - mse: 57.6015 - val_loss: 0.1507 - val_mae: 0.2166 - val_mse: 2.5307\n",
      "Epoch 127/10000\n",
      "1294/1294 [==============================] - 7s 6ms/step - loss: 0.3147 - mae: 0.3825 - mse: 57.5985 - val_loss: 0.1507 - val_mae: 0.2166 - val_mse: 2.5307\n",
      "Epoch 128/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3147 - mae: 0.3825 - mse: 57.5956 - val_loss: 0.1507 - val_mae: 0.2166 - val_mse: 2.5307\n",
      "Epoch 129/10000\n",
      "1294/1294 [==============================] - 7s 6ms/step - loss: 0.3147 - mae: 0.3824 - mse: 57.5926 - val_loss: 0.1507 - val_mae: 0.2166 - val_mse: 2.5307\n",
      "Epoch 130/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3147 - mae: 0.3824 - mse: 57.5897 - val_loss: 0.1507 - val_mae: 0.2166 - val_mse: 2.5306\n",
      "Epoch 131/10000\n",
      "1294/1294 [==============================] - 7s 6ms/step - loss: 0.3147 - mae: 0.3824 - mse: 57.5868 - val_loss: 0.1507 - val_mae: 0.2166 - val_mse: 2.5306\n",
      "Epoch 132/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3147 - mae: 0.3824 - mse: 57.5838 - val_loss: 0.1507 - val_mae: 0.2166 - val_mse: 2.5306\n",
      "Epoch 133/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3146 - mae: 0.3824 - mse: 57.5809 - val_loss: 0.1507 - val_mae: 0.2166 - val_mse: 2.5306\n",
      "Epoch 134/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3146 - mae: 0.3824 - mse: 57.5779 - val_loss: 0.1507 - val_mae: 0.2166 - val_mse: 2.5305\n",
      "Epoch 135/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3146 - mae: 0.3824 - mse: 57.5750 - val_loss: 0.1507 - val_mae: 0.2166 - val_mse: 2.5305\n",
      "Epoch 136/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3146 - mae: 0.3823 - mse: 57.5720 - val_loss: 0.1507 - val_mae: 0.2166 - val_mse: 2.5305\n",
      "Epoch 137/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3146 - mae: 0.3823 - mse: 57.5691 - val_loss: 0.1507 - val_mae: 0.2166 - val_mse: 2.5305\n",
      "Epoch 138/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3146 - mae: 0.3823 - mse: 57.5661 - val_loss: 0.1507 - val_mae: 0.2166 - val_mse: 2.5304\n",
      "Epoch 139/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3146 - mae: 0.3823 - mse: 57.5632 - val_loss: 0.1507 - val_mae: 0.2166 - val_mse: 2.5304\n",
      "Epoch 140/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3146 - mae: 0.3823 - mse: 57.5602 - val_loss: 0.1507 - val_mae: 0.2165 - val_mse: 2.5304\n",
      "Epoch 141/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3145 - mae: 0.3823 - mse: 57.5573 - val_loss: 0.1507 - val_mae: 0.2165 - val_mse: 2.5304\n",
      "Epoch 142/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3145 - mae: 0.3823 - mse: 57.5544 - val_loss: 0.1507 - val_mae: 0.2165 - val_mse: 2.5303\n",
      "Epoch 143/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3145 - mae: 0.3822 - mse: 57.5514 - val_loss: 0.1507 - val_mae: 0.2165 - val_mse: 2.5303\n",
      "Epoch 144/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3145 - mae: 0.3822 - mse: 57.5485 - val_loss: 0.1507 - val_mae: 0.2165 - val_mse: 2.5303\n",
      "Epoch 145/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3145 - mae: 0.3822 - mse: 57.5455 - val_loss: 0.1507 - val_mae: 0.2165 - val_mse: 2.5302\n",
      "Epoch 146/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3145 - mae: 0.3822 - mse: 57.5426 - val_loss: 0.1507 - val_mae: 0.2165 - val_mse: 2.5302\n",
      "Epoch 147/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3145 - mae: 0.3822 - mse: 57.5397 - val_loss: 0.1507 - val_mae: 0.2165 - val_mse: 2.5302\n",
      "Epoch 148/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3145 - mae: 0.3822 - mse: 57.5367 - val_loss: 0.1507 - val_mae: 0.2165 - val_mse: 2.5302\n",
      "Epoch 149/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3145 - mae: 0.3822 - mse: 57.5338 - val_loss: 0.1507 - val_mae: 0.2165 - val_mse: 2.5301\n",
      "Epoch 150/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3144 - mae: 0.3821 - mse: 57.5308 - val_loss: 0.1507 - val_mae: 0.2165 - val_mse: 2.5301\n",
      "Epoch 151/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3144 - mae: 0.3821 - mse: 57.5279 - val_loss: 0.1507 - val_mae: 0.2165 - val_mse: 2.5301\n",
      "Epoch 152/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3144 - mae: 0.3821 - mse: 57.5249 - val_loss: 0.1507 - val_mae: 0.2165 - val_mse: 2.5300\n",
      "Epoch 153/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3144 - mae: 0.3821 - mse: 57.5220 - val_loss: 0.1507 - val_mae: 0.2165 - val_mse: 2.5300\n",
      "Epoch 154/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3144 - mae: 0.3821 - mse: 57.5190 - val_loss: 0.1507 - val_mae: 0.2165 - val_mse: 2.5300\n",
      "Epoch 155/10000\n",
      "1294/1294 [==============================] - 7s 6ms/step - loss: 0.3144 - mae: 0.3821 - mse: 57.5161 - val_loss: 0.1507 - val_mae: 0.2165 - val_mse: 2.5300\n",
      "Epoch 156/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3144 - mae: 0.3821 - mse: 57.5132 - val_loss: 0.1507 - val_mae: 0.2165 - val_mse: 2.5299\n",
      "Epoch 157/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3144 - mae: 0.3820 - mse: 57.5102 - val_loss: 0.1507 - val_mae: 0.2165 - val_mse: 2.5299\n",
      "Epoch 158/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3143 - mae: 0.3820 - mse: 57.5073 - val_loss: 0.1507 - val_mae: 0.2165 - val_mse: 2.5299\n",
      "Epoch 159/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3143 - mae: 0.3820 - mse: 57.5043 - val_loss: 0.1507 - val_mae: 0.2165 - val_mse: 2.5298\n",
      "Epoch 160/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3143 - mae: 0.3820 - mse: 57.5014 - val_loss: 0.1507 - val_mae: 0.2165 - val_mse: 2.5298\n",
      "Epoch 161/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3143 - mae: 0.3820 - mse: 57.4984 - val_loss: 0.1507 - val_mae: 0.2165 - val_mse: 2.5298\n",
      "Epoch 162/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3143 - mae: 0.3820 - mse: 57.4955 - val_loss: 0.1507 - val_mae: 0.2165 - val_mse: 2.5298\n",
      "Epoch 163/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3143 - mae: 0.3820 - mse: 57.4926 - val_loss: 0.1507 - val_mae: 0.2164 - val_mse: 2.5297\n",
      "Epoch 164/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3143 - mae: 0.3819 - mse: 57.4897 - val_loss: 0.1507 - val_mae: 0.2164 - val_mse: 2.5297\n",
      "Epoch 165/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3143 - mae: 0.3819 - mse: 57.4867 - val_loss: 0.1507 - val_mae: 0.2164 - val_mse: 2.5297\n",
      "Epoch 166/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3142 - mae: 0.3819 - mse: 57.4838 - val_loss: 0.1507 - val_mae: 0.2164 - val_mse: 2.5296\n",
      "Epoch 167/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3142 - mae: 0.3819 - mse: 57.4808 - val_loss: 0.1507 - val_mae: 0.2164 - val_mse: 2.5296\n",
      "Epoch 168/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3142 - mae: 0.3819 - mse: 57.4779 - val_loss: 0.1507 - val_mae: 0.2164 - val_mse: 2.5296\n",
      "Epoch 169/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3142 - mae: 0.3819 - mse: 57.4750 - val_loss: 0.1507 - val_mae: 0.2164 - val_mse: 2.5295\n",
      "Epoch 170/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3142 - mae: 0.3819 - mse: 57.4720 - val_loss: 0.1507 - val_mae: 0.2164 - val_mse: 2.5295\n",
      "Epoch 171/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3142 - mae: 0.3818 - mse: 57.4691 - val_loss: 0.1507 - val_mae: 0.2164 - val_mse: 2.5295\n",
      "Epoch 172/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3142 - mae: 0.3818 - mse: 57.4661 - val_loss: 0.1507 - val_mae: 0.2164 - val_mse: 2.5295\n",
      "Epoch 173/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3142 - mae: 0.3818 - mse: 57.4632 - val_loss: 0.1507 - val_mae: 0.2164 - val_mse: 2.5294\n",
      "Epoch 174/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3142 - mae: 0.3818 - mse: 57.4603 - val_loss: 0.1507 - val_mae: 0.2164 - val_mse: 2.5294\n",
      "Epoch 175/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3141 - mae: 0.3818 - mse: 57.4573 - val_loss: 0.1507 - val_mae: 0.2164 - val_mse: 2.5294\n",
      "Epoch 176/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3141 - mae: 0.3818 - mse: 57.4544 - val_loss: 0.1507 - val_mae: 0.2164 - val_mse: 2.5293\n",
      "Epoch 177/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3141 - mae: 0.3818 - mse: 57.4515 - val_loss: 0.1507 - val_mae: 0.2164 - val_mse: 2.5293\n",
      "Epoch 178/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3141 - mae: 0.3818 - mse: 57.4485 - val_loss: 0.1507 - val_mae: 0.2164 - val_mse: 2.5293\n",
      "Epoch 179/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3141 - mae: 0.3817 - mse: 57.4456 - val_loss: 0.1507 - val_mae: 0.2164 - val_mse: 2.5292\n",
      "Epoch 180/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3141 - mae: 0.3817 - mse: 57.4426 - val_loss: 0.1507 - val_mae: 0.2164 - val_mse: 2.5292\n",
      "Epoch 181/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3141 - mae: 0.3817 - mse: 57.4397 - val_loss: 0.1507 - val_mae: 0.2164 - val_mse: 2.5292\n",
      "Epoch 182/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3141 - mae: 0.3817 - mse: 57.4367 - val_loss: 0.1507 - val_mae: 0.2164 - val_mse: 2.5292\n",
      "Epoch 183/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3140 - mae: 0.3817 - mse: 57.4338 - val_loss: 0.1507 - val_mae: 0.2164 - val_mse: 2.5291\n",
      "Epoch 184/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3140 - mae: 0.3817 - mse: 57.4308 - val_loss: 0.1507 - val_mae: 0.2164 - val_mse: 2.5291\n",
      "Epoch 185/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3140 - mae: 0.3817 - mse: 57.4279 - val_loss: 0.1507 - val_mae: 0.2164 - val_mse: 2.5291\n",
      "Epoch 186/10000\n",
      "1294/1294 [==============================] - 7s 6ms/step - loss: 0.3140 - mae: 0.3816 - mse: 57.4249 - val_loss: 0.1507 - val_mae: 0.2164 - val_mse: 2.5290\n",
      "Epoch 187/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3140 - mae: 0.3816 - mse: 57.4220 - val_loss: 0.1507 - val_mae: 0.2163 - val_mse: 2.5290\n",
      "Epoch 188/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3140 - mae: 0.3816 - mse: 57.4190 - val_loss: 0.1507 - val_mae: 0.2163 - val_mse: 2.5290\n",
      "Epoch 189/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3140 - mae: 0.3816 - mse: 57.4161 - val_loss: 0.1507 - val_mae: 0.2163 - val_mse: 2.5289\n",
      "Epoch 190/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3140 - mae: 0.3816 - mse: 57.4131 - val_loss: 0.1507 - val_mae: 0.2163 - val_mse: 2.5289\n",
      "Epoch 191/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3139 - mae: 0.3816 - mse: 57.4102 - val_loss: 0.1507 - val_mae: 0.2163 - val_mse: 2.5289\n",
      "Epoch 192/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3139 - mae: 0.3816 - mse: 57.4072 - val_loss: 0.1507 - val_mae: 0.2163 - val_mse: 2.5288\n",
      "Epoch 193/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3139 - mae: 0.3815 - mse: 57.4043 - val_loss: 0.1507 - val_mae: 0.2163 - val_mse: 2.5288\n",
      "Epoch 194/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3139 - mae: 0.3815 - mse: 57.4013 - val_loss: 0.1507 - val_mae: 0.2163 - val_mse: 2.5288\n",
      "Epoch 195/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3139 - mae: 0.3815 - mse: 57.3984 - val_loss: 0.1507 - val_mae: 0.2163 - val_mse: 2.5288\n",
      "Epoch 196/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3139 - mae: 0.3815 - mse: 57.3955 - val_loss: 0.1507 - val_mae: 0.2163 - val_mse: 2.5287\n",
      "Epoch 197/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3139 - mae: 0.3815 - mse: 57.3925 - val_loss: 0.1507 - val_mae: 0.2163 - val_mse: 2.5287\n",
      "Epoch 198/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3139 - mae: 0.3815 - mse: 57.3895 - val_loss: 0.1507 - val_mae: 0.2163 - val_mse: 2.5287\n",
      "Epoch 199/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3138 - mae: 0.3815 - mse: 57.3866 - val_loss: 0.1507 - val_mae: 0.2163 - val_mse: 2.5286\n",
      "Epoch 200/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3138 - mae: 0.3814 - mse: 57.3837 - val_loss: 0.1507 - val_mae: 0.2163 - val_mse: 2.5286\n",
      "Epoch 201/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3138 - mae: 0.3814 - mse: 57.3807 - val_loss: 0.1507 - val_mae: 0.2163 - val_mse: 2.5286\n",
      "Epoch 202/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3138 - mae: 0.3814 - mse: 57.3778 - val_loss: 0.1507 - val_mae: 0.2163 - val_mse: 2.5285\n",
      "Epoch 203/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3138 - mae: 0.3814 - mse: 57.3748 - val_loss: 0.1507 - val_mae: 0.2163 - val_mse: 2.5285\n",
      "Epoch 204/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3138 - mae: 0.3814 - mse: 57.3719 - val_loss: 0.1506 - val_mae: 0.2163 - val_mse: 2.5285\n",
      "Epoch 205/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3138 - mae: 0.3814 - mse: 57.3689 - val_loss: 0.1506 - val_mae: 0.2163 - val_mse: 2.5285\n",
      "Epoch 206/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3138 - mae: 0.3814 - mse: 57.3660 - val_loss: 0.1506 - val_mae: 0.2163 - val_mse: 2.5284\n",
      "Epoch 207/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3138 - mae: 0.3813 - mse: 57.3630 - val_loss: 0.1506 - val_mae: 0.2163 - val_mse: 2.5284\n",
      "Epoch 208/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3137 - mae: 0.3813 - mse: 57.3601 - val_loss: 0.1506 - val_mae: 0.2163 - val_mse: 2.5284\n",
      "Epoch 209/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3137 - mae: 0.3813 - mse: 57.3571 - val_loss: 0.1506 - val_mae: 0.2163 - val_mse: 2.5283\n",
      "Epoch 210/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3137 - mae: 0.3813 - mse: 57.3542 - val_loss: 0.1506 - val_mae: 0.2163 - val_mse: 2.5283\n",
      "Epoch 211/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3137 - mae: 0.3813 - mse: 57.3513 - val_loss: 0.1506 - val_mae: 0.2162 - val_mse: 2.5283\n",
      "Epoch 212/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3137 - mae: 0.3813 - mse: 57.3483 - val_loss: 0.1506 - val_mae: 0.2162 - val_mse: 2.5282\n",
      "Epoch 213/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3137 - mae: 0.3813 - mse: 57.3453 - val_loss: 0.1506 - val_mae: 0.2162 - val_mse: 2.5282\n",
      "Epoch 214/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3137 - mae: 0.3812 - mse: 57.3424 - val_loss: 0.1506 - val_mae: 0.2162 - val_mse: 2.5282\n",
      "Epoch 215/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3137 - mae: 0.3812 - mse: 57.3394 - val_loss: 0.1506 - val_mae: 0.2162 - val_mse: 2.5281\n",
      "Epoch 216/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3136 - mae: 0.3812 - mse: 57.3365 - val_loss: 0.1506 - val_mae: 0.2162 - val_mse: 2.5281\n",
      "Epoch 217/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3136 - mae: 0.3812 - mse: 57.3335 - val_loss: 0.1506 - val_mae: 0.2162 - val_mse: 2.5281\n",
      "Epoch 218/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3136 - mae: 0.3812 - mse: 57.3306 - val_loss: 0.1506 - val_mae: 0.2162 - val_mse: 2.5280\n",
      "Epoch 219/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3136 - mae: 0.3812 - mse: 57.3276 - val_loss: 0.1506 - val_mae: 0.2162 - val_mse: 2.5280\n",
      "Epoch 220/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3136 - mae: 0.3812 - mse: 57.3247 - val_loss: 0.1506 - val_mae: 0.2162 - val_mse: 2.5280\n",
      "Epoch 221/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3136 - mae: 0.3812 - mse: 57.3217 - val_loss: 0.1506 - val_mae: 0.2162 - val_mse: 2.5279\n",
      "Epoch 222/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3136 - mae: 0.3811 - mse: 57.3188 - val_loss: 0.1506 - val_mae: 0.2162 - val_mse: 2.5279\n",
      "Epoch 223/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3136 - mae: 0.3811 - mse: 57.3158 - val_loss: 0.1506 - val_mae: 0.2162 - val_mse: 2.5279\n",
      "Epoch 224/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3136 - mae: 0.3811 - mse: 57.3129 - val_loss: 0.1506 - val_mae: 0.2162 - val_mse: 2.5278\n",
      "Epoch 225/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3135 - mae: 0.3811 - mse: 57.3099 - val_loss: 0.1506 - val_mae: 0.2162 - val_mse: 2.5278\n",
      "Epoch 226/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3135 - mae: 0.3811 - mse: 57.3070 - val_loss: 0.1506 - val_mae: 0.2162 - val_mse: 2.5278\n",
      "Epoch 227/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3135 - mae: 0.3811 - mse: 57.3040 - val_loss: 0.1506 - val_mae: 0.2162 - val_mse: 2.5278\n",
      "Epoch 228/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3135 - mae: 0.3811 - mse: 57.3011 - val_loss: 0.1506 - val_mae: 0.2162 - val_mse: 2.5277\n",
      "Epoch 229/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3135 - mae: 0.3810 - mse: 57.2981 - val_loss: 0.1506 - val_mae: 0.2162 - val_mse: 2.5277\n",
      "Epoch 230/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3135 - mae: 0.3810 - mse: 57.2952 - val_loss: 0.1506 - val_mae: 0.2162 - val_mse: 2.5277\n",
      "Epoch 231/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3135 - mae: 0.3810 - mse: 57.2922 - val_loss: 0.1506 - val_mae: 0.2162 - val_mse: 2.5276\n",
      "Epoch 232/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3135 - mae: 0.3810 - mse: 57.2893 - val_loss: 0.1506 - val_mae: 0.2162 - val_mse: 2.5276\n",
      "Epoch 233/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3134 - mae: 0.3810 - mse: 57.2863 - val_loss: 0.1506 - val_mae: 0.2162 - val_mse: 2.5276\n",
      "Epoch 234/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3134 - mae: 0.3810 - mse: 57.2834 - val_loss: 0.1506 - val_mae: 0.2162 - val_mse: 2.5275\n",
      "Epoch 235/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3134 - mae: 0.3810 - mse: 57.2804 - val_loss: 0.1506 - val_mae: 0.2162 - val_mse: 2.5275\n",
      "Epoch 236/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3134 - mae: 0.3809 - mse: 57.2775 - val_loss: 0.1506 - val_mae: 0.2161 - val_mse: 2.5275\n",
      "Epoch 237/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3134 - mae: 0.3809 - mse: 57.2745 - val_loss: 0.1506 - val_mae: 0.2161 - val_mse: 2.5274\n",
      "Epoch 238/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3134 - mae: 0.3809 - mse: 57.2716 - val_loss: 0.1506 - val_mae: 0.2161 - val_mse: 2.5274\n",
      "Epoch 239/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3134 - mae: 0.3809 - mse: 57.2687 - val_loss: 0.1506 - val_mae: 0.2161 - val_mse: 2.5274\n",
      "Epoch 240/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3134 - mae: 0.3809 - mse: 57.2657 - val_loss: 0.1506 - val_mae: 0.2161 - val_mse: 2.5273\n",
      "Epoch 241/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3133 - mae: 0.3809 - mse: 57.2628 - val_loss: 0.1506 - val_mae: 0.2161 - val_mse: 2.5273\n",
      "Epoch 242/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3133 - mae: 0.3809 - mse: 57.2598 - val_loss: 0.1506 - val_mae: 0.2161 - val_mse: 2.5273\n",
      "Epoch 243/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3133 - mae: 0.3808 - mse: 57.2569 - val_loss: 0.1506 - val_mae: 0.2161 - val_mse: 2.5272\n",
      "Epoch 244/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3133 - mae: 0.3808 - mse: 57.2539 - val_loss: 0.1506 - val_mae: 0.2161 - val_mse: 2.5272\n",
      "Epoch 245/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3133 - mae: 0.3808 - mse: 57.2510 - val_loss: 0.1506 - val_mae: 0.2161 - val_mse: 2.5272\n",
      "Epoch 246/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3133 - mae: 0.3808 - mse: 57.2481 - val_loss: 0.1506 - val_mae: 0.2161 - val_mse: 2.5271\n",
      "Epoch 247/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3133 - mae: 0.3808 - mse: 57.2451 - val_loss: 0.1506 - val_mae: 0.2161 - val_mse: 2.5271\n",
      "Epoch 248/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3133 - mae: 0.3808 - mse: 57.2422 - val_loss: 0.1506 - val_mae: 0.2161 - val_mse: 2.5271\n",
      "Epoch 249/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3133 - mae: 0.3808 - mse: 57.2392 - val_loss: 0.1506 - val_mae: 0.2161 - val_mse: 2.5271\n",
      "Epoch 250/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3132 - mae: 0.3808 - mse: 57.2363 - val_loss: 0.1506 - val_mae: 0.2161 - val_mse: 2.5270\n",
      "Epoch 251/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3132 - mae: 0.3807 - mse: 57.2334 - val_loss: 0.1506 - val_mae: 0.2161 - val_mse: 2.5270\n",
      "Epoch 252/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3132 - mae: 0.3807 - mse: 57.2304 - val_loss: 0.1506 - val_mae: 0.2161 - val_mse: 2.5270\n",
      "Epoch 253/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3132 - mae: 0.3807 - mse: 57.2275 - val_loss: 0.1506 - val_mae: 0.2161 - val_mse: 2.5269\n",
      "Epoch 254/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3132 - mae: 0.3807 - mse: 57.2246 - val_loss: 0.1506 - val_mae: 0.2161 - val_mse: 2.5269\n",
      "Epoch 255/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3132 - mae: 0.3807 - mse: 57.2217 - val_loss: 0.1506 - val_mae: 0.2161 - val_mse: 2.5269\n",
      "Epoch 256/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3132 - mae: 0.3807 - mse: 57.2188 - val_loss: 0.1506 - val_mae: 0.2161 - val_mse: 2.5269\n",
      "Epoch 257/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3132 - mae: 0.3807 - mse: 57.2159 - val_loss: 0.1506 - val_mae: 0.2161 - val_mse: 2.5268\n",
      "Epoch 258/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3131 - mae: 0.3806 - mse: 57.2130 - val_loss: 0.1506 - val_mae: 0.2161 - val_mse: 2.5268\n",
      "Epoch 259/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3131 - mae: 0.3806 - mse: 57.2101 - val_loss: 0.1506 - val_mae: 0.2161 - val_mse: 2.5268\n",
      "Epoch 260/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3131 - mae: 0.3806 - mse: 57.2072 - val_loss: 0.1506 - val_mae: 0.2161 - val_mse: 2.5267\n",
      "Epoch 261/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3131 - mae: 0.3806 - mse: 57.2043 - val_loss: 0.1506 - val_mae: 0.2161 - val_mse: 2.5267\n",
      "Epoch 262/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3131 - mae: 0.3806 - mse: 57.2014 - val_loss: 0.1506 - val_mae: 0.2161 - val_mse: 2.5267\n",
      "Epoch 263/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3131 - mae: 0.3806 - mse: 57.1985 - val_loss: 0.1506 - val_mae: 0.2161 - val_mse: 2.5267\n",
      "Epoch 264/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3131 - mae: 0.3806 - mse: 57.1956 - val_loss: 0.1506 - val_mae: 0.2160 - val_mse: 2.5266\n",
      "Epoch 265/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3131 - mae: 0.3805 - mse: 57.1927 - val_loss: 0.1506 - val_mae: 0.2160 - val_mse: 2.5266\n",
      "Epoch 266/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3131 - mae: 0.3805 - mse: 57.1898 - val_loss: 0.1506 - val_mae: 0.2160 - val_mse: 2.5266\n",
      "Epoch 267/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3130 - mae: 0.3805 - mse: 57.1869 - val_loss: 0.1506 - val_mae: 0.2160 - val_mse: 2.5265\n",
      "Epoch 268/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3130 - mae: 0.3805 - mse: 57.1841 - val_loss: 0.1506 - val_mae: 0.2160 - val_mse: 2.5265\n",
      "Epoch 269/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3130 - mae: 0.3805 - mse: 57.1812 - val_loss: 0.1506 - val_mae: 0.2160 - val_mse: 2.5265\n",
      "Epoch 270/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3130 - mae: 0.3805 - mse: 57.1783 - val_loss: 0.1506 - val_mae: 0.2160 - val_mse: 2.5264\n",
      "Epoch 271/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3130 - mae: 0.3805 - mse: 57.1755 - val_loss: 0.1506 - val_mae: 0.2160 - val_mse: 2.5264\n",
      "Epoch 272/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3130 - mae: 0.3805 - mse: 57.1726 - val_loss: 0.1506 - val_mae: 0.2160 - val_mse: 2.5264\n",
      "Epoch 273/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3130 - mae: 0.3804 - mse: 57.1697 - val_loss: 0.1506 - val_mae: 0.2160 - val_mse: 2.5264\n",
      "Epoch 274/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3130 - mae: 0.3804 - mse: 57.1668 - val_loss: 0.1506 - val_mae: 0.2160 - val_mse: 2.5263\n",
      "Epoch 275/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3130 - mae: 0.3804 - mse: 57.1640 - val_loss: 0.1506 - val_mae: 0.2160 - val_mse: 2.5263\n",
      "Epoch 276/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3129 - mae: 0.3804 - mse: 57.1611 - val_loss: 0.1506 - val_mae: 0.2160 - val_mse: 2.5263\n",
      "Epoch 277/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3129 - mae: 0.3804 - mse: 57.1582 - val_loss: 0.1506 - val_mae: 0.2160 - val_mse: 2.5262\n",
      "Epoch 278/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3129 - mae: 0.3804 - mse: 57.1554 - val_loss: 0.1506 - val_mae: 0.2160 - val_mse: 2.5262\n",
      "Epoch 279/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3129 - mae: 0.3804 - mse: 57.1525 - val_loss: 0.1506 - val_mae: 0.2160 - val_mse: 2.5262\n",
      "Epoch 280/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3129 - mae: 0.3803 - mse: 57.1497 - val_loss: 0.1506 - val_mae: 0.2160 - val_mse: 2.5262\n",
      "Epoch 281/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3129 - mae: 0.3803 - mse: 57.1468 - val_loss: 0.1505 - val_mae: 0.2160 - val_mse: 2.5261\n",
      "Epoch 282/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3129 - mae: 0.3803 - mse: 57.1439 - val_loss: 0.1505 - val_mae: 0.2160 - val_mse: 2.5261\n",
      "Epoch 283/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3129 - mae: 0.3803 - mse: 57.1411 - val_loss: 0.1505 - val_mae: 0.2160 - val_mse: 2.5261\n",
      "Epoch 284/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3128 - mae: 0.3803 - mse: 57.1383 - val_loss: 0.1505 - val_mae: 0.2160 - val_mse: 2.5261\n",
      "Epoch 285/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3128 - mae: 0.3803 - mse: 57.1354 - val_loss: 0.1505 - val_mae: 0.2160 - val_mse: 2.5260\n",
      "Epoch 286/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3128 - mae: 0.3803 - mse: 57.1326 - val_loss: 0.1505 - val_mae: 0.2160 - val_mse: 2.5260\n",
      "Epoch 287/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3128 - mae: 0.3803 - mse: 57.1297 - val_loss: 0.1505 - val_mae: 0.2160 - val_mse: 2.5260\n",
      "Epoch 288/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3128 - mae: 0.3802 - mse: 57.1269 - val_loss: 0.1505 - val_mae: 0.2160 - val_mse: 2.5260\n",
      "Epoch 289/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3128 - mae: 0.3802 - mse: 57.1241 - val_loss: 0.1505 - val_mae: 0.2160 - val_mse: 2.5259\n",
      "Epoch 290/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3128 - mae: 0.3802 - mse: 57.1212 - val_loss: 0.1505 - val_mae: 0.2160 - val_mse: 2.5259\n",
      "Epoch 291/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3128 - mae: 0.3802 - mse: 57.1184 - val_loss: 0.1505 - val_mae: 0.2160 - val_mse: 2.5259\n",
      "Epoch 292/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3128 - mae: 0.3802 - mse: 57.1156 - val_loss: 0.1505 - val_mae: 0.2160 - val_mse: 2.5259\n",
      "Epoch 293/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3127 - mae: 0.3802 - mse: 57.1127 - val_loss: 0.1505 - val_mae: 0.2159 - val_mse: 2.5258\n",
      "Epoch 294/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3127 - mae: 0.3802 - mse: 57.1099 - val_loss: 0.1505 - val_mae: 0.2159 - val_mse: 2.5258\n",
      "Epoch 295/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3127 - mae: 0.3801 - mse: 57.1071 - val_loss: 0.1505 - val_mae: 0.2159 - val_mse: 2.5258\n",
      "Epoch 296/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3127 - mae: 0.3801 - mse: 57.1043 - val_loss: 0.1505 - val_mae: 0.2159 - val_mse: 2.5257\n",
      "Epoch 297/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3127 - mae: 0.3801 - mse: 57.1014 - val_loss: 0.1505 - val_mae: 0.2159 - val_mse: 2.5257\n",
      "Epoch 298/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3127 - mae: 0.3801 - mse: 57.0986 - val_loss: 0.1505 - val_mae: 0.2159 - val_mse: 2.5257\n",
      "Epoch 299/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3127 - mae: 0.3801 - mse: 57.0958 - val_loss: 0.1505 - val_mae: 0.2159 - val_mse: 2.5257\n",
      "Epoch 300/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3127 - mae: 0.3801 - mse: 57.0930 - val_loss: 0.1505 - val_mae: 0.2159 - val_mse: 2.5256\n",
      "Epoch 301/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3127 - mae: 0.3801 - mse: 57.0902 - val_loss: 0.1505 - val_mae: 0.2159 - val_mse: 2.5256\n",
      "Epoch 302/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3126 - mae: 0.3801 - mse: 57.0873 - val_loss: 0.1505 - val_mae: 0.2159 - val_mse: 2.5256\n",
      "Epoch 303/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3126 - mae: 0.3800 - mse: 57.0845 - val_loss: 0.1505 - val_mae: 0.2159 - val_mse: 2.5256\n",
      "Epoch 304/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3126 - mae: 0.3800 - mse: 57.0817 - val_loss: 0.1505 - val_mae: 0.2159 - val_mse: 2.5255\n",
      "Epoch 305/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3126 - mae: 0.3800 - mse: 57.0789 - val_loss: 0.1505 - val_mae: 0.2159 - val_mse: 2.5255\n",
      "Epoch 306/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3126 - mae: 0.3800 - mse: 57.0761 - val_loss: 0.1505 - val_mae: 0.2159 - val_mse: 2.5255\n",
      "Epoch 307/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3126 - mae: 0.3800 - mse: 57.0733 - val_loss: 0.1505 - val_mae: 0.2159 - val_mse: 2.5255\n",
      "Epoch 308/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3126 - mae: 0.3800 - mse: 57.0705 - val_loss: 0.1505 - val_mae: 0.2159 - val_mse: 2.5254\n",
      "Epoch 309/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3126 - mae: 0.3800 - mse: 57.0677 - val_loss: 0.1505 - val_mae: 0.2159 - val_mse: 2.5254\n",
      "Epoch 310/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3125 - mae: 0.3799 - mse: 57.0649 - val_loss: 0.1505 - val_mae: 0.2159 - val_mse: 2.5254\n",
      "Epoch 311/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3125 - mae: 0.3799 - mse: 57.0621 - val_loss: 0.1505 - val_mae: 0.2159 - val_mse: 2.5254\n",
      "Epoch 312/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3125 - mae: 0.3799 - mse: 57.0593 - val_loss: 0.1505 - val_mae: 0.2159 - val_mse: 2.5253\n",
      "Epoch 313/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3125 - mae: 0.3799 - mse: 57.0565 - val_loss: 0.1505 - val_mae: 0.2159 - val_mse: 2.5253\n",
      "Epoch 314/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3125 - mae: 0.3799 - mse: 57.0537 - val_loss: 0.1505 - val_mae: 0.2159 - val_mse: 2.5253\n",
      "Epoch 315/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3125 - mae: 0.3799 - mse: 57.0509 - val_loss: 0.1505 - val_mae: 0.2159 - val_mse: 2.5253\n",
      "Epoch 316/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3125 - mae: 0.3799 - mse: 57.0481 - val_loss: 0.1505 - val_mae: 0.2159 - val_mse: 2.5252\n",
      "Epoch 317/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3125 - mae: 0.3799 - mse: 57.0453 - val_loss: 0.1505 - val_mae: 0.2159 - val_mse: 2.5252\n",
      "Epoch 318/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3125 - mae: 0.3798 - mse: 57.0425 - val_loss: 0.1505 - val_mae: 0.2159 - val_mse: 2.5252\n",
      "Epoch 319/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3124 - mae: 0.3798 - mse: 57.0398 - val_loss: 0.1505 - val_mae: 0.2159 - val_mse: 2.5252\n",
      "Epoch 320/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3124 - mae: 0.3798 - mse: 57.0370 - val_loss: 0.1505 - val_mae: 0.2159 - val_mse: 2.5251\n",
      "Epoch 321/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3124 - mae: 0.3798 - mse: 57.0342 - val_loss: 0.1505 - val_mae: 0.2159 - val_mse: 2.5251\n",
      "Epoch 322/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3124 - mae: 0.3798 - mse: 57.0314 - val_loss: 0.1505 - val_mae: 0.2159 - val_mse: 2.5251\n",
      "Epoch 323/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3124 - mae: 0.3798 - mse: 57.0286 - val_loss: 0.1505 - val_mae: 0.2158 - val_mse: 2.5251\n",
      "Epoch 324/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3124 - mae: 0.3798 - mse: 57.0259 - val_loss: 0.1505 - val_mae: 0.2158 - val_mse: 2.5250\n",
      "Epoch 325/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3124 - mae: 0.3797 - mse: 57.0231 - val_loss: 0.1505 - val_mae: 0.2158 - val_mse: 2.5250\n",
      "Epoch 326/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3124 - mae: 0.3797 - mse: 57.0203 - val_loss: 0.1505 - val_mae: 0.2158 - val_mse: 2.5250\n",
      "Epoch 327/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3124 - mae: 0.3797 - mse: 57.0175 - val_loss: 0.1505 - val_mae: 0.2158 - val_mse: 2.5250\n",
      "Epoch 328/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3123 - mae: 0.3797 - mse: 57.0147 - val_loss: 0.1505 - val_mae: 0.2158 - val_mse: 2.5249\n",
      "Epoch 329/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3123 - mae: 0.3797 - mse: 57.0120 - val_loss: 0.1505 - val_mae: 0.2158 - val_mse: 2.5249\n",
      "Epoch 330/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3123 - mae: 0.3797 - mse: 57.0092 - val_loss: 0.1505 - val_mae: 0.2158 - val_mse: 2.5249\n",
      "Epoch 331/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3123 - mae: 0.3797 - mse: 57.0064 - val_loss: 0.1505 - val_mae: 0.2158 - val_mse: 2.5249\n",
      "Epoch 332/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3123 - mae: 0.3797 - mse: 57.0037 - val_loss: 0.1505 - val_mae: 0.2158 - val_mse: 2.5248\n",
      "Epoch 333/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3123 - mae: 0.3796 - mse: 57.0009 - val_loss: 0.1505 - val_mae: 0.2158 - val_mse: 2.5248\n",
      "Epoch 334/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3123 - mae: 0.3796 - mse: 56.9981 - val_loss: 0.1505 - val_mae: 0.2158 - val_mse: 2.5248\n",
      "Epoch 335/10000\n",
      "1294/1294 [==============================] - 8s 7ms/step - loss: 0.3123 - mae: 0.3796 - mse: 56.9953 - val_loss: 0.1505 - val_mae: 0.2158 - val_mse: 2.5248\n",
      "Epoch 336/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3123 - mae: 0.3796 - mse: 56.9925 - val_loss: 0.1505 - val_mae: 0.2158 - val_mse: 2.5247\n",
      "Epoch 337/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3122 - mae: 0.3796 - mse: 56.9898 - val_loss: 0.1505 - val_mae: 0.2158 - val_mse: 2.5247\n",
      "Epoch 338/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3122 - mae: 0.3796 - mse: 56.9870 - val_loss: 0.1505 - val_mae: 0.2158 - val_mse: 2.5247\n",
      "Epoch 339/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3122 - mae: 0.3796 - mse: 56.9842 - val_loss: 0.1505 - val_mae: 0.2158 - val_mse: 2.5247\n",
      "Epoch 340/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3122 - mae: 0.3796 - mse: 56.9814 - val_loss: 0.1505 - val_mae: 0.2158 - val_mse: 2.5246\n",
      "Epoch 341/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3122 - mae: 0.3795 - mse: 56.9787 - val_loss: 0.1505 - val_mae: 0.2158 - val_mse: 2.5246\n",
      "Epoch 342/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3122 - mae: 0.3795 - mse: 56.9759 - val_loss: 0.1505 - val_mae: 0.2158 - val_mse: 2.5246\n",
      "Epoch 343/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3122 - mae: 0.3795 - mse: 56.9732 - val_loss: 0.1505 - val_mae: 0.2158 - val_mse: 2.5246\n",
      "Epoch 344/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3122 - mae: 0.3795 - mse: 56.9704 - val_loss: 0.1505 - val_mae: 0.2158 - val_mse: 2.5246\n",
      "Epoch 345/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3121 - mae: 0.3795 - mse: 56.9676 - val_loss: 0.1505 - val_mae: 0.2158 - val_mse: 2.5245\n",
      "Epoch 346/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3121 - mae: 0.3795 - mse: 56.9649 - val_loss: 0.1505 - val_mae: 0.2158 - val_mse: 2.5245\n",
      "Epoch 347/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3121 - mae: 0.3795 - mse: 56.9621 - val_loss: 0.1505 - val_mae: 0.2158 - val_mse: 2.5245\n",
      "Epoch 348/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3121 - mae: 0.3794 - mse: 56.9593 - val_loss: 0.1505 - val_mae: 0.2158 - val_mse: 2.5245\n",
      "Epoch 349/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3121 - mae: 0.3794 - mse: 56.9566 - val_loss: 0.1505 - val_mae: 0.2158 - val_mse: 2.5244\n",
      "Epoch 350/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3121 - mae: 0.3794 - mse: 56.9538 - val_loss: 0.1505 - val_mae: 0.2158 - val_mse: 2.5244\n",
      "Epoch 351/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3121 - mae: 0.3794 - mse: 56.9511 - val_loss: 0.1505 - val_mae: 0.2158 - val_mse: 2.5244\n",
      "Epoch 352/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3121 - mae: 0.3794 - mse: 56.9483 - val_loss: 0.1505 - val_mae: 0.2158 - val_mse: 2.5244\n",
      "Epoch 353/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3121 - mae: 0.3794 - mse: 56.9456 - val_loss: 0.1505 - val_mae: 0.2158 - val_mse: 2.5244\n",
      "Epoch 354/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3120 - mae: 0.3794 - mse: 56.9428 - val_loss: 0.1505 - val_mae: 0.2158 - val_mse: 2.5243\n",
      "Epoch 355/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3120 - mae: 0.3794 - mse: 56.9401 - val_loss: 0.1505 - val_mae: 0.2158 - val_mse: 2.5243\n",
      "Epoch 356/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3120 - mae: 0.3793 - mse: 56.9373 - val_loss: 0.1505 - val_mae: 0.2158 - val_mse: 2.5243\n",
      "Epoch 357/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3120 - mae: 0.3793 - mse: 56.9346 - val_loss: 0.1505 - val_mae: 0.2157 - val_mse: 2.5243\n",
      "Epoch 358/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3120 - mae: 0.3793 - mse: 56.9318 - val_loss: 0.1505 - val_mae: 0.2157 - val_mse: 2.5242\n",
      "Epoch 359/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3120 - mae: 0.3793 - mse: 56.9291 - val_loss: 0.1505 - val_mae: 0.2157 - val_mse: 2.5242\n",
      "Epoch 360/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3120 - mae: 0.3793 - mse: 56.9263 - val_loss: 0.1505 - val_mae: 0.2157 - val_mse: 2.5242\n",
      "Epoch 361/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3120 - mae: 0.3793 - mse: 56.9236 - val_loss: 0.1505 - val_mae: 0.2157 - val_mse: 2.5242\n",
      "Epoch 362/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3120 - mae: 0.3793 - mse: 56.9208 - val_loss: 0.1505 - val_mae: 0.2157 - val_mse: 2.5242\n",
      "Epoch 363/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3119 - mae: 0.3793 - mse: 56.9181 - val_loss: 0.1505 - val_mae: 0.2157 - val_mse: 2.5241\n",
      "Epoch 364/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3119 - mae: 0.3792 - mse: 56.9153 - val_loss: 0.1505 - val_mae: 0.2157 - val_mse: 2.5241\n",
      "Epoch 365/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3119 - mae: 0.3792 - mse: 56.9126 - val_loss: 0.1505 - val_mae: 0.2157 - val_mse: 2.5241\n",
      "Epoch 366/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3119 - mae: 0.3792 - mse: 56.9099 - val_loss: 0.1505 - val_mae: 0.2157 - val_mse: 2.5241\n",
      "Epoch 367/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3119 - mae: 0.3792 - mse: 56.9071 - val_loss: 0.1504 - val_mae: 0.2157 - val_mse: 2.5241\n",
      "Epoch 368/10000\n",
      "1294/1294 [==============================] - 8s 6ms/step - loss: 0.3119 - mae: 0.3792 - mse: 56.9044 - val_loss: 0.1504 - val_mae: 0.2157 - val_mse: 2.5240\n",
      "Epoch 369/10000\n",
      " 644/1294 [=============>................] - ETA: 3s - loss: 0.2215 - mae: 0.2813 - mse: 26.5140"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "sY9EgNpOSp8s"
   },
   "source": [
    "c = 'Adair'\n",
    "s = 'OK'\n",
    "r = df[(df['state']==s)&(df['county']==c)]"
   ],
   "execution_count": 98,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "lCSQPQbsSp87"
   },
   "source": [
    "model4 = tf.keras.models.load_model('checkpoint2.h5')\n",
    "row = r.drop(columns=['state','county'])\n",
    "row = row.to_numpy()\n",
    "dataset = getDataset(row,8,False)"
   ],
   "execution_count": 99,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "IKZQWaW-Sp9N",
    "outputId": "a201a597-bade-49d6-fd9b-230df224ce05",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    }
   },
   "source": [
    "results = model4.predict(dataset)"
   ],
   "execution_count": 100,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:8 out of the last 27 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000151FF7C4488> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "J-2cK6uCSp9k",
    "outputId": "73307db3-2e1e-4dd5-ff6a-62ae54c0caa9",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    }
   },
   "source": [
    "dem = np.array(r.values.tolist()[0][2::3])\n",
    "gop = np.array(r.values.tolist()[0][3::3])\n",
    "other = np.array(r.values.tolist()[0][4::3])\n",
    "plotCounty(s,c,dem,gop,other,results)"
   ],
   "execution_count": 101,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ]
  }
 ]
}