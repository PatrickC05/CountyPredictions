{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "colab": {
   "name": "predictCounty.ipynb",
   "provenance": []
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "CsluoFv7Sp2j"
   },
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ],
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "tlcuuCFhSp3G"
   },
   "source": [
    "years = range(2000,2020,4)\n",
    "df = pd.read_csv('counties.csv')"
   ],
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "PrR6Do9iSp3f",
    "outputId": "34216620-414e-48ec-dbbd-7d53fb63e935",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    }
   },
   "source": [
    "data = df.drop(columns=['state', 'county'])\n",
    "data = data.dropna(axis=0,how='any')\n",
    "data.describe()"
   ],
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "              2000D          2000R          2000O         2004D         2004R  \\\ncount  2.875000e+03    2875.000000    2875.000000  2.875000e+03  2.875000e+03   \nmean   1.699424e+04   16533.856000    1352.073391  1.956915e+04  2.028555e+04   \nstd    5.942532e+04   39923.542134    4232.141433  6.769202e+04  4.869764e+04   \nmin    1.400000e+01     106.000000       3.000000  1.200000e+01  6.500000e+01   \n25%    1.751500e+03    2689.000000     109.000000  1.882500e+03  3.124500e+03   \n50%    3.830000e+03    5363.000000     278.000000  4.177000e+03  6.522000e+03   \n75%    9.580500e+03   13166.000000     886.500000  1.078350e+04  1.646800e+04   \nmax    1.710505e+06  871930.000000  112719.000000  1.907736e+06  1.076225e+06   \n\n              2004O         2008D          2008R         2008O         2012D  \\\ncount   2875.000000  2.875000e+03    2875.000000   2875.000000  2.875000e+03   \nmean     508.671652  2.298134e+04   19539.632696    610.991304  2.171365e+04   \nstd     1976.171276  7.944593e+04   45955.629730   1913.652362  7.628626e+04   \nmin        0.000000  8.000000e+00      67.000000      0.000000  5.000000e+00   \n25%       43.000000  1.930500e+03    2956.000000     78.500000  1.662500e+03   \n50%       98.000000  4.493000e+03    6297.000000    182.000000  4.014000e+03   \n75%      302.000000  1.262450e+04   16042.000000    453.500000  1.147700e+04   \nmax    39515.000000  2.295853e+06  956425.000000  65970.000000  2.216903e+06   \n\n               2012R         2012O         2016D          2016R          2016O  \ncount    2875.000000   2875.000000  2.875000e+03    2875.000000    2875.000000  \nmean    19761.090783    886.516522  2.175607e+04   20524.178783    2543.772522  \nstd     45649.528293   3056.034805  8.298175e+04   44262.493478    7726.198627  \nmin        54.000000      1.000000  4.000000e+00      58.000000       3.000000  \n25%      3021.500000     84.000000  1.240000e+03    3346.000000     195.000000  \n50%      6398.000000    195.000000  3.199000e+03    7194.000000     529.000000  \n75%     16391.500000    564.000000  9.940500e+03   17945.500000    1736.500000  \nmax    885333.000000  78831.000000  2.464364e+06  769743.000000  200201.000000  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>2000D</th>\n      <th>2000R</th>\n      <th>2000O</th>\n      <th>2004D</th>\n      <th>2004R</th>\n      <th>2004O</th>\n      <th>2008D</th>\n      <th>2008R</th>\n      <th>2008O</th>\n      <th>2012D</th>\n      <th>2012R</th>\n      <th>2012O</th>\n      <th>2016D</th>\n      <th>2016R</th>\n      <th>2016O</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>2.875000e+03</td>\n      <td>2875.000000</td>\n      <td>2875.000000</td>\n      <td>2.875000e+03</td>\n      <td>2.875000e+03</td>\n      <td>2875.000000</td>\n      <td>2.875000e+03</td>\n      <td>2875.000000</td>\n      <td>2875.000000</td>\n      <td>2.875000e+03</td>\n      <td>2875.000000</td>\n      <td>2875.000000</td>\n      <td>2.875000e+03</td>\n      <td>2875.000000</td>\n      <td>2875.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>1.699424e+04</td>\n      <td>16533.856000</td>\n      <td>1352.073391</td>\n      <td>1.956915e+04</td>\n      <td>2.028555e+04</td>\n      <td>508.671652</td>\n      <td>2.298134e+04</td>\n      <td>19539.632696</td>\n      <td>610.991304</td>\n      <td>2.171365e+04</td>\n      <td>19761.090783</td>\n      <td>886.516522</td>\n      <td>2.175607e+04</td>\n      <td>20524.178783</td>\n      <td>2543.772522</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>5.942532e+04</td>\n      <td>39923.542134</td>\n      <td>4232.141433</td>\n      <td>6.769202e+04</td>\n      <td>4.869764e+04</td>\n      <td>1976.171276</td>\n      <td>7.944593e+04</td>\n      <td>45955.629730</td>\n      <td>1913.652362</td>\n      <td>7.628626e+04</td>\n      <td>45649.528293</td>\n      <td>3056.034805</td>\n      <td>8.298175e+04</td>\n      <td>44262.493478</td>\n      <td>7726.198627</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>1.400000e+01</td>\n      <td>106.000000</td>\n      <td>3.000000</td>\n      <td>1.200000e+01</td>\n      <td>6.500000e+01</td>\n      <td>0.000000</td>\n      <td>8.000000e+00</td>\n      <td>67.000000</td>\n      <td>0.000000</td>\n      <td>5.000000e+00</td>\n      <td>54.000000</td>\n      <td>1.000000</td>\n      <td>4.000000e+00</td>\n      <td>58.000000</td>\n      <td>3.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>1.751500e+03</td>\n      <td>2689.000000</td>\n      <td>109.000000</td>\n      <td>1.882500e+03</td>\n      <td>3.124500e+03</td>\n      <td>43.000000</td>\n      <td>1.930500e+03</td>\n      <td>2956.000000</td>\n      <td>78.500000</td>\n      <td>1.662500e+03</td>\n      <td>3021.500000</td>\n      <td>84.000000</td>\n      <td>1.240000e+03</td>\n      <td>3346.000000</td>\n      <td>195.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>3.830000e+03</td>\n      <td>5363.000000</td>\n      <td>278.000000</td>\n      <td>4.177000e+03</td>\n      <td>6.522000e+03</td>\n      <td>98.000000</td>\n      <td>4.493000e+03</td>\n      <td>6297.000000</td>\n      <td>182.000000</td>\n      <td>4.014000e+03</td>\n      <td>6398.000000</td>\n      <td>195.000000</td>\n      <td>3.199000e+03</td>\n      <td>7194.000000</td>\n      <td>529.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>9.580500e+03</td>\n      <td>13166.000000</td>\n      <td>886.500000</td>\n      <td>1.078350e+04</td>\n      <td>1.646800e+04</td>\n      <td>302.000000</td>\n      <td>1.262450e+04</td>\n      <td>16042.000000</td>\n      <td>453.500000</td>\n      <td>1.147700e+04</td>\n      <td>16391.500000</td>\n      <td>564.000000</td>\n      <td>9.940500e+03</td>\n      <td>17945.500000</td>\n      <td>1736.500000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>1.710505e+06</td>\n      <td>871930.000000</td>\n      <td>112719.000000</td>\n      <td>1.907736e+06</td>\n      <td>1.076225e+06</td>\n      <td>39515.000000</td>\n      <td>2.295853e+06</td>\n      <td>956425.000000</td>\n      <td>65970.000000</td>\n      <td>2.216903e+06</td>\n      <td>885333.000000</td>\n      <td>78831.000000</td>\n      <td>2.464364e+06</td>\n      <td>769743.000000</td>\n      <td>200201.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "0-LVluzeSp4P"
   },
   "source": [
    "data = data.to_numpy(dtype=np.int64)\n"
   ],
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "YMNKgPOuSp5D"
   },
   "source": [
    "def plotCounty(state,county,dem,gop,other,predicted=None):\n",
    "    if predicted is None:\n",
    "        years = range(2000,2020,4)\n",
    "        plt.figure()\n",
    "        plt.plot(years,dem,'b',label='Democrat')\n",
    "        plt.plot(years,gop,'r',label='Republican')\n",
    "        plt.plot(years,other,'y',label='Other')\n",
    "        plt.plot(years,dem+gop+other,'k',label='Total')\n",
    "    else:\n",
    "        years = range(2000,2024,4)\n",
    "        plt.figure()\n",
    "        d = np.append(dem,predicted[0]*20000)\n",
    "        r = np.append(gop,predicted[1]*20000)\n",
    "        o = np.append(other,predicted[2]*250)\n",
    "        plt.plot(years,d,'b',label='Democrat')\n",
    "        plt.plot(years,r,'r',label='Republican')\n",
    "        plt.plot(years,o,'y',label='Other')\n",
    "        plt.plot(years,d+r+o,'k',label='Total')\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Votes')\n",
    "    plt.title(county+', '+state)\n",
    "    plt.xticks(years)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ],
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "BhlfhjBISp6o",
    "outputId": "9e21c410-521c-4fdd-c73b-9e49ce965506",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    }
   },
   "source": [
    "def convertbyParty(point):\n",
    "    d = point[:,::3]/20000\n",
    "    r = point[:,1::3]/20000\n",
    "    o = point[:,2::3]/250\n",
    "    return d,r,o\n",
    "print(df.loc[0])\n",
    "convertbyParty(np.expand_dims(df.loc[0].values[2:],0))"
   ],
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state          AL\n",
      "county    Autauga\n",
      "2000D        4942\n",
      "2000R       11993\n",
      "2000O         273\n",
      "2004D        4758\n",
      "2004R       15196\n",
      "2004O         127\n",
      "2008D        6093\n",
      "2008R       17403\n",
      "2008O         145\n",
      "2012D        6363\n",
      "2012R       17379\n",
      "2012O         190\n",
      "2016D        5936\n",
      "2016R       18172\n",
      "2016O         865\n",
      "Name: 0, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": "(array([[0.2471, 0.2379, 0.30465, 0.31815, 0.2968]], dtype=object),\n array([[0.59965, 0.7598, 0.87015, 0.86895, 0.9086]], dtype=object),\n array([[1.092, 0.508, 0.58, 0.76, 3.46]], dtype=object))"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "sKpS8inlSp7A"
   },
   "source": [
    "def getDataset(data,batch_size,split=True):\n",
    "    data = np.expand_dims(data,axis=-1)\n",
    "    if split:\n",
    "        np.random.shuffle(data)\n",
    "    num = data.shape[0]\n",
    "    train_data = data[:int(num*0.6)]\n",
    "    val_data = data[int(num*0.6):int(num*0.8)]\n",
    "    test_data = data[int(num*0.8):]\n",
    "    def createDS(point,label=True):\n",
    "        d,r,o=convertbyParty(point)\n",
    "\n",
    "        if label:\n",
    "            new_data = np.vstack((d,r))\n",
    "            new_data = np.vstack((new_data,new_data*1.5,new_data*2))\n",
    "        else:\n",
    "            new_data = np.vstack((d,r,o))\n",
    "        ds = tf.data.Dataset.from_tensor_slices(new_data)\n",
    "        if label:\n",
    "            ds = ds.map(lambda p: (p[:-1],p[-1]))\n",
    "        else:\n",
    "            ds = ds.map(lambda p: p[1:])\n",
    "        return ds.batch(batch_size).prefetch(1) if label else ds\n",
    "    if split:\n",
    "        return createDS(train_data),createDS(val_data),createDS(test_data)\n",
    "    else:\n",
    "        return createDS(data,False).batch(100000)"
   ],
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "FuRS8y-8Sp7O",
    "outputId": "b45aad36-054b-4939-e5ef-dca756d7c238",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    }
   },
   "source": [
    "train_ds, val_ds, test_ds = getDataset(data,8)\n",
    "# list(getDataset(data,1,False))[:3]\n",
    "train_ds\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "bHcc7PIxSp7h"
   },
   "source": [
    "# model = tf.keras.Sequential([\n",
    "#     tf.keras.layers.Conv1D(2,2,activation='relu',input_shape=(4,1)),\n",
    "#     tf.keras.layers.Conv1D(2,2,activation='relu'),\n",
    "#     tf.keras.layers.Flatten(),\n",
    "#     tf.keras.layers.Dense(4,activation='relu'),\n",
    "#     tf.keras.layers.Reshape((4,1)),\n",
    "#     tf.keras.layers.LSTM(8,return_sequences=True),\n",
    "#     tf.keras.layers.LSTM(8,return_sequences=True),\n",
    "#     tf.keras.layers.LSTM(8,return_sequences=True),\n",
    "# ])"
   ],
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "bKa-U7wESp7w"
   },
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.LSTM(8,input_shape=(4,1),return_sequences=True),\n",
    "    tf.keras.layers.LSTM(8),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])"
   ],
   "execution_count": 2,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-2-2839e1f93c62>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m model = tf.keras.Sequential([\n\u001B[0m\u001B[0;32m      2\u001B[0m     \u001B[0mtf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkeras\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlayers\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mLSTM\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m8\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0minput_shape\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m4\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mreturn_sequences\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m     \u001B[0mtf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkeras\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlayers\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mLSTM\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m8\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m     \u001B[0mtf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mkeras\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlayers\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mDense\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m ])\n",
      "\u001B[1;31mNameError\u001B[0m: name 'tf' is not defined"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "Sm_qpTzDSp7-",
    "outputId": "c0d4345f-93cb-4076-f463-76d78d4ce9ee",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    }
   },
   "source": [
    "lr_schedule = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-6 * 10**(epoch / 20))\n",
    "optimizer = tf.keras.optimizers.Adam(lr=1e-6)\n",
    "model.compile(loss='mae',optimizer=optimizer,metrics=['mse'])\n",
    "model.summary()"
   ],
   "execution_count": 71,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 4, 8)              320       \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 8)                 544       \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 873\n",
      "Trainable params: 873\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "SR-cfHLeSp8N"
   },
   "source": [
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\"model.h5\", save_best_only=True)\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=20)\n"
   ],
   "execution_count": 64,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "lmf1QmWbSp8c",
    "outputId": "948b9bd5-47ed-4156-f6d4-36aaf26034c0",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    }
   },
   "source": [
    "# history = model.fit(train_ds,epochs=100,validation_data=val_ds,callbacks=[lr_schedule])\n",
    "# plt.semilogx(history.history[\"lr\"], history.history[\"loss\"])\n",
    "# plt.axis([1e-6, 10, 0, 3])\n",
    "def getlr(epoch):\n",
    "  return 1e-4\n",
    "new_lr = tf.keras.callbacks.LearningRateScheduler(getlr)\n",
    "model.fit(train_ds,epochs=1000,validation_data=val_ds,callbacks=[model_checkpoint,early_stopping])"
   ],
   "execution_count": 72,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1562 - mse: 2.2603 - val_loss: 0.1809 - val_mse: 1.0886\n",
      "Epoch 2/1000\n",
      "1294/1294 [==============================] - 9s 7ms/step - loss: 0.1562 - mse: 2.2603 - val_loss: 0.1809 - val_mse: 1.0885\n",
      "Epoch 3/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1562 - mse: 2.2603 - val_loss: 0.1809 - val_mse: 1.0884\n",
      "Epoch 4/1000\n",
      "1294/1294 [==============================] - 9s 7ms/step - loss: 0.1562 - mse: 2.2603 - val_loss: 0.1809 - val_mse: 1.0883\n",
      "Epoch 5/1000\n",
      "1294/1294 [==============================] - 9s 7ms/step - loss: 0.1562 - mse: 2.2602 - val_loss: 0.1809 - val_mse: 1.0882\n",
      "Epoch 6/1000\n",
      "1294/1294 [==============================] - 9s 7ms/step - loss: 0.1562 - mse: 2.2602 - val_loss: 0.1808 - val_mse: 1.0881\n",
      "Epoch 7/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1562 - mse: 2.2602 - val_loss: 0.1808 - val_mse: 1.0880\n",
      "Epoch 8/1000\n",
      "1294/1294 [==============================] - 9s 7ms/step - loss: 0.1562 - mse: 2.2602 - val_loss: 0.1808 - val_mse: 1.0879\n",
      "Epoch 9/1000\n",
      "1294/1294 [==============================] - 9s 7ms/step - loss: 0.1562 - mse: 2.2602 - val_loss: 0.1808 - val_mse: 1.0879\n",
      "Epoch 10/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1562 - mse: 2.2602 - val_loss: 0.1808 - val_mse: 1.0878\n",
      "Epoch 11/1000\n",
      "1294/1294 [==============================] - 9s 7ms/step - loss: 0.1562 - mse: 2.2602 - val_loss: 0.1808 - val_mse: 1.0878\n",
      "Epoch 12/1000\n",
      "1294/1294 [==============================] - 9s 7ms/step - loss: 0.1562 - mse: 2.2602 - val_loss: 0.1808 - val_mse: 1.0878\n",
      "Epoch 13/1000\n",
      "1294/1294 [==============================] - 9s 7ms/step - loss: 0.1562 - mse: 2.2601 - val_loss: 0.1808 - val_mse: 1.0877\n",
      "Epoch 14/1000\n",
      "1294/1294 [==============================] - 9s 7ms/step - loss: 0.1562 - mse: 2.2601 - val_loss: 0.1808 - val_mse: 1.0877\n",
      "Epoch 15/1000\n",
      "1294/1294 [==============================] - 9s 7ms/step - loss: 0.1562 - mse: 2.2601 - val_loss: 0.1808 - val_mse: 1.0876\n",
      "Epoch 16/1000\n",
      "1294/1294 [==============================] - 9s 7ms/step - loss: 0.1562 - mse: 2.2601 - val_loss: 0.1808 - val_mse: 1.0876\n",
      "Epoch 17/1000\n",
      "1294/1294 [==============================] - 9s 7ms/step - loss: 0.1562 - mse: 2.2601 - val_loss: 0.1807 - val_mse: 1.0876\n",
      "Epoch 18/1000\n",
      "1294/1294 [==============================] - 9s 7ms/step - loss: 0.1562 - mse: 2.2601 - val_loss: 0.1807 - val_mse: 1.0875\n",
      "Epoch 19/1000\n",
      "1294/1294 [==============================] - 9s 7ms/step - loss: 0.1562 - mse: 2.2601 - val_loss: 0.1807 - val_mse: 1.0875\n",
      "Epoch 20/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1562 - mse: 2.2601 - val_loss: 0.1807 - val_mse: 1.0875\n",
      "Epoch 21/1000\n",
      "1294/1294 [==============================] - 9s 7ms/step - loss: 0.1562 - mse: 2.2601 - val_loss: 0.1807 - val_mse: 1.0874\n",
      "Epoch 22/1000\n",
      "1294/1294 [==============================] - 9s 7ms/step - loss: 0.1562 - mse: 2.2600 - val_loss: 0.1807 - val_mse: 1.0874\n",
      "Epoch 23/1000\n",
      "1294/1294 [==============================] - 9s 7ms/step - loss: 0.1562 - mse: 2.2600 - val_loss: 0.1807 - val_mse: 1.0874\n",
      "Epoch 24/1000\n",
      "1294/1294 [==============================] - 9s 7ms/step - loss: 0.1562 - mse: 2.2600 - val_loss: 0.1807 - val_mse: 1.0874\n",
      "Epoch 25/1000\n",
      "1294/1294 [==============================] - 9s 7ms/step - loss: 0.1561 - mse: 2.2600 - val_loss: 0.1807 - val_mse: 1.0874\n",
      "Epoch 26/1000\n",
      "1294/1294 [==============================] - 9s 7ms/step - loss: 0.1562 - mse: 2.2600 - val_loss: 0.1807 - val_mse: 1.0873\n",
      "Epoch 27/1000\n",
      "1294/1294 [==============================] - 9s 7ms/step - loss: 0.1561 - mse: 2.2600 - val_loss: 0.1807 - val_mse: 1.0873\n",
      "Epoch 28/1000\n",
      "1294/1294 [==============================] - 9s 7ms/step - loss: 0.1561 - mse: 2.2600 - val_loss: 0.1807 - val_mse: 1.0873\n",
      "Epoch 29/1000\n",
      "1294/1294 [==============================] - 9s 7ms/step - loss: 0.1561 - mse: 2.2600 - val_loss: 0.1807 - val_mse: 1.0873\n",
      "Epoch 30/1000\n",
      "1294/1294 [==============================] - 9s 7ms/step - loss: 0.1561 - mse: 2.2599 - val_loss: 0.1807 - val_mse: 1.0873\n",
      "Epoch 31/1000\n",
      "1294/1294 [==============================] - 9s 7ms/step - loss: 0.1561 - mse: 2.2599 - val_loss: 0.1807 - val_mse: 1.0872\n",
      "Epoch 32/1000\n",
      "1294/1294 [==============================] - 9s 7ms/step - loss: 0.1561 - mse: 2.2599 - val_loss: 0.1807 - val_mse: 1.0872\n",
      "Epoch 33/1000\n",
      "1294/1294 [==============================] - 9s 7ms/step - loss: 0.1561 - mse: 2.2599 - val_loss: 0.1807 - val_mse: 1.0872\n",
      "Epoch 34/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1561 - mse: 2.2599 - val_loss: 0.1807 - val_mse: 1.0872\n",
      "Epoch 35/1000\n",
      "1294/1294 [==============================] - 9s 7ms/step - loss: 0.1561 - mse: 2.2599 - val_loss: 0.1807 - val_mse: 1.0872\n",
      "Epoch 36/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1561 - mse: 2.2599 - val_loss: 0.1807 - val_mse: 1.0871\n",
      "Epoch 37/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1561 - mse: 2.2599 - val_loss: 0.1807 - val_mse: 1.0871\n",
      "Epoch 38/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1561 - mse: 2.2598 - val_loss: 0.1807 - val_mse: 1.0871\n",
      "Epoch 39/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1561 - mse: 2.2598 - val_loss: 0.1807 - val_mse: 1.0871\n",
      "Epoch 40/1000\n",
      "1294/1294 [==============================] - 9s 7ms/step - loss: 0.1561 - mse: 2.2598 - val_loss: 0.1807 - val_mse: 1.0871\n",
      "Epoch 41/1000\n",
      "1294/1294 [==============================] - 9s 7ms/step - loss: 0.1561 - mse: 2.2598 - val_loss: 0.1807 - val_mse: 1.0871\n",
      "Epoch 42/1000\n",
      "1294/1294 [==============================] - 9s 7ms/step - loss: 0.1561 - mse: 2.2598 - val_loss: 0.1807 - val_mse: 1.0871\n",
      "Epoch 43/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1561 - mse: 2.2598 - val_loss: 0.1807 - val_mse: 1.0870\n",
      "Epoch 44/1000\n",
      "1294/1294 [==============================] - 9s 7ms/step - loss: 0.1561 - mse: 2.2598 - val_loss: 0.1807 - val_mse: 1.0870\n",
      "Epoch 45/1000\n",
      "1294/1294 [==============================] - 9s 7ms/step - loss: 0.1561 - mse: 2.2598 - val_loss: 0.1807 - val_mse: 1.0870\n",
      "Epoch 46/1000\n",
      "1294/1294 [==============================] - 9s 7ms/step - loss: 0.1561 - mse: 2.2598 - val_loss: 0.1807 - val_mse: 1.0870\n",
      "Epoch 47/1000\n",
      "1294/1294 [==============================] - 9s 7ms/step - loss: 0.1561 - mse: 2.2597 - val_loss: 0.1807 - val_mse: 1.0870\n",
      "Epoch 48/1000\n",
      "1294/1294 [==============================] - 9s 7ms/step - loss: 0.1561 - mse: 2.2597 - val_loss: 0.1807 - val_mse: 1.0870\n",
      "Epoch 49/1000\n",
      "1294/1294 [==============================] - 9s 7ms/step - loss: 0.1561 - mse: 2.2597 - val_loss: 0.1807 - val_mse: 1.0869\n",
      "Epoch 50/1000\n",
      "1294/1294 [==============================] - 9s 7ms/step - loss: 0.1561 - mse: 2.2597 - val_loss: 0.1807 - val_mse: 1.0869\n",
      "Epoch 51/1000\n",
      "1294/1294 [==============================] - 9s 7ms/step - loss: 0.1561 - mse: 2.2597 - val_loss: 0.1807 - val_mse: 1.0869\n",
      "Epoch 52/1000\n",
      "1294/1294 [==============================] - 9s 7ms/step - loss: 0.1561 - mse: 2.2597 - val_loss: 0.1807 - val_mse: 1.0869\n",
      "Epoch 53/1000\n",
      "1294/1294 [==============================] - 9s 7ms/step - loss: 0.1561 - mse: 2.2597 - val_loss: 0.1807 - val_mse: 1.0869\n",
      "Epoch 54/1000\n",
      "1294/1294 [==============================] - 9s 7ms/step - loss: 0.1561 - mse: 2.2596 - val_loss: 0.1807 - val_mse: 1.0869\n",
      "Epoch 55/1000\n",
      "1294/1294 [==============================] - 9s 7ms/step - loss: 0.1561 - mse: 2.2596 - val_loss: 0.1807 - val_mse: 1.0869\n",
      "Epoch 56/1000\n",
      "1294/1294 [==============================] - 9s 7ms/step - loss: 0.1561 - mse: 2.2596 - val_loss: 0.1807 - val_mse: 1.0868\n",
      "Epoch 57/1000\n",
      "1294/1294 [==============================] - 9s 7ms/step - loss: 0.1561 - mse: 2.2596 - val_loss: 0.1807 - val_mse: 1.0868\n",
      "Epoch 58/1000\n",
      "1294/1294 [==============================] - 9s 7ms/step - loss: 0.1561 - mse: 2.2596 - val_loss: 0.1807 - val_mse: 1.0868\n",
      "Epoch 59/1000\n",
      "1294/1294 [==============================] - 9s 7ms/step - loss: 0.1561 - mse: 2.2596 - val_loss: 0.1807 - val_mse: 1.0868\n",
      "Epoch 60/1000\n",
      "1294/1294 [==============================] - 9s 7ms/step - loss: 0.1561 - mse: 2.2596 - val_loss: 0.1807 - val_mse: 1.0868\n",
      "Epoch 61/1000\n",
      "1294/1294 [==============================] - 9s 7ms/step - loss: 0.1561 - mse: 2.2595 - val_loss: 0.1807 - val_mse: 1.0867\n",
      "Epoch 62/1000\n",
      "1294/1294 [==============================] - 9s 7ms/step - loss: 0.1561 - mse: 2.2595 - val_loss: 0.1807 - val_mse: 1.0867\n",
      "Epoch 63/1000\n",
      "1294/1294 [==============================] - 9s 7ms/step - loss: 0.1561 - mse: 2.2595 - val_loss: 0.1807 - val_mse: 1.0867\n",
      "Epoch 64/1000\n",
      "1294/1294 [==============================] - 9s 7ms/step - loss: 0.1561 - mse: 2.2595 - val_loss: 0.1807 - val_mse: 1.0867\n",
      "Epoch 65/1000\n",
      "1294/1294 [==============================] - 9s 7ms/step - loss: 0.1561 - mse: 2.2595 - val_loss: 0.1807 - val_mse: 1.0867\n",
      "Epoch 66/1000\n",
      "1294/1294 [==============================] - 9s 7ms/step - loss: 0.1561 - mse: 2.2595 - val_loss: 0.1807 - val_mse: 1.0867\n",
      "Epoch 67/1000\n",
      "1294/1294 [==============================] - 9s 7ms/step - loss: 0.1561 - mse: 2.2594 - val_loss: 0.1807 - val_mse: 1.0866\n",
      "Epoch 68/1000\n",
      "1294/1294 [==============================] - 9s 7ms/step - loss: 0.1561 - mse: 2.2594 - val_loss: 0.1807 - val_mse: 1.0866\n",
      "Epoch 69/1000\n",
      "1294/1294 [==============================] - 9s 7ms/step - loss: 0.1561 - mse: 2.2594 - val_loss: 0.1807 - val_mse: 1.0866\n",
      "Epoch 70/1000\n",
      "1294/1294 [==============================] - 9s 7ms/step - loss: 0.1561 - mse: 2.2594 - val_loss: 0.1807 - val_mse: 1.0866\n",
      "Epoch 71/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1561 - mse: 2.2594 - val_loss: 0.1807 - val_mse: 1.0866\n",
      "Epoch 72/1000\n",
      "1294/1294 [==============================] - 9s 7ms/step - loss: 0.1561 - mse: 2.2594 - val_loss: 0.1807 - val_mse: 1.0865\n",
      "Epoch 73/1000\n",
      "1294/1294 [==============================] - 9s 7ms/step - loss: 0.1561 - mse: 2.2594 - val_loss: 0.1807 - val_mse: 1.0865\n",
      "Epoch 74/1000\n",
      "1294/1294 [==============================] - 9s 7ms/step - loss: 0.1561 - mse: 2.2594 - val_loss: 0.1807 - val_mse: 1.0865\n",
      "Epoch 75/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1561 - mse: 2.2593 - val_loss: 0.1807 - val_mse: 1.0865\n",
      "Epoch 76/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1561 - mse: 2.2593 - val_loss: 0.1807 - val_mse: 1.0865\n",
      "Epoch 77/1000\n",
      "1294/1294 [==============================] - 9s 7ms/step - loss: 0.1561 - mse: 2.2593 - val_loss: 0.1807 - val_mse: 1.0864\n",
      "Epoch 78/1000\n",
      "1294/1294 [==============================] - 9s 7ms/step - loss: 0.1561 - mse: 2.2593 - val_loss: 0.1807 - val_mse: 1.0864\n",
      "Epoch 79/1000\n",
      "1294/1294 [==============================] - 9s 7ms/step - loss: 0.1561 - mse: 2.2593 - val_loss: 0.1807 - val_mse: 1.0864\n",
      "Epoch 80/1000\n",
      "1294/1294 [==============================] - 9s 7ms/step - loss: 0.1561 - mse: 2.2593 - val_loss: 0.1807 - val_mse: 1.0864\n",
      "Epoch 81/1000\n",
      "1294/1294 [==============================] - 9s 7ms/step - loss: 0.1561 - mse: 2.2592 - val_loss: 0.1807 - val_mse: 1.0863\n",
      "Epoch 82/1000\n",
      "1294/1294 [==============================] - 9s 7ms/step - loss: 0.1561 - mse: 2.2592 - val_loss: 0.1807 - val_mse: 1.0863\n",
      "Epoch 83/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1560 - mse: 2.2592 - val_loss: 0.1807 - val_mse: 1.0863\n",
      "Epoch 84/1000\n",
      "1294/1294 [==============================] - 9s 7ms/step - loss: 0.1560 - mse: 2.2592 - val_loss: 0.1807 - val_mse: 1.0863\n",
      "Epoch 85/1000\n",
      "1294/1294 [==============================] - 9s 7ms/step - loss: 0.1560 - mse: 2.2592 - val_loss: 0.1807 - val_mse: 1.0863\n",
      "Epoch 86/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1560 - mse: 2.2592 - val_loss: 0.1807 - val_mse: 1.0862\n",
      "Epoch 87/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1560 - mse: 2.2592 - val_loss: 0.1807 - val_mse: 1.0862\n",
      "Epoch 88/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1560 - mse: 2.2591 - val_loss: 0.1807 - val_mse: 1.0862\n",
      "Epoch 89/1000\n",
      "1294/1294 [==============================] - 9s 7ms/step - loss: 0.1560 - mse: 2.2591 - val_loss: 0.1807 - val_mse: 1.0862\n",
      "Epoch 90/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1560 - mse: 2.2591 - val_loss: 0.1807 - val_mse: 1.0862\n",
      "Epoch 91/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1560 - mse: 2.2591 - val_loss: 0.1807 - val_mse: 1.0861\n",
      "Epoch 92/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1560 - mse: 2.2591 - val_loss: 0.1807 - val_mse: 1.0861\n",
      "Epoch 93/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1560 - mse: 2.2591 - val_loss: 0.1807 - val_mse: 1.0861\n",
      "Epoch 94/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1560 - mse: 2.2590 - val_loss: 0.1807 - val_mse: 1.0861\n",
      "Epoch 95/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1560 - mse: 2.2590 - val_loss: 0.1807 - val_mse: 1.0861\n",
      "Epoch 96/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1560 - mse: 2.2590 - val_loss: 0.1807 - val_mse: 1.0860\n",
      "Epoch 97/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1560 - mse: 2.2590 - val_loss: 0.1807 - val_mse: 1.0860\n",
      "Epoch 98/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1560 - mse: 2.2590 - val_loss: 0.1807 - val_mse: 1.0860\n",
      "Epoch 99/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1560 - mse: 2.2590 - val_loss: 0.1807 - val_mse: 1.0859\n",
      "Epoch 100/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1560 - mse: 2.2590 - val_loss: 0.1807 - val_mse: 1.0859\n",
      "Epoch 101/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1560 - mse: 2.2590 - val_loss: 0.1807 - val_mse: 1.0859\n",
      "Epoch 102/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1560 - mse: 2.2589 - val_loss: 0.1807 - val_mse: 1.0859\n",
      "Epoch 103/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1560 - mse: 2.2589 - val_loss: 0.1807 - val_mse: 1.0859\n",
      "Epoch 104/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1560 - mse: 2.2589 - val_loss: 0.1807 - val_mse: 1.0859\n",
      "Epoch 105/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1560 - mse: 2.2589 - val_loss: 0.1807 - val_mse: 1.0858\n",
      "Epoch 106/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1560 - mse: 2.2589 - val_loss: 0.1807 - val_mse: 1.0858\n",
      "Epoch 107/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1560 - mse: 2.2589 - val_loss: 0.1807 - val_mse: 1.0858\n",
      "Epoch 108/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1560 - mse: 2.2588 - val_loss: 0.1807 - val_mse: 1.0858\n",
      "Epoch 109/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1560 - mse: 2.2588 - val_loss: 0.1807 - val_mse: 1.0857\n",
      "Epoch 110/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1560 - mse: 2.2588 - val_loss: 0.1807 - val_mse: 1.0857\n",
      "Epoch 111/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1560 - mse: 2.2588 - val_loss: 0.1807 - val_mse: 1.0857\n",
      "Epoch 112/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1560 - mse: 2.2588 - val_loss: 0.1807 - val_mse: 1.0857\n",
      "Epoch 113/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1560 - mse: 2.2588 - val_loss: 0.1807 - val_mse: 1.0857\n",
      "Epoch 114/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1560 - mse: 2.2587 - val_loss: 0.1807 - val_mse: 1.0856\n",
      "Epoch 115/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1560 - mse: 2.2587 - val_loss: 0.1807 - val_mse: 1.0856\n",
      "Epoch 116/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1560 - mse: 2.2587 - val_loss: 0.1807 - val_mse: 1.0856\n",
      "Epoch 117/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1560 - mse: 2.2587 - val_loss: 0.1807 - val_mse: 1.0856\n",
      "Epoch 118/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1560 - mse: 2.2587 - val_loss: 0.1807 - val_mse: 1.0856\n",
      "Epoch 119/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1560 - mse: 2.2587 - val_loss: 0.1807 - val_mse: 1.0855\n",
      "Epoch 120/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1560 - mse: 2.2587 - val_loss: 0.1807 - val_mse: 1.0855\n",
      "Epoch 121/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1560 - mse: 2.2586 - val_loss: 0.1807 - val_mse: 1.0855\n",
      "Epoch 122/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1560 - mse: 2.2586 - val_loss: 0.1807 - val_mse: 1.0855\n",
      "Epoch 123/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1560 - mse: 2.2586 - val_loss: 0.1807 - val_mse: 1.0854\n",
      "Epoch 124/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1560 - mse: 2.2586 - val_loss: 0.1807 - val_mse: 1.0854\n",
      "Epoch 125/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1560 - mse: 2.2586 - val_loss: 0.1807 - val_mse: 1.0854\n",
      "Epoch 126/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1560 - mse: 2.2586 - val_loss: 0.1807 - val_mse: 1.0854\n",
      "Epoch 127/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1560 - mse: 2.2586 - val_loss: 0.1807 - val_mse: 1.0854\n",
      "Epoch 128/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1560 - mse: 2.2585 - val_loss: 0.1807 - val_mse: 1.0853\n",
      "Epoch 129/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1560 - mse: 2.2585 - val_loss: 0.1807 - val_mse: 1.0853\n",
      "Epoch 130/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1560 - mse: 2.2585 - val_loss: 0.1807 - val_mse: 1.0853\n",
      "Epoch 131/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1560 - mse: 2.2585 - val_loss: 0.1807 - val_mse: 1.0853\n",
      "Epoch 132/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1560 - mse: 2.2585 - val_loss: 0.1807 - val_mse: 1.0853\n",
      "Epoch 133/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1560 - mse: 2.2585 - val_loss: 0.1807 - val_mse: 1.0852\n",
      "Epoch 134/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1560 - mse: 2.2584 - val_loss: 0.1807 - val_mse: 1.0852\n",
      "Epoch 135/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1560 - mse: 2.2584 - val_loss: 0.1807 - val_mse: 1.0852\n",
      "Epoch 136/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1560 - mse: 2.2584 - val_loss: 0.1807 - val_mse: 1.0852\n",
      "Epoch 137/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1560 - mse: 2.2584 - val_loss: 0.1807 - val_mse: 1.0852\n",
      "Epoch 138/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1560 - mse: 2.2584 - val_loss: 0.1807 - val_mse: 1.0852\n",
      "Epoch 139/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1560 - mse: 2.2584 - val_loss: 0.1807 - val_mse: 1.0851\n",
      "Epoch 140/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1560 - mse: 2.2584 - val_loss: 0.1807 - val_mse: 1.0851\n",
      "Epoch 141/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1560 - mse: 2.2583 - val_loss: 0.1807 - val_mse: 1.0851\n",
      "Epoch 142/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1560 - mse: 2.2583 - val_loss: 0.1807 - val_mse: 1.0851\n",
      "Epoch 143/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1560 - mse: 2.2583 - val_loss: 0.1807 - val_mse: 1.0850\n",
      "Epoch 144/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1560 - mse: 2.2583 - val_loss: 0.1807 - val_mse: 1.0850\n",
      "Epoch 145/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1560 - mse: 2.2583 - val_loss: 0.1807 - val_mse: 1.0850\n",
      "Epoch 146/1000\n",
      "1294/1294 [==============================] - 9s 7ms/step - loss: 0.1560 - mse: 2.2583 - val_loss: 0.1807 - val_mse: 1.0850\n",
      "Epoch 147/1000\n",
      "1294/1294 [==============================] - 9s 7ms/step - loss: 0.1560 - mse: 2.2582 - val_loss: 0.1807 - val_mse: 1.0850\n",
      "Epoch 148/1000\n",
      "1294/1294 [==============================] - 9s 7ms/step - loss: 0.1560 - mse: 2.2582 - val_loss: 0.1807 - val_mse: 1.0849\n",
      "Epoch 149/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1560 - mse: 2.2582 - val_loss: 0.1807 - val_mse: 1.0849\n",
      "Epoch 150/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1560 - mse: 2.2582 - val_loss: 0.1807 - val_mse: 1.0849\n",
      "Epoch 151/1000\n",
      "1294/1294 [==============================] - 9s 7ms/step - loss: 0.1560 - mse: 2.2582 - val_loss: 0.1807 - val_mse: 1.0849\n",
      "Epoch 152/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1560 - mse: 2.2582 - val_loss: 0.1807 - val_mse: 1.0848\n",
      "Epoch 153/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1560 - mse: 2.2581 - val_loss: 0.1807 - val_mse: 1.0848\n",
      "Epoch 154/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1560 - mse: 2.2581 - val_loss: 0.1807 - val_mse: 1.0848\n",
      "Epoch 155/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1560 - mse: 2.2581 - val_loss: 0.1807 - val_mse: 1.0848\n",
      "Epoch 156/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1560 - mse: 2.2581 - val_loss: 0.1807 - val_mse: 1.0848\n",
      "Epoch 157/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1559 - mse: 2.2581 - val_loss: 0.1807 - val_mse: 1.0847\n",
      "Epoch 158/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1559 - mse: 2.2581 - val_loss: 0.1807 - val_mse: 1.0847\n",
      "Epoch 159/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1559 - mse: 2.2580 - val_loss: 0.1807 - val_mse: 1.0847\n",
      "Epoch 160/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1559 - mse: 2.2580 - val_loss: 0.1807 - val_mse: 1.0847\n",
      "Epoch 161/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1559 - mse: 2.2580 - val_loss: 0.1807 - val_mse: 1.0846\n",
      "Epoch 162/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1559 - mse: 2.2580 - val_loss: 0.1807 - val_mse: 1.0846\n",
      "Epoch 163/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1559 - mse: 2.2580 - val_loss: 0.1807 - val_mse: 1.0846\n",
      "Epoch 164/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1559 - mse: 2.2580 - val_loss: 0.1807 - val_mse: 1.0846\n",
      "Epoch 165/1000\n",
      "1294/1294 [==============================] - 9s 7ms/step - loss: 0.1559 - mse: 2.2580 - val_loss: 0.1807 - val_mse: 1.0845\n",
      "Epoch 166/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1559 - mse: 2.2579 - val_loss: 0.1807 - val_mse: 1.0845\n",
      "Epoch 167/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1559 - mse: 2.2579 - val_loss: 0.1807 - val_mse: 1.0845\n",
      "Epoch 168/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1559 - mse: 2.2579 - val_loss: 0.1807 - val_mse: 1.0845\n",
      "Epoch 169/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1559 - mse: 2.2579 - val_loss: 0.1807 - val_mse: 1.0845\n",
      "Epoch 170/1000\n",
      "1294/1294 [==============================] - 9s 7ms/step - loss: 0.1559 - mse: 2.2579 - val_loss: 0.1807 - val_mse: 1.0844\n",
      "Epoch 171/1000\n",
      "1294/1294 [==============================] - 9s 7ms/step - loss: 0.1559 - mse: 2.2579 - val_loss: 0.1807 - val_mse: 1.0844\n",
      "Epoch 172/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1559 - mse: 2.2579 - val_loss: 0.1807 - val_mse: 1.0844\n",
      "Epoch 173/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1559 - mse: 2.2578 - val_loss: 0.1806 - val_mse: 1.0844\n",
      "Epoch 174/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1559 - mse: 2.2578 - val_loss: 0.1806 - val_mse: 1.0843\n",
      "Epoch 175/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1559 - mse: 2.2578 - val_loss: 0.1806 - val_mse: 1.0843\n",
      "Epoch 176/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1559 - mse: 2.2578 - val_loss: 0.1806 - val_mse: 1.0843\n",
      "Epoch 177/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1559 - mse: 2.2578 - val_loss: 0.1806 - val_mse: 1.0843\n",
      "Epoch 178/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1559 - mse: 2.2578 - val_loss: 0.1807 - val_mse: 1.0842\n",
      "Epoch 179/1000\n",
      "1294/1294 [==============================] - 9s 7ms/step - loss: 0.1559 - mse: 2.2578 - val_loss: 0.1806 - val_mse: 1.0842\n",
      "Epoch 180/1000\n",
      "1294/1294 [==============================] - 9s 7ms/step - loss: 0.1559 - mse: 2.2577 - val_loss: 0.1806 - val_mse: 1.0842\n",
      "Epoch 181/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1559 - mse: 2.2577 - val_loss: 0.1806 - val_mse: 1.0842\n",
      "Epoch 182/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1559 - mse: 2.2577 - val_loss: 0.1806 - val_mse: 1.0841\n",
      "Epoch 183/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1559 - mse: 2.2577 - val_loss: 0.1806 - val_mse: 1.0841\n",
      "Epoch 184/1000\n",
      "1294/1294 [==============================] - 9s 7ms/step - loss: 0.1559 - mse: 2.2577 - val_loss: 0.1806 - val_mse: 1.0841\n",
      "Epoch 185/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1559 - mse: 2.2577 - val_loss: 0.1806 - val_mse: 1.0841\n",
      "Epoch 186/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1559 - mse: 2.2576 - val_loss: 0.1806 - val_mse: 1.0841\n",
      "Epoch 187/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1559 - mse: 2.2576 - val_loss: 0.1806 - val_mse: 1.0840\n",
      "Epoch 188/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1559 - mse: 2.2576 - val_loss: 0.1806 - val_mse: 1.0840\n",
      "Epoch 189/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1559 - mse: 2.2576 - val_loss: 0.1806 - val_mse: 1.0840\n",
      "Epoch 190/1000\n",
      "1294/1294 [==============================] - 9s 7ms/step - loss: 0.1559 - mse: 2.2576 - val_loss: 0.1806 - val_mse: 1.0839\n",
      "Epoch 191/1000\n",
      "1294/1294 [==============================] - 9s 7ms/step - loss: 0.1559 - mse: 2.2576 - val_loss: 0.1806 - val_mse: 1.0839\n",
      "Epoch 192/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1559 - mse: 2.2576 - val_loss: 0.1806 - val_mse: 1.0839\n",
      "Epoch 193/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1559 - mse: 2.2575 - val_loss: 0.1806 - val_mse: 1.0839\n",
      "Epoch 194/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1559 - mse: 2.2575 - val_loss: 0.1806 - val_mse: 1.0839\n",
      "Epoch 195/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1559 - mse: 2.2575 - val_loss: 0.1806 - val_mse: 1.0838\n",
      "Epoch 196/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1559 - mse: 2.2575 - val_loss: 0.1806 - val_mse: 1.0838\n",
      "Epoch 197/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1559 - mse: 2.2575 - val_loss: 0.1806 - val_mse: 1.0838\n",
      "Epoch 198/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1559 - mse: 2.2575 - val_loss: 0.1806 - val_mse: 1.0838\n",
      "Epoch 199/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1559 - mse: 2.2574 - val_loss: 0.1806 - val_mse: 1.0837\n",
      "Epoch 200/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1559 - mse: 2.2574 - val_loss: 0.1806 - val_mse: 1.0837\n",
      "Epoch 201/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1559 - mse: 2.2574 - val_loss: 0.1806 - val_mse: 1.0837\n",
      "Epoch 202/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1559 - mse: 2.2574 - val_loss: 0.1806 - val_mse: 1.0837\n",
      "Epoch 203/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1559 - mse: 2.2574 - val_loss: 0.1806 - val_mse: 1.0836\n",
      "Epoch 204/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1559 - mse: 2.2574 - val_loss: 0.1806 - val_mse: 1.0836\n",
      "Epoch 205/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1559 - mse: 2.2574 - val_loss: 0.1806 - val_mse: 1.0836\n",
      "Epoch 206/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1559 - mse: 2.2573 - val_loss: 0.1806 - val_mse: 1.0836\n",
      "Epoch 207/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1559 - mse: 2.2573 - val_loss: 0.1806 - val_mse: 1.0836\n",
      "Epoch 208/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1559 - mse: 2.2573 - val_loss: 0.1806 - val_mse: 1.0835\n",
      "Epoch 209/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1559 - mse: 2.2573 - val_loss: 0.1806 - val_mse: 1.0835\n",
      "Epoch 210/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1559 - mse: 2.2573 - val_loss: 0.1806 - val_mse: 1.0835\n",
      "Epoch 211/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1559 - mse: 2.2572 - val_loss: 0.1806 - val_mse: 1.0835\n",
      "Epoch 212/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1559 - mse: 2.2572 - val_loss: 0.1806 - val_mse: 1.0834\n",
      "Epoch 213/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1559 - mse: 2.2572 - val_loss: 0.1806 - val_mse: 1.0834\n",
      "Epoch 214/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1559 - mse: 2.2572 - val_loss: 0.1806 - val_mse: 1.0834\n",
      "Epoch 215/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1559 - mse: 2.2572 - val_loss: 0.1806 - val_mse: 1.0834\n",
      "Epoch 216/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1559 - mse: 2.2572 - val_loss: 0.1806 - val_mse: 1.0833\n",
      "Epoch 217/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1559 - mse: 2.2572 - val_loss: 0.1806 - val_mse: 1.0833\n",
      "Epoch 218/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1559 - mse: 2.2571 - val_loss: 0.1806 - val_mse: 1.0833\n",
      "Epoch 219/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1559 - mse: 2.2571 - val_loss: 0.1806 - val_mse: 1.0833\n",
      "Epoch 220/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1559 - mse: 2.2571 - val_loss: 0.1806 - val_mse: 1.0832\n",
      "Epoch 221/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1559 - mse: 2.2571 - val_loss: 0.1806 - val_mse: 1.0832\n",
      "Epoch 222/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1559 - mse: 2.2571 - val_loss: 0.1806 - val_mse: 1.0832\n",
      "Epoch 223/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1559 - mse: 2.2571 - val_loss: 0.1806 - val_mse: 1.0832\n",
      "Epoch 224/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1559 - mse: 2.2571 - val_loss: 0.1806 - val_mse: 1.0831\n",
      "Epoch 225/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1559 - mse: 2.2570 - val_loss: 0.1806 - val_mse: 1.0831\n",
      "Epoch 226/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1559 - mse: 2.2570 - val_loss: 0.1806 - val_mse: 1.0831\n",
      "Epoch 227/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1559 - mse: 2.2570 - val_loss: 0.1806 - val_mse: 1.0831\n",
      "Epoch 228/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1559 - mse: 2.2570 - val_loss: 0.1806 - val_mse: 1.0830\n",
      "Epoch 229/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1559 - mse: 2.2570 - val_loss: 0.1806 - val_mse: 1.0830\n",
      "Epoch 230/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1559 - mse: 2.2570 - val_loss: 0.1806 - val_mse: 1.0830\n",
      "Epoch 231/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1559 - mse: 2.2570 - val_loss: 0.1806 - val_mse: 1.0830\n",
      "Epoch 232/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1559 - mse: 2.2569 - val_loss: 0.1806 - val_mse: 1.0830\n",
      "Epoch 233/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1559 - mse: 2.2569 - val_loss: 0.1806 - val_mse: 1.0829\n",
      "Epoch 234/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1559 - mse: 2.2569 - val_loss: 0.1806 - val_mse: 1.0829\n",
      "Epoch 235/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1559 - mse: 2.2569 - val_loss: 0.1806 - val_mse: 1.0829\n",
      "Epoch 236/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1559 - mse: 2.2569 - val_loss: 0.1806 - val_mse: 1.0829\n",
      "Epoch 237/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1559 - mse: 2.2569 - val_loss: 0.1806 - val_mse: 1.0829\n",
      "Epoch 238/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1559 - mse: 2.2568 - val_loss: 0.1806 - val_mse: 1.0828\n",
      "Epoch 239/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1559 - mse: 2.2568 - val_loss: 0.1806 - val_mse: 1.0828\n",
      "Epoch 240/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1558 - mse: 2.2568 - val_loss: 0.1806 - val_mse: 1.0828\n",
      "Epoch 241/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1559 - mse: 2.2568 - val_loss: 0.1806 - val_mse: 1.0827\n",
      "Epoch 242/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1558 - mse: 2.2568 - val_loss: 0.1806 - val_mse: 1.0827\n",
      "Epoch 243/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1559 - mse: 2.2568 - val_loss: 0.1806 - val_mse: 1.0827\n",
      "Epoch 244/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1558 - mse: 2.2567 - val_loss: 0.1806 - val_mse: 1.0827\n",
      "Epoch 245/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1558 - mse: 2.2567 - val_loss: 0.1806 - val_mse: 1.0827\n",
      "Epoch 246/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1558 - mse: 2.2567 - val_loss: 0.1806 - val_mse: 1.0826\n",
      "Epoch 247/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1558 - mse: 2.2567 - val_loss: 0.1806 - val_mse: 1.0826\n",
      "Epoch 248/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1558 - mse: 2.2567 - val_loss: 0.1806 - val_mse: 1.0826\n",
      "Epoch 249/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1558 - mse: 2.2567 - val_loss: 0.1806 - val_mse: 1.0826\n",
      "Epoch 250/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1558 - mse: 2.2567 - val_loss: 0.1806 - val_mse: 1.0825\n",
      "Epoch 251/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1558 - mse: 2.2566 - val_loss: 0.1806 - val_mse: 1.0825\n",
      "Epoch 252/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1558 - mse: 2.2566 - val_loss: 0.1806 - val_mse: 1.0825\n",
      "Epoch 253/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1558 - mse: 2.2566 - val_loss: 0.1806 - val_mse: 1.0825\n",
      "Epoch 254/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1558 - mse: 2.2566 - val_loss: 0.1806 - val_mse: 1.0824\n",
      "Epoch 255/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1558 - mse: 2.2566 - val_loss: 0.1806 - val_mse: 1.0824\n",
      "Epoch 256/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1558 - mse: 2.2566 - val_loss: 0.1806 - val_mse: 1.0824\n",
      "Epoch 257/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1558 - mse: 2.2565 - val_loss: 0.1806 - val_mse: 1.0824\n",
      "Epoch 258/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1558 - mse: 2.2565 - val_loss: 0.1806 - val_mse: 1.0823\n",
      "Epoch 259/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1558 - mse: 2.2565 - val_loss: 0.1806 - val_mse: 1.0823\n",
      "Epoch 260/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1558 - mse: 2.2565 - val_loss: 0.1806 - val_mse: 1.0823\n",
      "Epoch 261/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1558 - mse: 2.2565 - val_loss: 0.1806 - val_mse: 1.0823\n",
      "Epoch 262/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1558 - mse: 2.2565 - val_loss: 0.1806 - val_mse: 1.0822\n",
      "Epoch 263/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1558 - mse: 2.2565 - val_loss: 0.1806 - val_mse: 1.0822\n",
      "Epoch 264/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1558 - mse: 2.2564 - val_loss: 0.1806 - val_mse: 1.0822\n",
      "Epoch 265/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1558 - mse: 2.2564 - val_loss: 0.1806 - val_mse: 1.0822\n",
      "Epoch 266/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1558 - mse: 2.2564 - val_loss: 0.1806 - val_mse: 1.0821\n",
      "Epoch 267/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1558 - mse: 2.2564 - val_loss: 0.1806 - val_mse: 1.0821\n",
      "Epoch 268/1000\n",
      "1294/1294 [==============================] - 11s 8ms/step - loss: 0.1558 - mse: 2.2564 - val_loss: 0.1806 - val_mse: 1.0821\n",
      "Epoch 269/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1558 - mse: 2.2564 - val_loss: 0.1806 - val_mse: 1.0821\n",
      "Epoch 270/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1558 - mse: 2.2563 - val_loss: 0.1806 - val_mse: 1.0821\n",
      "Epoch 271/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1558 - mse: 2.2563 - val_loss: 0.1806 - val_mse: 1.0820\n",
      "Epoch 272/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1558 - mse: 2.2563 - val_loss: 0.1806 - val_mse: 1.0820\n",
      "Epoch 273/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1558 - mse: 2.2563 - val_loss: 0.1806 - val_mse: 1.0820\n",
      "Epoch 274/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1558 - mse: 2.2563 - val_loss: 0.1806 - val_mse: 1.0820\n",
      "Epoch 275/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1558 - mse: 2.2563 - val_loss: 0.1806 - val_mse: 1.0820\n",
      "Epoch 276/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1558 - mse: 2.2562 - val_loss: 0.1806 - val_mse: 1.0819\n",
      "Epoch 277/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1558 - mse: 2.2562 - val_loss: 0.1806 - val_mse: 1.0819\n",
      "Epoch 278/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1558 - mse: 2.2562 - val_loss: 0.1806 - val_mse: 1.0819\n",
      "Epoch 279/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1558 - mse: 2.2562 - val_loss: 0.1805 - val_mse: 1.0818\n",
      "Epoch 280/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1558 - mse: 2.2562 - val_loss: 0.1805 - val_mse: 1.0818\n",
      "Epoch 281/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1558 - mse: 2.2562 - val_loss: 0.1805 - val_mse: 1.0818\n",
      "Epoch 282/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1558 - mse: 2.2561 - val_loss: 0.1805 - val_mse: 1.0818\n",
      "Epoch 283/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1558 - mse: 2.2561 - val_loss: 0.1805 - val_mse: 1.0818\n",
      "Epoch 284/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1558 - mse: 2.2561 - val_loss: 0.1805 - val_mse: 1.0817\n",
      "Epoch 285/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1558 - mse: 2.2561 - val_loss: 0.1805 - val_mse: 1.0817\n",
      "Epoch 286/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1558 - mse: 2.2561 - val_loss: 0.1805 - val_mse: 1.0817\n",
      "Epoch 287/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1558 - mse: 2.2561 - val_loss: 0.1805 - val_mse: 1.0817\n",
      "Epoch 288/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1558 - mse: 2.2561 - val_loss: 0.1805 - val_mse: 1.0816\n",
      "Epoch 289/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1558 - mse: 2.2560 - val_loss: 0.1805 - val_mse: 1.0816\n",
      "Epoch 290/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1558 - mse: 2.2560 - val_loss: 0.1805 - val_mse: 1.0816\n",
      "Epoch 291/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1558 - mse: 2.2560 - val_loss: 0.1805 - val_mse: 1.0816\n",
      "Epoch 292/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1558 - mse: 2.2560 - val_loss: 0.1805 - val_mse: 1.0815\n",
      "Epoch 293/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1558 - mse: 2.2560 - val_loss: 0.1805 - val_mse: 1.0815\n",
      "Epoch 294/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1558 - mse: 2.2560 - val_loss: 0.1805 - val_mse: 1.0815\n",
      "Epoch 295/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1558 - mse: 2.2559 - val_loss: 0.1805 - val_mse: 1.0815\n",
      "Epoch 296/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1558 - mse: 2.2559 - val_loss: 0.1805 - val_mse: 1.0814\n",
      "Epoch 297/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1558 - mse: 2.2559 - val_loss: 0.1805 - val_mse: 1.0814\n",
      "Epoch 298/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1558 - mse: 2.2559 - val_loss: 0.1805 - val_mse: 1.0814\n",
      "Epoch 299/1000\n",
      "1294/1294 [==============================] - 11s 8ms/step - loss: 0.1558 - mse: 2.2559 - val_loss: 0.1805 - val_mse: 1.0814\n",
      "Epoch 300/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1558 - mse: 2.2559 - val_loss: 0.1805 - val_mse: 1.0813\n",
      "Epoch 301/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1558 - mse: 2.2559 - val_loss: 0.1805 - val_mse: 1.0813\n",
      "Epoch 302/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1558 - mse: 2.2558 - val_loss: 0.1805 - val_mse: 1.0813\n",
      "Epoch 303/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1558 - mse: 2.2558 - val_loss: 0.1805 - val_mse: 1.0813\n",
      "Epoch 304/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1558 - mse: 2.2558 - val_loss: 0.1805 - val_mse: 1.0812\n",
      "Epoch 305/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1558 - mse: 2.2558 - val_loss: 0.1805 - val_mse: 1.0812\n",
      "Epoch 306/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1558 - mse: 2.2558 - val_loss: 0.1805 - val_mse: 1.0812\n",
      "Epoch 307/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1558 - mse: 2.2557 - val_loss: 0.1805 - val_mse: 1.0812\n",
      "Epoch 308/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1558 - mse: 2.2557 - val_loss: 0.1805 - val_mse: 1.0812\n",
      "Epoch 309/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1558 - mse: 2.2557 - val_loss: 0.1805 - val_mse: 1.0811\n",
      "Epoch 310/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1558 - mse: 2.2557 - val_loss: 0.1805 - val_mse: 1.0811\n",
      "Epoch 311/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1558 - mse: 2.2557 - val_loss: 0.1805 - val_mse: 1.0811\n",
      "Epoch 312/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1558 - mse: 2.2557 - val_loss: 0.1805 - val_mse: 1.0811\n",
      "Epoch 313/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1558 - mse: 2.2557 - val_loss: 0.1805 - val_mse: 1.0810\n",
      "Epoch 314/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1558 - mse: 2.2556 - val_loss: 0.1805 - val_mse: 1.0810\n",
      "Epoch 315/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1558 - mse: 2.2556 - val_loss: 0.1805 - val_mse: 1.0810\n",
      "Epoch 316/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1558 - mse: 2.2556 - val_loss: 0.1805 - val_mse: 1.0810\n",
      "Epoch 317/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1558 - mse: 2.2556 - val_loss: 0.1805 - val_mse: 1.0809\n",
      "Epoch 318/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1558 - mse: 2.2556 - val_loss: 0.1805 - val_mse: 1.0809\n",
      "Epoch 319/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1558 - mse: 2.2556 - val_loss: 0.1805 - val_mse: 1.0809\n",
      "Epoch 320/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1558 - mse: 2.2555 - val_loss: 0.1805 - val_mse: 1.0809\n",
      "Epoch 321/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1558 - mse: 2.2555 - val_loss: 0.1805 - val_mse: 1.0808\n",
      "Epoch 322/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1558 - mse: 2.2555 - val_loss: 0.1805 - val_mse: 1.0808\n",
      "Epoch 323/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1558 - mse: 2.2555 - val_loss: 0.1805 - val_mse: 1.0808\n",
      "Epoch 324/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1558 - mse: 2.2555 - val_loss: 0.1805 - val_mse: 1.0808\n",
      "Epoch 325/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1558 - mse: 2.2555 - val_loss: 0.1805 - val_mse: 1.0807\n",
      "Epoch 326/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1558 - mse: 2.2555 - val_loss: 0.1805 - val_mse: 1.0807\n",
      "Epoch 327/1000\n",
      "1294/1294 [==============================] - 10s 7ms/step - loss: 0.1558 - mse: 2.2555 - val_loss: 0.1805 - val_mse: 1.0807\n",
      "Epoch 328/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1558 - mse: 2.2554 - val_loss: 0.1805 - val_mse: 1.0807\n",
      "Epoch 329/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1558 - mse: 2.2554 - val_loss: 0.1805 - val_mse: 1.0806\n",
      "Epoch 330/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1558 - mse: 2.2554 - val_loss: 0.1805 - val_mse: 1.0806\n",
      "Epoch 331/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1558 - mse: 2.2554 - val_loss: 0.1805 - val_mse: 1.0806\n",
      "Epoch 332/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1558 - mse: 2.2554 - val_loss: 0.1805 - val_mse: 1.0806\n",
      "Epoch 333/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1558 - mse: 2.2553 - val_loss: 0.1805 - val_mse: 1.0806\n",
      "Epoch 334/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1558 - mse: 2.2553 - val_loss: 0.1805 - val_mse: 1.0805\n",
      "Epoch 335/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1558 - mse: 2.2553 - val_loss: 0.1805 - val_mse: 1.0805\n",
      "Epoch 336/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1557 - mse: 2.2553 - val_loss: 0.1805 - val_mse: 1.0805\n",
      "Epoch 337/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1557 - mse: 2.2553 - val_loss: 0.1805 - val_mse: 1.0805\n",
      "Epoch 338/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1557 - mse: 2.2553 - val_loss: 0.1805 - val_mse: 1.0804\n",
      "Epoch 339/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1557 - mse: 2.2553 - val_loss: 0.1805 - val_mse: 1.0804\n",
      "Epoch 340/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1557 - mse: 2.2552 - val_loss: 0.1805 - val_mse: 1.0804\n",
      "Epoch 341/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1557 - mse: 2.2552 - val_loss: 0.1805 - val_mse: 1.0803\n",
      "Epoch 342/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1557 - mse: 2.2552 - val_loss: 0.1805 - val_mse: 1.0803\n",
      "Epoch 343/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1557 - mse: 2.2552 - val_loss: 0.1805 - val_mse: 1.0803\n",
      "Epoch 344/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1557 - mse: 2.2552 - val_loss: 0.1805 - val_mse: 1.0803\n",
      "Epoch 345/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1557 - mse: 2.2552 - val_loss: 0.1805 - val_mse: 1.0802\n",
      "Epoch 346/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1557 - mse: 2.2551 - val_loss: 0.1805 - val_mse: 1.0802\n",
      "Epoch 347/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1557 - mse: 2.2551 - val_loss: 0.1805 - val_mse: 1.0802\n",
      "Epoch 348/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1557 - mse: 2.2551 - val_loss: 0.1805 - val_mse: 1.0802\n",
      "Epoch 349/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1557 - mse: 2.2551 - val_loss: 0.1805 - val_mse: 1.0801\n",
      "Epoch 350/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1557 - mse: 2.2551 - val_loss: 0.1805 - val_mse: 1.0801\n",
      "Epoch 351/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1557 - mse: 2.2551 - val_loss: 0.1805 - val_mse: 1.0801\n",
      "Epoch 352/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1557 - mse: 2.2550 - val_loss: 0.1805 - val_mse: 1.0801\n",
      "Epoch 353/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1557 - mse: 2.2550 - val_loss: 0.1805 - val_mse: 1.0800\n",
      "Epoch 354/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1557 - mse: 2.2550 - val_loss: 0.1805 - val_mse: 1.0800\n",
      "Epoch 355/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1557 - mse: 2.2550 - val_loss: 0.1805 - val_mse: 1.0800\n",
      "Epoch 356/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1557 - mse: 2.2550 - val_loss: 0.1805 - val_mse: 1.0800\n",
      "Epoch 357/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1557 - mse: 2.2550 - val_loss: 0.1805 - val_mse: 1.0799\n",
      "Epoch 358/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1557 - mse: 2.2549 - val_loss: 0.1805 - val_mse: 1.0799\n",
      "Epoch 359/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1557 - mse: 2.2549 - val_loss: 0.1805 - val_mse: 1.0799\n",
      "Epoch 360/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1557 - mse: 2.2549 - val_loss: 0.1805 - val_mse: 1.0799\n",
      "Epoch 361/1000\n",
      "1294/1294 [==============================] - 11s 8ms/step - loss: 0.1557 - mse: 2.2549 - val_loss: 0.1805 - val_mse: 1.0798\n",
      "Epoch 362/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1557 - mse: 2.2549 - val_loss: 0.1805 - val_mse: 1.0798\n",
      "Epoch 363/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1557 - mse: 2.2549 - val_loss: 0.1805 - val_mse: 1.0798\n",
      "Epoch 364/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1557 - mse: 2.2549 - val_loss: 0.1805 - val_mse: 1.0798\n",
      "Epoch 365/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1557 - mse: 2.2548 - val_loss: 0.1805 - val_mse: 1.0797\n",
      "Epoch 366/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1557 - mse: 2.2548 - val_loss: 0.1805 - val_mse: 1.0797\n",
      "Epoch 367/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1557 - mse: 2.2548 - val_loss: 0.1805 - val_mse: 1.0797\n",
      "Epoch 368/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1557 - mse: 2.2548 - val_loss: 0.1805 - val_mse: 1.0797\n",
      "Epoch 369/1000\n",
      "1294/1294 [==============================] - 10s 8ms/step - loss: 0.1557 - mse: 2.2548 - val_loss: 0.1805 - val_mse: 1.0796\n",
      "Epoch 370/1000\n",
      " 514/1294 [==========>...................] - ETA: 5s - loss: 0.1075 - mse: 0.4247"
     ],
     "name": "stdout"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-72-9dbb82c4367b>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      5\u001B[0m   \u001B[0;32mreturn\u001B[0m \u001B[0;36m1e-4\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[0mnew_lr\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mkeras\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcallbacks\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mLearningRateScheduler\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mgetlr\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 7\u001B[0;31m \u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_ds\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mepochs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1000\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mvalidation_data\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mval_ds\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mcallbacks\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mmodel_checkpoint\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mearly_stopping\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001B[0m in \u001B[0;36m_method_wrapper\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    106\u001B[0m   \u001B[0;32mdef\u001B[0m \u001B[0m_method_wrapper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    107\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_in_multi_worker_mode\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m  \u001B[0;31m# pylint: disable=protected-access\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 108\u001B[0;31m       \u001B[0;32mreturn\u001B[0m \u001B[0mmethod\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    109\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    110\u001B[0m     \u001B[0;31m# Running inside `run_distribute_coordinator` already.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[1;32m   1096\u001B[0m                 batch_size=batch_size):\n\u001B[1;32m   1097\u001B[0m               \u001B[0mcallbacks\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mon_train_batch_begin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1098\u001B[0;31m               \u001B[0mtmp_logs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtrain_function\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0miterator\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1099\u001B[0m               \u001B[0;32mif\u001B[0m \u001B[0mdata_handler\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshould_sync\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1100\u001B[0m                 \u001B[0mcontext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0masync_wait\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    778\u001B[0m       \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    779\u001B[0m         \u001B[0mcompiler\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m\"nonXla\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 780\u001B[0;31m         \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    781\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    782\u001B[0m       \u001B[0mnew_tracing_count\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_get_tracing_count\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001B[0m in \u001B[0;36m_call\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    805\u001B[0m       \u001B[0;31m# In this case we have created variables on the first call, so we run the\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    806\u001B[0m       \u001B[0;31m# defunned version which is guaranteed to never create variables.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 807\u001B[0;31m       \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_stateless_fn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# pylint: disable=not-callable\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    808\u001B[0m     \u001B[0;32melif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_stateful_fn\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    809\u001B[0m       \u001B[0;31m# Release the lock early so that multiple threads can perform the call\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   2827\u001B[0m     \u001B[0;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_lock\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2828\u001B[0m       \u001B[0mgraph_function\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwargs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_maybe_define_function\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2829\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0mgraph_function\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_filtered_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# pylint: disable=protected-access\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2830\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2831\u001B[0m   \u001B[0;34m@\u001B[0m\u001B[0mproperty\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001B[0m in \u001B[0;36m_filtered_call\u001B[0;34m(self, args, kwargs, cancellation_manager)\u001B[0m\n\u001B[1;32m   1846\u001B[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001B[1;32m   1847\u001B[0m         \u001B[0mcaptured_inputs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcaptured_inputs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1848\u001B[0;31m         cancellation_manager=cancellation_manager)\n\u001B[0m\u001B[1;32m   1849\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1850\u001B[0m   \u001B[0;32mdef\u001B[0m \u001B[0m_call_flat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcaptured_inputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcancellation_manager\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001B[0m in \u001B[0;36m_call_flat\u001B[0;34m(self, args, captured_inputs, cancellation_manager)\u001B[0m\n\u001B[1;32m   1922\u001B[0m       \u001B[0;31m# No tape is watching; skip to running the function.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1923\u001B[0m       return self._build_call_outputs(self._inference_function.call(\n\u001B[0;32m-> 1924\u001B[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001B[0m\u001B[1;32m   1925\u001B[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001B[1;32m   1926\u001B[0m         \u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001B[0m in \u001B[0;36mcall\u001B[0;34m(self, ctx, args, cancellation_manager)\u001B[0m\n\u001B[1;32m    548\u001B[0m               \u001B[0minputs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    549\u001B[0m               \u001B[0mattrs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mattrs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 550\u001B[0;31m               ctx=ctx)\n\u001B[0m\u001B[1;32m    551\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    552\u001B[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001B[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001B[0m in \u001B[0;36mquick_execute\u001B[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[1;32m     58\u001B[0m     \u001B[0mctx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mensure_initialized\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     59\u001B[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001B[0;32m---> 60\u001B[0;31m                                         inputs, attrs, num_outputs)\n\u001B[0m\u001B[1;32m     61\u001B[0m   \u001B[0;32mexcept\u001B[0m \u001B[0mcore\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_NotOkStatusException\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     62\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mname\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "sY9EgNpOSp8s"
   },
   "source": [
    "c = 'Washington'\n",
    "s = 'WI'\n",
    "r = df[(df['state']==s)&(df['county']==c)]"
   ],
   "execution_count": 32,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "lCSQPQbsSp87"
   },
   "source": [
    "model4 = tf.keras.models.load_model('checkpoint0.h5')\n",
    "row = r.drop(columns=['state','county'])\n",
    "row = row.to_numpy()\n",
    "dataset = getDataset(row,8,False)"
   ],
   "execution_count": 33,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "IKZQWaW-Sp9N",
    "outputId": "c5381422-db3a-4a11-b618-75cbec7c9c67",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    }
   },
   "source": [
    "results = model4.predict(dataset)\n",
    "results"
   ],
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 1.1554176],\n       [ 2.6282215],\n       [15.761047 ]], dtype=float32)"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "J-2cK6uCSp9k",
    "outputId": "f59a1c58-641f-434d-d83c-6813eddec4e1",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    }
   },
   "source": [
    "dem = np.array(r.values.tolist()[0][2::3])\n",
    "gop = np.array(r.values.tolist()[0][3::3])\n",
    "other = np.array(r.values.tolist()[0][4::3])\n",
    "plotCounty(s,c,dem,gop,other,results)"
   ],
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZcAAAEWCAYAAACqitpwAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAA5rUlEQVR4nO3deXxU5d338c8veyAhhFWQXRNZFFmCgIhAuUUUfdy3goBacS3F2ler9e6tbbV6t9YiraUPj7VqwbUutYoLIrgjiyCyyb4EkB2SQBKSye/54zqTmSSTDWYyWX7v1+u8ZuY6Z85cZwLzneu6zrlGVBVjjDEmnGKiXQFjjDGNj4WLMcaYsLNwMcYYE3YWLsYYY8LOwsUYY0zYWbgYY4wJOwsXY+qAiDwrIg+HcX8jRSQ7XPszJtwsXEyTJSL3i8i75co2VFJ2fd3WLjpEpIOIqIi0Dyp7oJKy97z7YQ1O0zhYuJim7BPgXBGJBffBCsQD/cuVne5t2+ip6m5gI3B+UPH5wLoQZU3iPTEnxsLFNGVLcGHSz3s8HFgAfFeubJOq7hKRm0RkrYjkishmEbnNvyMRmSwinwXv3Pu2f3r5FxWRVBFZICIzxOkpIvNE5KCIfCci1wZte7GIrPFec6eI/Kzcvu4Vkb0isltEbgoqHyciy0UkR0R2iMhDtXhfPsELEi9kBwBPlisbioWLqYKFi2myVPU48BWBb+TnA58Cn5Ur83+I7gUuAVoANwF/EpEBtXlNEWkNzAc+V9WpQDNgHvAC0A64HviriPT2nvJ34DZVTQXOBD4K2t0pQBpwKnAL8JSIpHvrjgITgZbAOOAOEbm8htUsDRegP7DWq3NwWTywuIb7M02QhYtp6j4m8KE5HBcun5Yr+xhAVd9R1U3qfAx84K2vqY7evl5V1f/2yi4BtqrqP1S1WFWXA68B13jri4DeItJCVQ+p6tdB+ysCfqOqRao6F8gDzvDqulBVv1XVElVdCbwIjKhhPT8GzhSRlv73RFU3AG2DyhZ54WxMSBYupqn7BDhPRFoBbb0P0S9wYzGtcK2FTwBE5CIRWeR1Xx0GLgba1OK1xgHJwN+CyroCg0XksH8BxuNaJQBXea+zTUQ+FpGhQc89oKrFQY+PASleXQd7XW/7ROQIcHtN66qqW4GduBDxt+bAvS/+MusSM1WycDFN3Ze4rqVbgc8BVDUH2OWV7VLVLSKSiGtRPA60V9WWwFxAvP0cxXVxASAi/nAI9v+A94C5ItLcK9sBfKyqLYOWFFW9w6vLElW9DNdl9ibwSg2P6wXgLaCzqqbhAk2qfkoZ/q6xobhQgUCL7jwsXEw1LFxMk6aq+cBS4KcEvqGDG3f5KYEP0QQgEdgHFIvIRcCYoO2/AfqISD8RSQIequQl78adMPAfEUkG3gYyReRGEYn3lkEi0ktEEkRkvIikqWoRkAOU1PDQUoGDqlogIucAPwxeKSJbRWRyFc//BDdms8sLW3DvyURcGH9Zw3qYJsrCxRg3xtAO9+Hp96lX9gmAquYCU3Eth0O4D+u3/Bur6nrgN8CHwIZy+yJoOwWmANnAv3HjJmNwA/m7gO+B/8UFGcCNwFYRycF1bY2v4THdCfxGRHKB/yGoxSMiCUBrYFEVzw/1nqzAdestU9VjNayHaaLEfizMmKZFRM4D7lLVG6JdF9N4WbgYY4wJO+sWM8YYE3YWLsYYY8LOwsUYY0zYxUW7AvVFmzZttFu3btGuhjHGNCjLli3br6pty5dbuHi6devG0qVLo10NY4xpUERkW6jyiHWLiUhnb/qJNSKyWkR+4pU/5M3uusJbLg56zv0istGbGfbCoPKxXtlGEbkvqLy7iHzllb/snb+PiCR6jzd667tF6jiNMcZUFMkxl2LgXlXtDQwB7gqa6fVPqtrPW+YCeOuuB/oAY3Ezw8Z603s/BVwE9AZuCNrP/3r7Oh13YdstXvktwCGv/E/edsYYY+pIxMJFVXf7Z3D1rm5ei5savDKXAS+paqGqbsH9YNE53rJRVTd7s7C+BFwmIgL8APiX9/zngMuD9vWcd/9fwGhve2OMMXWgTsZcvG6p/rjfzhgG3C0iE3FzOt2rqodwwRM8HUU2gTDaUa58MG76isNBs8IGb3+q/zmqWuzNCtsa2F+uXlNwU3HQpUuXkz5OY0zdKyoqIjs7m4KCgmhXpVFLSkqiU6dOxMfH12j7iIeLiKTgZpOdpqo5IjIT+C2g3u0fgZsjXY9QVHUWMAsgKyvLpiowpgHKzs4mNTWVbt26YR0UkaGqHDhwgOzsbLp3716j50T0OhcRiccFyxxVfR1AVfeoqk9VS3BTkJ/jbb4T6Bz09E5eWWXlB4CWIhJXrrzMvrz1ad72xphGpqCggNatW1uwRJCI0Lp161q1DiN5tpjgfqJ1rao+EVTeIWizK4BV3v23gOu9M726Axm4n1FdAmR4Z4Yl4Ab93/Jml10AXO09fxJulln/viZ5968GPlKbRM2YRsuCJfJq+x5HsltsGG668G9FZIVX9kvc2V79cN1iW4HbAFR1tYi8AqzBnWl2l6r6AETkbuB9IBZ4RlVXe/v7BfCSiDwMLMeFGd7tP0VkI3AQF0jGmGocPXqUjRs3smHDBjZu3IiIkJ6eTsuWLSvctmzZkrg4u1TOhBaxfxmq+hmhf/lubhXPeQR4JET53FDPU9XNBLrVgssLCPwGuTEmyPHjx9myZQvr169n/fr1bNiwofT+zp07q99BkNTU1JDBU5PbZs2aNZoWR2xsLGeddRZFRUXExcUxceJE7rnnHmJi6u8MW9OnT2fKlCk0a9as+o1PgH3tMKYR8vl87Nixo0xw+O9v3boVn89Xum3r1q3JyMhg9OjRZGZmkpGRQWZmJqeffjoxMTEcOnSIw4cP1+h2y5Ytpfdzc3OrrGN8fHytAym41RQbGxvpt7HGkpOTWbFiBQB79+7lhz/8ITk5Ofz617+OWp1UFVWtNOCmT5/OhAkTLFyMMWWpKnv27KnQ+vB3aRUWFpZu27x5czIzMxk4cCA33HBDaYhkZGTQunXrKl+nWbNmnHpqVZeohVZcXMyRI0eqDaTy4eQvKy4urnL/qamppKen88wzzxATE0NsbCyxsbHExcVVez8mJiZiraZ27doxa9YsBg0axEMPPURJSQn33XcfCxcupLCwkLvuuovbbruNhQsX8uCDD9KyZUu+/fZbrr32Ws466yyefPJJ8vPzefPNNznttNPYunUrN998M/v376dt27b84x//oEuXLuzZs4fbb7+dzZs3AzBz5kw6duzIhRdeyODBg1m2bBlz587lscceY8mSJeTn53P11Vfz61//mhkzZrBr1y5GjRpFmzZtWLBgQdjfBwsXY+q5Q4cOhWyBbNiwoUzrICEhgdNOO43MzEwuuugiMjMzS0OkQ4cOdd4FFRcXR+vWrasNr1BUlWPHjtUokOLi4lBVCgsLefTRdqxbl0To03eKvcUREW8J3AchJkaIiYnxAiiG/v2F6dNrV/8ePXrg8/nYu3cv//73v0lLS2PJkiUUFhYybNgwxowZA8A333zD2rVradWqFT169OBHP/oRixcv5sknn+TPf/4z06dP58c//jGTJk1i0qRJPPPMM0ydOpU333yTqVOnMmLECN544w18Ph95eXml/1aee+45hgwZAsAjjzxCq1at8Pl8jB49mpUrVzJ16lSeeOIJFixYQJs2bWr996kJCxdj6oHggfTyIbJ/f+Da35iYGLp160ZGRgbDhg0r043VpUuXetVVdDJEhObNm9O8efNqW01r166lZ8+eALRpAykpAIpqoGvIPQ5eKq4rKSkpLQ+2b18Ba9fuJzExsXRJSkoiMTGRuLi4akP7gw8+YOXKlfzrX24ykSNHjrBhwwYSEhIYNGgQHTq4E2hPO+200tA566yzSlsTX375Ja+//joAN954Iz//+c8B+Oijj3j++ecBN+aTlpbGoUOH6Nq1a2mwALzyyivMmjWL4uJidu/ezZo1a+jbt2/Vf4AwsHAxpo7UZiC9Y8eOZGZmcsUVV5RpgfTo0YPExMQoHUH9F2hhSLnbmvP5fBQWFpZZCgpiyMvL4+DBg2W2jYmJITExEVUlOzu7NHx27txJbGws7dq1Q1X585//zIUXXljmuQsXLizzt/Tvy3+/um7ByjRv3rz0/pYtW3j88cdZsmQJ6enpTJ48uc5mMrBwMSaMajOQ3qpVKzIzMxk9enRp68M/kJ7ivn6bKIiNjaVZs2YhB7pLSko4fvy4FzgFpeHjH/9SVQ4dOsSvfvUrrrrqKlatWkW/fv144okn6NOnDykpKWzfvr3GV7kDnHvuubz00kvceOONzJkzh+HDhwMwevRoZs6cybRp00q7xcrLycmhefPmpKWlsWfPHt59911GjhwJuDGr3Nxc6xYzpr6o7UB6RkZG6UC6P0RqMpBu6p+YmBiSkpJISkoiLS2ttLywsJBbbrmF48ePExsby1VXXcXNN99MUVERV1xxBVu2bGHkyJGoKunp6Tz++ONs2rSJvLw8tm7dSmJiIsXFxeTn51dosfz5z3/mpptu4g9/+EPpgD7Ak08+yZQpU/j73/9ObGwsM2fOLO1i8zv77LPp378/PXv2pHPnzgwbNqx03ZQpUxg7diwdO3aMyIC+2IXrTlZWltqPhTVeJSUlFBYWkp+fT35+PgUFBaX3yz+u7P7OnTtDDqTHx8dz+umnlxn/iOZAelOzdu1aevXqFe1qVElVKS4urtDd5l+KiorKbB8XF1dmjCd4rKcm4zyREuq9FpFlqppVfltruZg6V1RUVKMP8xMJgMrWBbcmaktESE5Opn379mRmZjJs2LAyIdKYBtJNZIgI8fHxxMfHh+zyDD3OU1DlOE+oEwwSEhLqzZcZCxdTK8eOHePtt99m1apVJxwAweMOtRUfH09ycjLJyckkJSVVuN+yZctK15W/X9N18fHx9eY/rGmcajrOEzzWU1BQwJEjR8qc3SYiJCQklAmc4KUuZwywcDHV8vl8fPTRR8yePZvXX3+9dOCwqg/ntLQ02rdvX+sP9eq2sxaCaWqCx3nKU1WKiooqnGBQWFjIgQMHKnyRi4+PDxk8kfi/ZeFiQlJVvv76a+bMmcOLL77I999/T1paGtdddx0TJkxg+PDh9kFvTJT5WyoJCQmkpqaWWVfVOM+RI0fKjPOcfvrptGzZMqx1s3AxZWzevJkXXniBOXPmsG7dOhISEhg3bhzjx49n3LhxIb89GWPqn9qM8wRfGxMuFi6G/fv388orrzBnzhy++OILAEaMGMFPf/pTrr76atLT06NcQ2NMuFU1zhMO9Xc+aBNRx44d4+WXX+bSSy+lQ4cO3HXXXeTk5PDYY4+xbds2Fi5cyK233mrBYkwNxMbG0q9fP84880wuvfRSDh8+HPbXqOzC2smTJ5dOLfOjH/2INWvWhP21T4S1XJoQ/8D8nDlzeO2118jLy+PUU0/lnnvuYcKECXUy35AxjVHwlPuTJk3iqaee4oEHHqjzejz99NN1/pqVsZZLI6eqLFu2jJ/+9Kd06tSJMWPG8MYbb3DdddexYMECtm/fzu9//3sLFmPCZOjQoaVzxW3atImxY8cycOBAhg8fzrp16wDX2rj99tvJysoiMzOTt99+G4Bnn32Wu+++u3Rfl1xyCQsXLix9fM8999CnTx9Gjx7Nvn37Krz2yJEj8V8M/t577zFgwADOPvtsRo8eDcDixYsZOnQo/fv359xzz+W7774rfd0rr7ySsWPHkpGRUTo55smwlksjtWXLFubMmWMD86ZpmTYNvBZE2PTrR03n3Pf5fMyfP59bbrkFcFOs/O1vfyMjI4OvvvqKO++8k48++giArVu3snjxYjZt2sSoUaPYuHFjlfs+evQoWVlZ/OlPf+I3v/kNv/71r/nLX/4Sctt9+/Zx66238sknn9C9e/fSCzF79uzJp59+SlxcHB9++CG//OUvee211wBYsWIFy5cvJzExkTPOOIMf//jHdO7cuUbHHYqFSyOyf/9+Xn31VWbPnl06MH/++efbwLwxEZafn0+/fv3YuXMnvXr14oILLiAvL48vvviCa64J/OJ68EwR1157LTExMaWzXftbNZWJiYnhuuuuA2DChAlceeWVlW67aNEizj///NIJMlu1agW46f4nTZrEhg0bEJEypyOPHj26dL603r17s23bNguXpuzYsWP85z//Yfbs2bz33nsUFxfTp08fHn30UW644Qa6du0a7SoaU3dq+6teYeIfczl27BgXXnghTz31FJMnT6Zly5alYzHllZ/1QUSIi4ujpKSktKyq6fFPZNaIX/3qV4waNYo33niDrVu3ls6QDJSZ/j82NvaEp/z3szGXBsjn8zFv3jwmT55M+/btuf7661m+fDn33HMPK1as4Ntvv+W+++6zYDGmjjVr1owZM2bwxz/+kWbNmtG9e3deffVVwI1/fvPNN6Xbvvrqq5SUlLBp0yY2b97MGWecQbdu3VixYgUlJSXs2LGDxYsXl25fUlJSelbYCy+8wHnnnVdpPYYMGcInn3zCli1bAEq7xY4cOVL642vPPvtsWI+9PGu5NBCqyvLly5k9ezYvvfQSu3fvpkWLFlx77bVMmDCB888/366YN6Ye6N+/P3379uXFF19kzpw53HHHHTz88MMUFRVx/fXXc/bZZwPQpUsXzjnnHHJycvjb3/5GUlISw4YNo3v37vTu3ZtevXoxYMCA0v02b96cxYsX8/DDD9OuXTtefvnlSuvQtm1bZs2axZVXXklJSQnt2rVj3rx5/PznP2fSpEk8/PDDjBs3LqLvg02576mvU+6XH5iPj49n3LhxTJgwwQbmjaFhTLlf3uTJk7nkkku4+uqro12VWrEp9xu4AwcO8Morr1QYmL/nnnu4+uqrSwfnjDGmvrJwqSf8A/Nz5szh3XfftYF5YxqxSI931AcWLlEUfMX866+/Tm5ubukV8+PHj6dv3772OyLGmAbJwqWOVTYwf80119jAvDGm0bBwqSNbtmzhhRdeYPbs2WUG5sePH88ll1xiA/PGmEbFwiWC/APzc+bM4fPPPwdg+PDhNjBvjGn0LFzCLD8/n7feeqvMwHzv3r353e9+xw9/+EMbmDemEcrOzuauu+5izZo1lJSUcMkll/CHP/yBNWvWsGvXLi6++GIAHnroIVJSUvjZz34W5RpHnl2hHwY+n48PP/ywzBXzy5YtY9q0aSxfvpxVq1Zx//33W7AY0wipKldeeSWXX345GzZsYP369eTl5fHAAw+wYsUK5s6dG7bX8vl8YdtXpFnL5STNmDGDxx57rMzA/Pjx4xkxYoQNzBvTBHz00UckJSVx0003AW5erj/96U907dqV+Ph4VJXPPvuM+++/H4A1a9YwcuRItm/fzrRp05g6dSoAs2fPZsaMGRw/fpzBgwfz17/+ldjYWFJSUrjtttv48MMPeeqpp6qc9qU+iVi4iEhn4HmgPaDALFV9UkRaAS8D3YCtwLWqekjcObdPAhcDx4DJqvq1t69JwH97u35YVZ/zygcCzwLJwFzgJ6qqlb1GpI71nHPOKb1iPjk5OVIvY4ypxoYN08jLWxHWfaak9CMjY3ql61evXs3AgQPLlLVo0YJu3bpx0003sX79+tKp8R966CHWrVvHggULyM3N5YwzzuCOO+5g48aNvPzyy3z++efEx8dz5513MmfOHCZOnMjRo0cZPHgwf/zjH8N6XJEWyZZLMXCvqn4tIqnAMhGZB0wG5qvqYyJyH3Af8AvgIiDDWwYDM4HBXlA8CGThQmqZiLzlhcVM4FbgK1y4jAXe9fYZ6jXCburUqaXfPIwxpjrjxo0jMTGRxMRE2rVrx549e5g/fz7Lli1j0KBBgBu7bdeuHeBaQldddVU0q3xCIhYuqrob2O3dzxWRtcCpwGXASG+z54CFuA/+y4Dn1U12tkhEWopIB2/beap6EMALqLEishBooaqLvPLngctx4VLZaxhjGrGqWhiR0rt379LZiv1ycnLYvn07cXEVP2JDTW2vqkyaNIlHH320wvZJSUkNsou9Tgb0RaQb0B/XwmjvBQ/A97huM3DBsyPoadleWVXl2SHKqeI1ytdriogsFZGloX4y1BhjqjN69GiOHTvG888/D7hB93vvvbf0BJ/c3Nwa7eNf//oXe/fuBdwU+du2bYtovSMt4uEiIinAa8A0Vc0JXue1UiI6LXNVr6Gqs1Q1S1Wz2rZtG8lqGGMaKRHhjTfe4NVXXyUjI4PMzEySkpL43e9+x6hRo1izZg39+vWrcor83r178/DDDzNmzBj69u3LBRdcwO7duyvdviGI6NliIhKPC5Y5qvq6V7xHRDqo6m6v22uvV74TCP5NzU5e2U4CXVz+8oVeeacQ21f1GsYYE3adO3fmP//5T4XyxMRElixZUunzVq1aVXr/uuuuK/0Z42B5eXnhqWQdi1jLxTv76+/AWlV9ImjVW8Ak7/4k4N9B5RPFGQIc8bq23gfGiEi6iKQDY4D3vXU5IjLEe62J5fYV6jWMMcbUgUi2XIYBNwLfisgKr+yXwGPAKyJyC7ANuNZbNxd3GvJG3KnINwGo6kER+S3gj//f+Af3gTsJnIr8rrdQxWsYY4ypA5E8W+wzoLL54keH2F6BuyrZ1zPAMyHKlwJnhig/EOo1jDHG1A2b/sUYY0zYWbgYY4wJOwsXY4wxYWfhYowxJ+HAgQP069ePfv36ccopp3DqqaeWPj5+/HiZbadPn86xY8eq3efIkSNZunRppKpcJ2xWZGOMOQmtW7dmxYoVQPW/1zJ9+nQmTJhAs2bN6rCG0WEtF2OMCbP58+fTv39/zjrrLG6++WYKCwuZMWMGu3btYtSoUYwaNQqAO+64g6ysLPr06cODDz4Y5VqHl7VcjDGNxrRp00pbEeHSr18/pk+fXuPtCwoKmDx5MvPnzyczM5OJEycyc+ZMpk2bxhNPPMGCBQto06YNAI888gitWrXC5/MxevRoVq5cSd++fcNa/2ixlosxxoSRz+eje/fuZGZmAjBp0iQ++eSTkNu+8sorDBgwgP79+7N69WrWrFlTl1WNKGu5GGMajdq0MKJty5YtPP744yxZsoT09HQmT55MQUFBtKsVNtZyMcaYMIqNjWXr1q1s3LgRgH/+85+MGDECgNTU1NIp+HNycmjevDlpaWns2bOHd999t9J9NkTWcjHGmDBKSkriH//4B9dccw3FxcUMGjSI22+/HYApU6YwduxYOnbsyIIFC+jfvz89e/akc+fODBs2LMo1Dy9xU3qZrKwsbejnlRvTFK1du5ZevXpFuxpNQqj3WkSWqWpW+W2tW8wYY0zYWbgYY4wJOwsXY0yDZ937kVfb99jCxRjToCUlJXHgwAELmAhSVQ4cOEBSUlKNn2NnixljGrROnTqRnZ3Nvn37ol2VRi0pKYlOnTrVeHsLF2NMgxYfH0/37t2jXQ1TjnWLGWOMCTsLF2OMMWFn4WKMMSbsLFyMMcaEnYWLMcaYsLNwMcZUlJcHH38MGzaAXT9iToCdimyMgb174bPP3PLpp7B8Ofh8bl23bjBmDFxwAYweDenpUa2qaRgsXIxpalRh8+ZAkHz2GXz3nVuXlASDB8N998HQobB9O3zwAbz0EsyaBTExMGiQC5oxY2DIEIiPj+7xmHrJptz32JT7ptHy+eDbbwNB8umnsHu3W5eeDsOGwfDhcN55MHAgJCZW3EdxMSxe7ILmgw/gq6+gpARSUmDUKBc0Y8ZARgaI1O3xmaiqbMp9CxePhYtpNAoKXBD4g+SLLyAnx63r3DkQJMOHQ+/erjVSW4cPw4IFLmjmzYNNm1x5ly5lu9Batw7bYZn6ycKlGhYupsE6dMgFyKefumXpUjh+3K3r0ycQJOedB127RqYOmza5kJk3D+bPhyNHXAtm4MBAq2boUEhIiMzrm6ixcKmGhYtpMLKzA0Hy2WewapUbR4mLg6wsFyTDh8O550an5VBcDEuWuKD54ANYtMh1zTVvDiNHBlo2PXtaF1ojYOFSDQuXaqjCmjXwzjswdy4cOADdu0OPHmVvu3d3HyImPFRh7dqy4yXbtrl1KSkuQPwtk3POgWbNolvfUI4cgYULA11oGza48k6dAkHzX/8FbdpEtZrmxFi4VMPCJYT8fPjoIxcm77wT+FA7+2zXvbJli1vy8so+r107Fzblg6dHD/eBEhtb98fSUBw/Dl9/HQiSzz93QQ7Qvn0gSIYPh759XWulodmyJdCF9uGHbvxGBAYMCJyFdu65oU8sMPVOnYeLiDwDXALsVdUzvbKHgFsB/w8v/FJV53rr7gduAXzAVFV93ysfCzwJxAJPq+pjXnl34CWgNbAMuFFVj4tIIvA8MBA4AFynqlurq6+Fi2fbNhck77zjgqWgwH0b/q//gnHj4OKLXUD4qcL+/e4DY/Nmt/jvb9niTmX1Xy8B7sOwa9dA2JQPoFatmlZXSV4efPlloGWyaJELdYDTTw8EyXnnuceN7b3x+dwYkb8L7csvXbdas2YwYkSgZdO7d+M79kYiGuFyPpAHPF8uXPJU9fFy2/YGXgTOAToCHwKZ3ur1wAVANrAEuEFV14jIK8DrqvqSiPwN+EZVZ4rInUBfVb1dRK4HrlDV66qrb5MNl+JiNxjsD5TVq115jx4uTMaNc//Ja/ELdGUUFbkxglDBs3mzC6ZgLVpUbO3473frduL1qC/27Cl7seKKFe4DNiYG+vUrO/h+yinRrm3dy80NdKF98AGsX+/KO3Ys24XWrl1Uq2kCotItJiLdgLdrEC73A6jqo97j94GHvNUPqeqFwdsBj+FaP6eoarGIDPVv53+uqn4pInHA90BbreZAm1S47NsH773nwuT99123RFwcnH9+IFAyM+vmm2JubqB7LVQAFRSU3b5jx9Ddbd27u3UnclptpKi6s6j8QfLpp4HxhqQkdwGiP0yGDHHBasrati3Qqpk/Hw4edOX9+we60IYNa/hfOhqwysIlGh22d4vIRGApcK+qHgJOBRYFbZPtlQHsKFc+GNcVdlhVi0Nsf6r/OV7wHPG2L/cVGURkCjAFoEuXLid/ZPWVqvuG7G+dfPWVK2vfHq64woXJBRdE58MtNdWNHfTtW3GdKnz/fejgWbgQZs8uO+9VYqJr3YQKnh49IC0tssfi88HKlWUH37//3q1LT3dBcuutgYsV7bTc6nXtCj/6kVt8Pjce5T8x4Ikn4Pe/h+Rk98XIf8pznz7WhVYP1HW4zAR+C6h3+0fg5jquQylVnQXMAtdyiVY9IiI31w2Wzp3rll27XPmgQfDggy5QBgyoX9/0yxOBDh3ccu65FdcXFroxnVAtnkWLXIssWHp65ScadOlS+w/7/PyKFyvm5rp1Xbq4iwj9LZNever3e90QxMa6f7+DBsEDD7j3+uOPAy2be+9123Xo4L4s+Zf27aNb7yaqTsNFVff474vI/wPe9h7uBDoHbdrJK6OS8gNASxGJ81ovwdv795XtdYuleds3fhs2BFonn3zizjxq0cJ9mxs3Di66qHH9R0tMdNONZGSEXn/oUMUxni1bXCvuzTfdeJBfTIw7UaGyEw3atw99saJ/H2eeCePHB8ZLGnNLuL5ITYVLLnELwI4dgaB55x14/nlXfvbZgS60885zLR0TcXU95tJBVXd79+8BBqvq9SLSB3iBwID+fCADENyA/mhcaCwBfqiqq0XkVeC1oAH9lar6VxG5CzgraED/SlW9trq6Nsgxl+PHXYj4A8Xfn9+rV2DsZNgwm1gwFJ/PteZCnWSwZUtg7i2/5OTAWVzx8YGLFc87z73HrVrV/TGYypWUuJmd/V1on33mvggkJbm/m78L7ayzrAvtJEXjbLEXgZFAG2AP8KD3uB+uW2wrcFtQ2DyA6yIrBqap6rte+cXAdNypyM+o6iNeeQ/cqcitgOXABFUtFJEk4J9Af+AgcL2qbq6uvg0mXHbvDlx3Mm+eO5U1MdFNHug/VbhHj2jXsuHLz4etW8t2ubVqFbhY0b79NixHj5btQluzxpW3b1+2C61Dh+jWswGyiyirUW/DpaTETaXhb518/bUr79Qp0Dr5wQ/sqnhjaiM7241J+ls2/lPi+/RxAZOU5L60BS/hLktIaBStJguXatSrcDl82J0i/M477pThffvcmMDQoYFAsea8MeFRUgLffOOC5uOP3f+/wkJ3GnxhYdmloCAwKWg4RDrAalqWnHzCJ5xYuFQjquESPG/XO++4KT98PtcNc9FFLkzGjLHpy42pD1RdwAQHTqgQimRZ8OPgGTBO1Ny57rPmBNSn61wMuD79BQsCgRI8b9cvfuECZfBgm4fLmPpGJPCNvz7w+U4+vHr2DHu1LFzq0vbtZeftys8PzNv1y19WnLfLGGOqExvrPkfq2YzYFi6RVFzsJuLzB8qqVa68Rw93xfHJzttljDH1lIVLuO3fX3berkOH3Lxdw4fD44+7QDnjDBuMN8Y0atWGi4j8BPgHkAs8jbt+5D5V/SDCdWsYguftmjvXTTvin7fr8sujO2+XMcZESU1aLjer6pMiciGQDtyIu0jRwgXcRIR//7u735Dm7TLGmAiqSbj4+28uBv7pTb1ifTp+48e7KUAa27xdxhhzEmoSLstE5AOgO3C/iKQCJZGtVgMyalS0a2CMMfVOTcLlFtx8YJtV9ZiItAZuimitjDHGNGg1GRRQoDcw1XvcHLBzZ40xxlSqJuHyV2AocIP3OBd4KmI1MsYY0+DVpFtssKoOEJHlAKp6SETs91mNMcZUqiYtlyIRicV1jyEibbEBfWOMMVWoSbjMAN4A2onII8BnwKMRrZUxxpgGrdpuMVWdIyLLcD81LMDlqro24jUzxhjTYNVk+pd/quqNwLoQZcYYY0wFNekW6xP8wBt/GRiZ6hhjjGkMKg0XEblfRHKBviKSIyK53uO9wL/rrIbGGGManErDRVUfVdVU4A+q2kJVU72ltareX4d1NMYY08DUZED/fhH5P8D5XtFCVX07stUyxhjTkFU75iIijwI/AdZ4y09E5HeRrpgxxpiGqyZX6I8D+qlqCYCIPAcsB34ZyYoZY4xpuGr6a1Ytg+6nRaAexhhjGpFKWy4i8hTwIvA74GsRWYi7iPJ84L46qZ0xxpgGqapusfXAH4AOwHxgK7AC+IWqfh/xmhljjGmwqjoV+UlVHQqMwAXNlbiwmSIiGXVUP2OMMQ1QtWMuqrpNVf9XVfvjftPlCoKmgjHGGGPKq8mpyHEicqmIzAHeBb7DtWKMMcaYkKoa0L8A11K5GFgMvARMUdWjdVQ3Y4wxDVRVA/r3Ay8A96rqoTqqjzHGmEagqgH9H6jq0ycaLCLyjIjsFZFVQWWtRGSeiGzwbtO9chGRGSKyUURWisiAoOdM8rbfICKTgsoHisi33nNmiIhU9RrGGGPqTk0vojwRzwJjy5XdB8xX1Qzc6c3+62UuAjK8ZQowE1xQAA8Cg4FzgAeDwmImcGvQ88ZW8xrGGGPqSMTCRVU/AQ6WK74MeM67/xxweVD58+osAlqKSAfgQmCeqh70WlDzgLHeuhaqukhVFXi+3L5CvYYxxpg6EsmWSyjtVXW3d/97oL13/1RgR9B22V5ZVeXZIcqreo0KRGSKiCwVkaX79u07gcMxxhgTSl2HSymvxaHRfA1VnaWqWaqa1bZt20hWxRhjmpS6Dpc9XpcW3u1er3wn0Dlou05eWVXlnUKUV/Uaxhhj6khdh8tbgP+Mr0kEfi75LWCid9bYEOCI17X1PjBGRNK9gfwxwPveuhwRGeKdJTax3L5CvYYxxpg6UpPfczkhIvIiMBJoIyLZuLO+HgNeEZFbgG3Atd7mc3EXa24EjgE3AajqQRH5LbDE2+43quo/SeBO3BlpybiZA971yit7DWOMMXVE3LCEycrK0qVLl0a7GsYY06CIyDJVzSpfHrUBfWOMMY2XhYsxxpiws3AxxhgTdhYuxhhjws7CxZgaKC52izGmZiJ2KrIxDVFODqxbV3HZuNGFS5s20K4dtG9f/W1ycrSPxpjosXAxTY4q7NwZCI61awP3d+0KbBcXB6efDr16wWWXQUIC7Nnjlr17YckSd5ubG/p1UlLKhk1VQdSyJbgfjTCmcbBwMY1WYaFrcQS3QNauhe++g7y8wHYtWrgAueACd9uzp1t69ID4+OpfJz/fhYw/dELdbtwIn38O+/e7cCsvPt4FTfnQCRVEbdrUrF7GRJOFi2nwDh2q2AJZtw42bwafL7Bdly4uNG6+ORAgvXq5D+yTaTUkJ0PXrm6pjs/nAqZ8+JQPpNWr3f3jx0Pvp3Xr6rvl/PebNz/xYzPmRFm4mAahpAS2b6/YClm3zn0Y+yUkQGYm9OsH118fCJHMTNdNFW2xsYEP/rPOqnpbVTcGVF2raMUKd3vkSOj9NG9e83Gi9HSIaQKn+ai6paSk7K0qJCY2jfcg0ixcTL2Snw/r11ccUP/uO7fOr1Ur1+q49NJAC6RnT+jWzX2ANwYikJbmloyM6rcvLHSBU1lraM8e2LoVvvoK9u1zH6jlxcVB27aBwElIqPxDuHxZfd8meNvqNG/uvoykpEBqauD+iS7Nm7v3tilpYodr6gNV1zUUakB969bAf34RFxa9esEPflC2K6tNm2geQf2UmAidO7ulOiUlcOBA9a2ioiL3LV4kcBt8P/g2Nrb6bULd1qdtwH2JycuruBw86FrPwWWVdVuGkpR08iFVfklIqL8ngli4mIjx+WDLloqtkLVr3X9Uv+RkOOMMGDwYJk8OhEhGhp3OGykxMa6F0rYt9OkT7do0XMePw9GjocMo1JKbW7Fsz56yj4Nb6NWJi6s6fGra6urRI/zdxhYu5qTl5QW6soJbIevXl/1m1769C41rrinbCunc2fq4TcOUkOCW9PTw7dPnq11ghVp27qxYVlV34Ny5cNFF4TsGsHAxNZSb67qs/MvGjYEg2bEjsF1MDJx2mguOiy8OhMgZZ7hxEmNM1WJj3enxLVqEb5+qobv7/C2pAQPC91p+Fi4GqBge5ZfgbixwTeiePWHEiLID6qed5vr+jTH1hwg0a+aWdu3q5jUtXJqInBzYtq3m4ZGc7AbTu3VzYyH++/6lbdv6O5BojIk+C5dGIienYmAEh4mFhzGmLlm4NBChwiN4OXSo7PbNmgWCYsiQiuHRpo2FhzEmcixc6omTCY+hQy08jDH1i4VLHTlypOoxDwsPY0xjYuESJkeOVN3yOHy47PbNmkH37i4ozj3XTXpo4WGMaSwsXE7SAw/AX/9adXgMG1ax5dG6tYWHMabxsnA5Sb16wfjxFh7GGBPMwuUkTZjgFmOMMQE2o5Mxxpiws3AxxhgTdhYuxhhjws7CxRhjTNhZuBhjjAk7CxdjjDFhF5VwEZGtIvKtiKwQkaVeWSsRmSciG7zbdK9cRGSGiGwUkZUiMiBoP5O87TeIyKSg8oHe/jd6z7UrTowxpg5Fs+UySlX7qWqW9/g+YL6qZgDzvccAFwEZ3jIFmAkujIAHgcHAOcCD/kDytrk16HljI384xhhj/OpTt9hlwHPe/eeAy4PKn1dnEdBSRDoAFwLzVPWgqh4C5gFjvXUtVHWRqirwfNC+jDHG1IFohYsCH4jIMhGZ4pW1V9Xd3v3vgfbe/VOBoF9pJ9srq6o8O0S5McaYOhKt6V/OU9WdItIOmCci64JXqqqKiEa6El6wTQHo0qVLpF/OGGOajKi0XFR1p3e7F3gDN2ayx+vSwrvd622+E+gc9PROXllV5Z1ClIeqxyxVzVLVrLZt257sYRljjPHUebiISHMRSfXfB8YAq4C3AP8ZX5OAf3v33wImemeNDQGOeN1n7wNjRCTdG8gfA7zvrcsRkSHeWWITg/ZljDGmDkSjW6w98IZ3dnAc8IKqviciS4BXROQWYBtwrbf9XOBiYCNwDLgJQFUPishvgSXedr9R1YPe/TuBZ4Fk4F1vMcYYU0fEnVBlsrKydOnSpdGuhjHGNCgisizokpJS9elUZGOMMY2EhYsxxpiws3AxxhgTdhYuxhhjws7CxRhjTNhZuBhjjAk7CxdjjDFhZ+FijDEm7CxcjDHGhJ2FizHGmLCzcDHGGBN2Fi7GGGPCzsLFGGNM2Fm4GGOMCTsLF2OMMWFn4WKMMSbsLFyMMcaEnYWLMcaYsLNwMcYYE3YWLsYYY8IuLtoVMMaYaDt+fB+5ucs4evQbIJa4uJYVlvj4dGJj04iJsY/NmrB3yRjTpBQVHSA3dxm5uUu9ZRmFhdtr/PzY2JSQ4VOTpSmFU9M4SmNMk1RUdKg0SPLy3G1BwdbS9cnJGaSlnUtq6lRSU7NISekHxFBcfLiK5VCZx4WFOzl6dLX3+AhQUmWdXDiln2BApSESG8m3LGwsXIwxjUJR0WHy8r4u0yopKNhcuj4p6TRSU8+hY8c7vSDpT3x8y5D7iotLBTrXug6qJfh8edWE0+Fy4ZTN0aOrSh+DVvkasbGplQRPTQKrRZ2Fk4WLMabBKS7OIS9veZmurfz8DaXrk5K6kZqaRceOU0hJGUhq6gDi41tFvF4iMcTFtSAurgXQpdbPd+GUW8tw2sHRo98GtZyqC6cWFUKnW7f/ITV14IkddCUsXIwx9VpxcV5QkLhWSX7+evwfoomJXUhNzeKUUyaTmppFaupA4uNbR7fSJ8iFUxpxcWlA11o/v7pwKio6VKGsoGAbJSXHw34sFi7GmHrD5ztKXt6KMl1bx46tIxAknUhJGUj79hNKgyQhoW10K12PnGw4hZOFizEmKny+fPLyvikNkby8ZRw9ugb/gHhCQgdSU7No1+56UlMHkpIykMTEU6JbaVNjFi7GmIjz+Qo4enRlma6to0dXAz4A4uPbkZo6iDZtrixtkSQmdoxupc1JsXA5Sfv3v01e3telpwn6z2UPPE5rUKcPGnOySkoKycv7tvTUXxckq1AtBiA+vg2pqVm0afN/vLO2BpKYeCoiEuWam3CycDlJBw++x65dT1W7nf/Cq7LBU/a2qnUxMc3sP5+pd0pKjnP06OoyZ20dPboS1SIA4uJakZqaRefOPyc1dSCpqVkkJna2f8tNgKhWfdpaU5GVlaVLly49oeeWlBTj8x2huPhI6emA/vuu/HC5dYcrrPd/q6uMSFyIFlFlraWKt03pymATGSUlRRw7tqZM11Ze3jeoujON4uJalrZEXNdWFklJXS1IGjkRWaaqWeXLG+2njYiMBZ4EYoGnVfWxSL1WTEwcMTGtT/j0R1WlpCS/0uAJFUw+3xHy8zcEbZtbg3o2qzR4KpZXXBcbmxKxDwr3JUdRLQFKTvLWF5b9qPrKlJUlZW4D74s0om1KOHZsfVDX1jeUlBQA7lqJ1NSBdOr0k9IxkqSkHhYkplSjDBdxAxxPARcA2cASEXlLVddEt2ahiQixsc2IjW1GYmKHE9qHqo/i4pxqgqnsuqKiA+Tnbyot938DrVxMaejExCSF9YO8uikzTPTExqaQkjKQjh3vKu3aSk4+DRGbVN1UrlGGC3AOsFFVNwOIyEvAZUC9DJdwEIklPj6d+Pj0E96Hz1cQIphCt6ZKSgq9kxRivA+Zk72NDdN+YhAJ777crRD4Vu+6kgNdyuVv68s2Fbev3TaQnNyd5OQMCxJTa401XE4FdgQ9zgYGl99IRKYAUwC6dKn9VA2NTWxsErGxpwB2LYEx5uQ06a8jqjpLVbNUNattW7vK1xhjwqWxhstOyk5p2skrM8YYUwcaa7gsATJEpLuIJADXA29FuU7GGNNkNMoxF1UtFpG7gfdxpyI/o6qro1wtY4xpMhpluACo6lxgbrTrYYwxTVFj7RYzxhgTRRYuxhhjws7CxRhjTNjZxJUeEdkHbDvBp7cB9oexOg2BHXPTYMfcNJzMMXdV1QoXClq4hIGILA01K2hjZsfcNNgxNw2ROGbrFjPGGBN2Fi7GGGPCzsIlPGZFuwJRYMfcNNgxNw1hP2YbczHGGBN21nIxxhgTdhYuxhhjws7CJQQR6SwiC0RkjYisFpGfeOWtRGSeiGzwbtO9chGRGSKyUURWisiAoH1N8rbfICKTonVM1QnnMXvrW4hItoj8JRrHUxNh/jv/3tvHWm+bevlj8idwzD1F5EsRKRSRn1W3n/ooXMfsrWspIv8SkXXe33poNI6pOidwzOO9f9PfisgXInJ20L7Gish33r/7+2pcCVW1pdwCdAAGePdTgfVAb+D3wH1e+X3A/3r3Lwbexf0O7hDgK6+8FbDZu0337qdH+/giecxB+3sSeAH4S7SPrQ7+zucCn+Nm4I4FvgRGRvv4wnTM7YBBwCPAz6rbT7SPL5LH7K17DviRdz8BaBnt4wvTMZ/r/2wCLgr6tx0LbAJ6eMf7TU3/ztZyCUFVd6vq1979XGAt7qeTL8P948K7vdy7fxnwvDqLgJYi0gG4EJinqgdV9RAwDxhbd0dSc2E8ZkRkINAe+KDujqD2wnjMCiTh/vMlAvHAnro6jtqo7TGr6l5VXQIU1XA/9U64jllE0oDzgb972x1X1cN1cAi1dgLH/IX3GQWwCPcDiwDnABtVdbOqHgde8vZRLQuXaohIN6A/8BXQXlV3e6u+x32Agvuj7Qh6WrZXVll5vXYyxywiMcAfgTLdCfXdyRyzqn4JLAB2e8v7qrq2Lup9Mmp4zLXdT712ksfcHdgH/ENElovI0yLSPGKVDZMTOOZbcC10OInPMAuXKohICvAaME1Vc4LXqWszNrrzuMNwzHcCc1U1O0JVDLuTPWYROR3ohfu2dyrwAxEZHqHqhkW4/m1XtZ/6JgzHHAcMAGaqan/gKK5rqd6q7TGLyChcuPziZF/bwqUSIhKP+6PMUdXXveI9QV0/HYC9XvlOoHPQ0zt5ZZWV10thOuahwN0ishV4HJgoIo/VQfVPSJiO+QpgkarmqWoe7ltfvRzohVofc233Uy+F6ZizgWxV9bfQ/oULm3qptscsIn2Bp4HLVPWAV3zCn2EWLiF4Z/r8HVirqk8ErXoL8J/xNQn4d1D5RO9soiHAEa/p+T4wRkTSvbMyxnhl9U64jllVx6tqF1Xthusae15V6+W3uzD+nbcDI0QkzvsPPQLXx13vnMAx13Y/9U64jllVvwd2iMgZXtFoYE2YqxsWtT1mEekCvA7cqKrrg7ZfAmSISHcRSQCu9/ZRvZqM+je1BTgP11xcCazwlouB1sB8YAPwIdDK216Ap3BnVXwLZAXt62Zgo7fcFO1jq4tjDtrnZOr32WJhOWbcGTX/Fxcoa4Anon1sYTzmU3Df2HOAw979FpXtJ9rHF8lj9tb1A5Z6+3qT+nv2Z22P+WngUNC2S4P2dTHubLNNwAM1rYNN/2KMMSbsrFvMGGNM2Fm4GGOMCTsLF2OMMWFn4WKMMSbsLFyMMcaEnYWLMVHiXS/zmYhcFFR2jYi8F816GRMOdiqyMVEkImcCr+LmfooDlgNjVXXTCewrTlWLw1xFY06IhYsxUSYiv8fNU9Xcu+0KnImbXfkhVf23N/ngP71tAO5W1S9EZCTwW9wFcD1VNbNua29MaBYuxkSZN7Pu18Bx4G1gtarOFpGWwGJcq0aBElUtEJEM4EVVzfLC5R3gTFXdEo36GxNKXLQrYExTp6pHReRlIA+4FrhUAr+AmAR0AXYBfxGRfoAPCG6hLLZgMfWNhYsx9UOJtwhwlap+F7xSRB7C/QDZ2bgTcQqCVh+tozoaU2N2tpgx9cv7wI+9WW0Rkf5eeRqwW1VLgBtxk2UaU29ZuBhTv/wWN5C/UkRWe48B/gpMEpFvgJ5Ya8XUczagb4wxJuys5WKMMSbsLFyMMcaEnYWLMcaYsLNwMcYYE3YWLsYYY8LOwsUYY0zYWbgYY4wJu/8PoCK4fxfPXTEAAAAASUVORK5CYII=\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ]
  }
 ]
}